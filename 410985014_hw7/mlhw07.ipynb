{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"b5cFq_TgWlQ_"},"source":["# Homework 11 - Transfer Learning (Domain Adversarial Training)\n","\n","> Author: Arvin Liu (r09922071@ntu.edu.tw)\n","\n","若有任何問題，歡迎來信至助教信箱 kafuchino0410@gmail.com\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"3-qnUkspmap3"},"source":["# Data Introduce\n","\n","這次的任務是Source Data: 真實照片，Target Data: 手畫塗鴉。\n","\n","我們必須讓model看過真實照片以及標籤，嘗試去預測手畫塗鴉的標籤為何。\n","\n","資料位於[這裡](https://drive.google.com/file/d/1e4CaQ5VUF3F04XRDGXrnRQGogo89TiF8/view?usp=sharing)，以下的code分別為下載和觀看這次的資料大概長甚麼樣子。\n","\n","特別注意一點: **這次的source和target data的圖片都是平衡的，你們可以使用這個資訊做其他事情。**"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"vNiZCGrIYKdR"},"source":["# Readme\n","\n","\n","這份作業的任務是Transfer Learning中的Domain Adversarial Training。\n","\n","<img src=\"https://i.imgur.com/iMVIxCH.png\" width=\"500px\">\n","\n","> 也就是左下角的那一塊。\n","\n","## Scenario and Why Domain Adversarial Training\n","你現在有Source Data + label，其中Source Data和Target Data可能有點關係，所以你想要訓練一個model做在Source Data上並Predict在Target Data上。\n","\n","但這樣有什麼樣的問題? 相信大家學過Anomaly Detection就會知道，如果有data是在Source Data沒有出現過的(或稱Abnormal的)，那麼model大部分都會因為不熟悉這個data而可能亂做一發。 \n","\n","以下我們將model拆成Feature Extractor(上半部)和Classifier(下半部)來作例子:\n","<img src=\"https://i.imgur.com/IL0PxCY.png\" width=\"500px\">\n","\n","整個Model在學習Source Data的時候，Feature Extrator因為看過很多次Source Data，所以所抽取出來的Feature可能就頗具意義，例如像圖上的藍色Distribution，已經將圖片分成各個Cluster，所以這個時候Classifier就可以依照這個Cluster去預測結果。\n","\n","但是在做Target Data的時候，Feature Extractor會沒看過這樣的Data，導致輸出的Target Feature可能不屬於在Source Feature Distribution上，這樣的Feature給Classifier預測結果顯然就不會做得好。\n","\n","## Domain Adversarial Training of Nerural Networks (DaNN)\n","基於如此，是不是只要讓Soucre Data和Target Data經過Feature Extractor都在同個Distribution上，就會做得好了呢? 這就是DaNN的主要核心。\n","\n","<img src=\"https://i.imgur.com/vrOE5a6.png\" width=\"500px\">\n","\n","我們追加一個Domain Classifier，在學習的過程中，讓Domain Classifier去判斷經過Feature Extractor後的Feature是源自於哪個domain，讓Feature Extractor學習如何產生Feature以**騙過**Domain Classifier。 持久下來，通常Feature Extractor都會打贏Domain Classifier。(因為Domain Classifier的Input來自於Feature Extractor，而且對Feature Extractor來說Domain&Classification的任務並沒有衝突。)\n","\n","如此一來，我們就可以確信不管是哪一個Domain，Feature Extractor都會把它產生在同一個Feature Distribution上。"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"moXQw9To5TqZ"},"source":["# Special Domain Knowledge\n","\n","因為大家塗鴉的時候通常只會畫輪廓，我們可以根據這點將source data做點邊緣偵測處理，讓source data更像target data一點。\n","\n","## Canny Edge Detection\n","算法這邊不贅述，只教大家怎麼用。若有興趣歡迎參考wiki或[這裡](https://medium.com/@pomelyu5199/canny-edge-detector-%E5%AF%A6%E4%BD%9C-opencv-f7d1a0a57d19)。\n","\n","cv2.Canny使用非常方便，只需要兩個參數: low_threshold, high_threshold。\n","\n","```cv2.Canny(image, low_threshold, high_threshold)```\n","\n","簡單來說就是當邊緣值超過high_threshold，我們就確定它是edge。如果只有超過low_threshold，那就先判斷一下再決定是不是edge。\n","\n","以下我們直接拿source data做做看。"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"8THSdt_hmwYh"},"source":["# Data Process\n","\n","在這裡我故意將data用成可以使用torchvision.ImageFolder的形式，所以只要使用該函式便可以做出一個datasets。\n","\n","transform的部分請參考以下註解。\n","<!-- \n","#### 一些細節\n","\n","在一般的版本上，對灰階圖片使用RandomRotation使用```transforms.RandomRotation(15)```即可。但在colab上需要加上```fill=(0,)```才可運行。\n","在n98上執行需要把```fill=(0,)```拿掉才可運行。 -->\n"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-06-03T06:47:15.480706Z","iopub.status.busy":"2023-06-03T06:47:15.478622Z","iopub.status.idle":"2023-06-03T06:47:52.724131Z","shell.execute_reply":"2023-06-03T06:47:52.723152Z","shell.execute_reply.started":"2023-06-03T06:47:15.480672Z"},"id":"WZHIBGknmi8Z","trusted":true},"outputs":[{"ename":"OSError","evalue":"[WinError 127] 找不到指定的程序。 Error loading \"c:\\Users\\許軒輔\\.conda\\envs\\torch\\lib\\site-packages\\torch\\lib\\c10_cuda.dll\" or one of its dependencies.","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)","Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrandom\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mF\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\許軒輔\\.conda\\envs\\torch\\lib\\site-packages\\torch\\__init__.py:129\u001b[0m\n\u001b[0;32m    127\u001b[0m     err \u001b[39m=\u001b[39m ctypes\u001b[39m.\u001b[39mWinError(last_error)\n\u001b[0;32m    128\u001b[0m     err\u001b[39m.\u001b[39mstrerror \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m Error loading \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mdll\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m or one of its dependencies.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 129\u001b[0m     \u001b[39mraise\u001b[39;00m err\n\u001b[0;32m    130\u001b[0m \u001b[39melif\u001b[39;00m res \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    131\u001b[0m     is_loaded \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n","\u001b[1;31mOSError\u001b[0m: [WinError 127] 找不到指定的程序。 Error loading \"c:\\Users\\許軒輔\\.conda\\envs\\torch\\lib\\site-packages\\torch\\lib\\c10_cuda.dll\" or one of its dependencies."]}],"source":["from tqdm import tqdm\n","import cv2\n","import random\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Function\n","\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","from torchvision.datasets import ImageFolder\n","from torch.utils.data import DataLoader\n","\n","def same_seeds(seed):\n","    # Python built-in random module\n","    random.seed(seed)\n","    # Numpy\n","    np.random.seed(seed)\n","    # Torch\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed)\n","        torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.benchmark = False\n","    torch.backends.cudnn.deterministic = True\n","\n","# Training hyperparameters\n","same_seeds(1178)\n","\n","source_transform = transforms.Compose([\n","    # Turn RGB to grayscale. (Bacause Canny do not support RGB images.)\n","    transforms.Grayscale(),\n","    # cv2 do not support skimage.Image, so we transform it to np.array, \n","    # and then adopt cv2.Canny algorithm.\n","    transforms.Lambda(lambda x: cv2.Canny(np.array(x), 170, 300)),\n","    # Transform np.array back to the skimage.Image.\n","    transforms.ToPILImage(),\n","    # 50% Horizontal Flip. (For Augmentation)\n","    transforms.RandomHorizontalFlip(),\n","    # Rotate +- 15 degrees. (For Augmentation), and filled with zero \n","    # if there's empty pixel after rotation.\n","    transforms.RandomRotation(15, fill=(0,)),\n","    # Transform to tensor for model inputs.\n","    transforms.ToTensor(),\n","])\n","target_transform = transforms.Compose([\n","    # Turn RGB to grayscale.\n","    transforms.Grayscale(),\n","    # Resize: size of source data is 32x32, thus we need to \n","    #  enlarge the size of target data from 28x28 to 32x32。\n","    transforms.Resize((32, 32)),\n","    # 50% Horizontal Flip. (For Augmentation)\n","    transforms.RandomHorizontalFlip(),\n","    # Rotate +- 15 degrees. (For Augmentation), and filled with zero \n","    # if there's empty pixel after rotation.\n","    transforms.RandomRotation(15, fill=(0,)),\n","    # Transform to tensor for model inputs.\n","    transforms.ToTensor(),\n","])\n","\n","source_dataset = ImageFolder('C:/code/pytorch/real_or_drawing/train_data', transform=source_transform)\n","target_dataset = ImageFolder('C:/code/pytorch/real_or_drawing/test_data', transform=target_transform)\n","\n","source_dataloader = DataLoader(source_dataset, batch_size=32, shuffle=True)\n","target_dataloader = DataLoader(target_dataset, batch_size=32, shuffle=True)\n","test_dataloader = DataLoader(target_dataset, batch_size=128, shuffle=False)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"hdwDEMrOycs5"},"source":["# Model\n","\n","Feature Extractor: 典型的VGG-like疊法。\n","\n","Label Predictor / Domain Classifier: MLP到尾。\n","\n","相信作業寫到這邊大家對以下的Layer都很熟悉，因此不再贅述。"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-03T06:47:52.726501Z","iopub.status.busy":"2023-06-03T06:47:52.725626Z","iopub.status.idle":"2023-06-03T06:47:52.741152Z","shell.execute_reply":"2023-06-03T06:47:52.740188Z","shell.execute_reply.started":"2023-06-03T06:47:52.726463Z"},"id":"3uw2eP09z-pD","trusted":true},"outputs":[],"source":["class FeatureExtractor(nn.Module):\n","\n","    def __init__(self):\n","        super(FeatureExtractor, self).__init__()\n","\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(1, 64, 3, 1, 1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2),\n","\n","            nn.Conv2d(64, 128, 3, 1, 1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2),\n","\n","            nn.Conv2d(128, 256, 3, 1, 1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2),\n","\n","            nn.Conv2d(256, 256, 3, 1, 1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2),\n","\n","            nn.Conv2d(256, 512, 3, 1, 1),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2)\n","        )\n","        \n","    def forward(self, x):\n","        x = self.conv(x).squeeze()\n","        return x\n","\n","class LabelPredictor(nn.Module):\n","\n","    def __init__(self):\n","        super(LabelPredictor, self).__init__()\n","\n","        self.layer = nn.Sequential(\n","            nn.Linear(512, 512),\n","            nn.ReLU(),\n","\n","            nn.Linear(512, 512),\n","            nn.ReLU(),\n","\n","            nn.Linear(512, 10),\n","        )\n","\n","    def forward(self, h):\n","        c = self.layer(h)\n","        return c\n","\n","class DomainClassifier(nn.Module):\n","\n","    def __init__(self):\n","        super(DomainClassifier, self).__init__()\n","\n","        self.layer = nn.Sequential(\n","            nn.Linear(512, 512),\n","            nn.BatchNorm1d(512),\n","            nn.ReLU(),\n","\n","            nn.Linear(512, 512),\n","            nn.BatchNorm1d(512),\n","            nn.ReLU(),\n","\n","            nn.Linear(512, 512),\n","            nn.BatchNorm1d(512),\n","            nn.ReLU(),\n","\n","            nn.Linear(512, 512),\n","            nn.BatchNorm1d(512),\n","            nn.ReLU(),\n","\n","            nn.Linear(512, 1),\n","        )\n","\n","    def forward(self, h):\n","        y = self.layer(h)\n","        return y"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"lxdBIPhF0Icb"},"source":["# Pre-processing\n","\n","這裡我們選用Adam來當Optimizer。"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-03T06:47:52.742995Z","iopub.status.busy":"2023-06-03T06:47:52.742487Z","iopub.status.idle":"2023-06-03T06:47:55.502219Z","shell.execute_reply":"2023-06-03T06:47:55.501274Z","shell.execute_reply.started":"2023-06-03T06:47:52.742953Z"},"id":"hrxKelBy0PJ7","trusted":true},"outputs":[],"source":["feature_extractor = FeatureExtractor().cuda()\n","label_predictor = LabelPredictor().cuda()\n","domain_classifier = DomainClassifier().cuda()\n","\n","class_criterion = nn.CrossEntropyLoss()\n","domain_criterion = nn.BCEWithLogitsLoss()\n","\n","optimizer_F = optim.Adam(feature_extractor.parameters())\n","optimizer_C = optim.Adam(label_predictor.parameters())\n","optimizer_D = optim.Adam(domain_classifier.parameters())\n","learning_rate = 1e-3"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"xuAE4cqJ0itR"},"source":["# Start Training\n","\n","\n","## 如何實作DaNN?\n","\n","理論上，在原始paper中是加上Gradient Reversal Layer，並將Feature Extractor / Label Predictor / Domain Classifier 一起train，但其實我們也可以交換的train Domain Classfier & Feature Extractor(就像在train GAN的Generator & Discriminator一樣)，這也是可行的。\n","\n","在code實現中，我們採取後者的方式。\n","\n","## 小提醒\n","* 原文中的lambda(控制Domain Adversarial Loss的係數)是有Adaptive的版本，如果有興趣可以參考[原文](https://arxiv.org/pdf/1505.07818.pdf)。\n","* 因為我們完全沒有target的label，所以結果如何，只好丟kaggle看看囉:)?"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-03T06:49:51.044025Z","iopub.status.busy":"2023-06-03T06:49:51.043665Z","iopub.status.idle":"2023-06-03T06:50:29.363470Z","shell.execute_reply":"2023-06-03T06:50:29.361837Z","shell.execute_reply.started":"2023-06-03T06:49:51.043996Z"},"id":"lRAFFKvX0p9y","outputId":"493d3566-991b-440a-c9e8-abf33cec7d49","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["epoch   0: train D loss: 0.3668, train F loss: 1.8865, train acc: 0.3046, domain acc: 0.8723, lr: 0.001000, lamb: 0.0100, time: 11.6745\n","epoch   1: train D loss: 0.3852, train F loss: 1.5875, train acc: 0.4322, domain acc: 0.8364, lr: 0.001000, lamb: 0.0100, time: 9.4132\n","epoch   2: train D loss: 0.3773, train F loss: 1.4886, train acc: 0.4748, domain acc: 0.8399, lr: 0.001000, lamb: 0.1821, time: 9.1725\n","epoch   3: train D loss: 0.5846, train F loss: 1.3082, train acc: 0.5044, domain acc: 0.7057, lr: 0.001000, lamb: 0.2887, time: 9.2676\n","epoch   4: train D loss: 0.6241, train F loss: 1.1946, train acc: 0.5240, domain acc: 0.6678, lr: 0.001000, lamb: 0.3643, time: 9.3196\n","epoch   5: train D loss: 0.6489, train F loss: 1.0895, train acc: 0.5364, domain acc: 0.6300, lr: 0.001000, lamb: 0.4229, time: 9.1962\n","epoch   6: train D loss: 0.6476, train F loss: 0.9927, train acc: 0.5552, domain acc: 0.6279, lr: 0.001000, lamb: 0.4708, time: 9.2526\n","epoch   7: train D loss: 0.6678, train F loss: 0.9267, train acc: 0.5656, domain acc: 0.6045, lr: 0.001000, lamb: 0.5114, time: 9.2997\n","epoch   8: train D loss: 0.6744, train F loss: 0.8869, train acc: 0.5738, domain acc: 0.6033, lr: 0.001000, lamb: 0.5464, time: 9.3153\n","epoch   9: train D loss: 0.6705, train F loss: 0.8192, train acc: 0.5834, domain acc: 0.5964, lr: 0.001000, lamb: 0.5774, time: 9.2848\n","epoch  10: train D loss: 0.6740, train F loss: 0.7515, train acc: 0.6074, domain acc: 0.5832, lr: 0.001000, lamb: 0.6051, time: 9.3003\n","epoch  11: train D loss: 0.6701, train F loss: 0.7091, train acc: 0.6150, domain acc: 0.5903, lr: 0.001000, lamb: 0.6301, time: 9.3441\n","epoch  12: train D loss: 0.6786, train F loss: 0.6556, train acc: 0.6192, domain acc: 0.5769, lr: 0.001000, lamb: 0.6530, time: 9.2603\n","epoch  13: train D loss: 0.6700, train F loss: 0.6134, train acc: 0.6340, domain acc: 0.6038, lr: 0.001000, lamb: 0.6740, time: 9.2738\n","epoch  14: train D loss: 0.6744, train F loss: 0.5627, train acc: 0.6444, domain acc: 0.5707, lr: 0.001000, lamb: 0.6935, time: 9.3068\n","epoch  15: train D loss: 0.6770, train F loss: 0.5147, train acc: 0.6646, domain acc: 0.5846, lr: 0.001000, lamb: 0.7116, time: 9.3107\n","epoch  16: train D loss: 0.6750, train F loss: 0.4636, train acc: 0.6688, domain acc: 0.5883, lr: 0.001000, lamb: 0.7286, time: 9.3245\n","epoch  17: train D loss: 0.6636, train F loss: 0.4492, train acc: 0.6792, domain acc: 0.6234, lr: 0.001000, lamb: 0.7445, time: 9.2313\n","epoch  18: train D loss: 0.6701, train F loss: 0.3812, train acc: 0.6928, domain acc: 0.6020, lr: 0.001000, lamb: 0.7595, time: 9.4088\n","epoch  19: train D loss: 0.6711, train F loss: 0.3392, train acc: 0.7018, domain acc: 0.5930, lr: 0.001000, lamb: 0.7738, time: 9.3482\n","epoch  20: train D loss: 0.6836, train F loss: 0.2889, train acc: 0.7138, domain acc: 0.5636, lr: 0.001000, lamb: 0.7872, time: 9.2561\n","epoch  21: train D loss: 0.6804, train F loss: 0.2727, train acc: 0.7216, domain acc: 0.5988, lr: 0.001000, lamb: 0.8001, time: 9.2799\n","epoch  22: train D loss: 0.6712, train F loss: 0.2504, train acc: 0.7298, domain acc: 0.6209, lr: 0.001000, lamb: 0.8123, time: 9.2616\n","epoch  23: train D loss: 0.6903, train F loss: 0.1866, train acc: 0.7476, domain acc: 0.5659, lr: 0.001000, lamb: 0.8240, time: 9.3413\n","epoch  24: train D loss: 0.6742, train F loss: 0.1436, train acc: 0.7608, domain acc: 0.5878, lr: 0.001000, lamb: 0.8351, time: 9.3570\n","epoch  25: train D loss: 0.6735, train F loss: 0.1286, train acc: 0.7658, domain acc: 0.5930, lr: 0.001000, lamb: 0.8459, time: 9.3184\n","epoch  26: train D loss: 0.6895, train F loss: 0.0701, train acc: 0.7760, domain acc: 0.5583, lr: 0.001000, lamb: 0.8562, time: 9.3381\n","epoch  27: train D loss: 0.6724, train F loss: 0.0261, train acc: 0.7916, domain acc: 0.6049, lr: 0.001000, lamb: 0.8661, time: 9.4848\n","epoch  28: train D loss: 0.6607, train F loss: 0.0084, train acc: 0.8008, domain acc: 0.6239, lr: 0.001000, lamb: 0.8757, time: 9.5429\n","epoch  29: train D loss: 0.6741, train F loss: -0.0107, train acc: 0.8046, domain acc: 0.6063, lr: 0.001000, lamb: 0.8849, time: 10.5077\n","epoch  30: train D loss: 0.6648, train F loss: -0.0384, train acc: 0.8182, domain acc: 0.6138, lr: 0.001000, lamb: 0.8938, time: 9.3835\n","epoch  31: train D loss: 0.6757, train F loss: -0.0805, train acc: 0.8246, domain acc: 0.5916, lr: 0.001000, lamb: 0.9024, time: 9.7309\n","epoch  32: train D loss: 0.6698, train F loss: -0.1058, train acc: 0.8366, domain acc: 0.6061, lr: 0.001000, lamb: 0.9107, time: 9.2874\n","epoch  33: train D loss: 0.6713, train F loss: -0.1303, train acc: 0.8428, domain acc: 0.5935, lr: 0.001000, lamb: 0.9188, time: 9.6149\n","epoch  34: train D loss: 0.6658, train F loss: -0.1515, train acc: 0.8508, domain acc: 0.6117, lr: 0.001000, lamb: 0.9267, time: 9.4198\n","epoch  35: train D loss: 0.6664, train F loss: -0.1925, train acc: 0.8560, domain acc: 0.6074, lr: 0.001000, lamb: 0.9343, time: 9.6072\n","epoch  36: train D loss: 0.6671, train F loss: -0.1916, train acc: 0.8600, domain acc: 0.6153, lr: 0.001000, lamb: 0.9417, time: 10.2387\n","epoch  37: train D loss: 0.6719, train F loss: -0.2325, train acc: 0.8688, domain acc: 0.5893, lr: 0.001000, lamb: 0.9489, time: 10.0129\n","epoch  38: train D loss: 0.6650, train F loss: -0.2421, train acc: 0.8696, domain acc: 0.6216, lr: 0.001000, lamb: 0.9559, time: 9.9920\n","epoch  39: train D loss: 0.6744, train F loss: -0.2570, train acc: 0.8732, domain acc: 0.6047, lr: 0.001000, lamb: 0.9627, time: 9.9608\n","epoch  40: train D loss: 0.6666, train F loss: -0.2899, train acc: 0.8836, domain acc: 0.6100, lr: 0.001000, lamb: 0.9694, time: 9.6271\n","epoch  41: train D loss: 0.6676, train F loss: -0.3042, train acc: 0.8886, domain acc: 0.6221, lr: 0.001000, lamb: 0.9759, time: 9.5414\n","epoch  42: train D loss: 0.6596, train F loss: -0.3131, train acc: 0.8982, domain acc: 0.6265, lr: 0.001000, lamb: 0.9822, time: 9.4824\n","epoch  43: train D loss: 0.6684, train F loss: -0.3343, train acc: 0.8984, domain acc: 0.5990, lr: 0.001000, lamb: 0.9884, time: 9.2504\n","epoch  44: train D loss: 0.6702, train F loss: -0.3471, train acc: 0.8968, domain acc: 0.6103, lr: 0.001000, lamb: 0.9944, time: 9.3521\n","epoch  45: train D loss: 0.6646, train F loss: -0.3718, train acc: 0.9034, domain acc: 0.6154, lr: 0.001000, lamb: 1.0003, time: 9.5086\n","epoch  46: train D loss: 0.6744, train F loss: -0.3744, train acc: 0.9014, domain acc: 0.5956, lr: 0.001000, lamb: 1.0061, time: 9.4328\n","epoch  47: train D loss: 0.6568, train F loss: -0.3853, train acc: 0.9178, domain acc: 0.6353, lr: 0.001000, lamb: 1.0118, time: 9.5406\n","epoch  48: train D loss: 0.6698, train F loss: -0.4143, train acc: 0.9170, domain acc: 0.5999, lr: 0.001000, lamb: 1.0173, time: 9.6701\n","epoch  49: train D loss: 0.6618, train F loss: -0.4156, train acc: 0.9204, domain acc: 0.6322, lr: 0.001000, lamb: 1.0227, time: 9.6222\n","epoch  50: train D loss: 0.6669, train F loss: -0.4366, train acc: 0.9212, domain acc: 0.6125, lr: 0.001000, lamb: 1.0280, time: 9.5742\n","epoch  51: train D loss: 0.6608, train F loss: -0.4278, train acc: 0.9174, domain acc: 0.6262, lr: 0.001000, lamb: 1.0332, time: 9.7927\n","epoch  52: train D loss: 0.6756, train F loss: -0.4535, train acc: 0.9208, domain acc: 0.5857, lr: 0.001000, lamb: 1.0383, time: 9.5987\n","epoch  53: train D loss: 0.6496, train F loss: -0.4322, train acc: 0.9214, domain acc: 0.6411, lr: 0.001000, lamb: 1.0433, time: 9.4267\n","epoch  54: train D loss: 0.6738, train F loss: -0.4563, train acc: 0.9234, domain acc: 0.5911, lr: 0.001000, lamb: 1.0482, time: 10.4380\n","epoch  55: train D loss: 0.6680, train F loss: -0.4893, train acc: 0.9330, domain acc: 0.6072, lr: 0.001000, lamb: 1.0531, time: 9.5271\n","epoch  56: train D loss: 0.6694, train F loss: -0.5124, train acc: 0.9400, domain acc: 0.6065, lr: 0.001000, lamb: 1.0578, time: 9.1714\n","epoch  57: train D loss: 0.6662, train F loss: -0.4817, train acc: 0.9308, domain acc: 0.6046, lr: 0.001000, lamb: 1.0625, time: 9.2439\n","epoch  58: train D loss: 0.6564, train F loss: -0.4654, train acc: 0.9320, domain acc: 0.6353, lr: 0.001000, lamb: 1.0670, time: 9.2996\n","epoch  59: train D loss: 0.6679, train F loss: -0.5201, train acc: 0.9404, domain acc: 0.6101, lr: 0.001000, lamb: 1.0715, time: 9.2129\n","epoch  60: train D loss: 0.6617, train F loss: -0.5002, train acc: 0.9362, domain acc: 0.6216, lr: 0.001000, lamb: 1.0759, time: 9.2420\n","epoch  61: train D loss: 0.6675, train F loss: -0.5187, train acc: 0.9306, domain acc: 0.6190, lr: 0.001000, lamb: 1.0803, time: 9.1528\n","epoch  62: train D loss: 0.6486, train F loss: -0.4927, train acc: 0.9418, domain acc: 0.6463, lr: 0.001000, lamb: 1.0846, time: 9.1803\n","epoch  63: train D loss: 0.6635, train F loss: -0.5345, train acc: 0.9426, domain acc: 0.6175, lr: 0.001000, lamb: 1.0888, time: 9.1759\n","epoch  64: train D loss: 0.6607, train F loss: -0.5277, train acc: 0.9418, domain acc: 0.6181, lr: 0.001000, lamb: 1.0929, time: 9.2273\n","epoch  65: train D loss: 0.6620, train F loss: -0.5273, train acc: 0.9378, domain acc: 0.6230, lr: 0.001000, lamb: 1.0970, time: 9.1885\n","epoch  66: train D loss: 0.6643, train F loss: -0.5424, train acc: 0.9386, domain acc: 0.6136, lr: 0.001000, lamb: 1.1010, time: 9.2284\n","epoch  67: train D loss: 0.6556, train F loss: -0.5419, train acc: 0.9428, domain acc: 0.6285, lr: 0.001000, lamb: 1.1049, time: 9.2054\n","epoch  68: train D loss: 0.6707, train F loss: -0.5577, train acc: 0.9406, domain acc: 0.6019, lr: 0.001000, lamb: 1.1088, time: 9.1704\n","epoch  69: train D loss: 0.6642, train F loss: -0.5606, train acc: 0.9482, domain acc: 0.6081, lr: 0.001000, lamb: 1.1127, time: 9.1564\n","epoch  70: train D loss: 0.6577, train F loss: -0.5759, train acc: 0.9530, domain acc: 0.6291, lr: 0.000999, lamb: 1.1164, time: 9.1800\n","epoch  71: train D loss: 0.6684, train F loss: -0.5774, train acc: 0.9488, domain acc: 0.5954, lr: 0.000999, lamb: 1.1202, time: 9.1283\n","epoch  72: train D loss: 0.6541, train F loss: -0.5441, train acc: 0.9396, domain acc: 0.6399, lr: 0.000999, lamb: 1.1238, time: 9.3763\n","epoch  73: train D loss: 0.6644, train F loss: -0.5892, train acc: 0.9566, domain acc: 0.6176, lr: 0.000998, lamb: 1.1275, time: 9.5239\n","epoch  74: train D loss: 0.6562, train F loss: -0.5793, train acc: 0.9512, domain acc: 0.6333, lr: 0.000997, lamb: 1.1310, time: 9.6225\n","epoch  75: train D loss: 0.6526, train F loss: -0.5493, train acc: 0.9420, domain acc: 0.6388, lr: 0.000997, lamb: 1.1346, time: 10.6012\n","epoch  76: train D loss: 0.6617, train F loss: -0.5817, train acc: 0.9492, domain acc: 0.6200, lr: 0.000997, lamb: 1.1381, time: 9.8188\n","epoch  77: train D loss: 0.6686, train F loss: -0.6113, train acc: 0.9550, domain acc: 0.6061, lr: 0.000996, lamb: 1.1415, time: 10.1153\n","epoch  78: train D loss: 0.6625, train F loss: -0.5817, train acc: 0.9500, domain acc: 0.6234, lr: 0.000996, lamb: 1.1449, time: 9.5155\n","epoch  79: train D loss: 0.6460, train F loss: -0.5779, train acc: 0.9522, domain acc: 0.6469, lr: 0.000995, lamb: 1.1482, time: 9.9801\n","epoch  80: train D loss: 0.6687, train F loss: -0.6104, train acc: 0.9510, domain acc: 0.6142, lr: 0.000994, lamb: 1.1515, time: 10.7347\n","epoch  81: train D loss: 0.6589, train F loss: -0.6057, train acc: 0.9548, domain acc: 0.6248, lr: 0.000993, lamb: 1.1548, time: 10.0354\n","epoch  82: train D loss: 0.6610, train F loss: -0.6185, train acc: 0.9564, domain acc: 0.6207, lr: 0.000992, lamb: 1.1580, time: 10.3257\n","epoch  83: train D loss: 0.6619, train F loss: -0.6026, train acc: 0.9510, domain acc: 0.6201, lr: 0.000991, lamb: 1.1612, time: 9.4556\n","epoch  84: train D loss: 0.6622, train F loss: -0.6175, train acc: 0.9544, domain acc: 0.6251, lr: 0.000990, lamb: 1.1644, time: 9.6459\n","epoch  85: train D loss: 0.6607, train F loss: -0.6133, train acc: 0.9510, domain acc: 0.6276, lr: 0.000989, lamb: 1.1675, time: 9.3375\n","epoch  86: train D loss: 0.6625, train F loss: -0.6286, train acc: 0.9534, domain acc: 0.6186, lr: 0.000988, lamb: 1.1705, time: 9.4759\n","epoch  87: train D loss: 0.6570, train F loss: -0.6198, train acc: 0.9560, domain acc: 0.6323, lr: 0.000987, lamb: 1.1736, time: 9.4741\n","epoch  88: train D loss: 0.6732, train F loss: -0.6186, train acc: 0.9474, domain acc: 0.5938, lr: 0.000987, lamb: 1.1766, time: 9.1934\n","epoch  89: train D loss: 0.6570, train F loss: -0.6252, train acc: 0.9552, domain acc: 0.6263, lr: 0.000986, lamb: 1.1796, time: 9.6512\n","epoch  90: train D loss: 0.6701, train F loss: -0.6458, train acc: 0.9524, domain acc: 0.6049, lr: 0.000985, lamb: 1.1825, time: 9.2379\n","epoch  91: train D loss: 0.6567, train F loss: -0.6250, train acc: 0.9552, domain acc: 0.6260, lr: 0.000984, lamb: 1.1854, time: 9.5815\n","epoch  92: train D loss: 0.6567, train F loss: -0.6233, train acc: 0.9512, domain acc: 0.6272, lr: 0.000983, lamb: 1.1883, time: 9.3173\n","epoch  93: train D loss: 0.6619, train F loss: -0.6416, train acc: 0.9578, domain acc: 0.6216, lr: 0.000982, lamb: 1.1911, time: 9.3919\n","epoch  94: train D loss: 0.6593, train F loss: -0.6340, train acc: 0.9536, domain acc: 0.6295, lr: 0.000981, lamb: 1.1939, time: 9.5315\n","epoch  95: train D loss: 0.6665, train F loss: -0.6529, train acc: 0.9556, domain acc: 0.6101, lr: 0.000980, lamb: 1.1967, time: 9.3012\n","epoch  96: train D loss: 0.6535, train F loss: -0.6404, train acc: 0.9598, domain acc: 0.6361, lr: 0.000979, lamb: 1.1994, time: 10.6975\n","epoch  97: train D loss: 0.6602, train F loss: -0.6746, train acc: 0.9672, domain acc: 0.6278, lr: 0.000978, lamb: 1.2022, time: 9.6438\n","epoch  98: train D loss: 0.6596, train F loss: -0.6481, train acc: 0.9562, domain acc: 0.6266, lr: 0.000977, lamb: 1.2049, time: 9.6245\n","epoch  99: train D loss: 0.6625, train F loss: -0.6614, train acc: 0.9584, domain acc: 0.6256, lr: 0.000976, lamb: 1.2075, time: 9.5207\n","epoch 100: train D loss: 0.6565, train F loss: -0.6440, train acc: 0.9582, domain acc: 0.6288, lr: 0.000975, lamb: 1.2102, time: 9.6244\n","epoch 101: train D loss: 0.6667, train F loss: -0.6464, train acc: 0.9506, domain acc: 0.6104, lr: 0.000974, lamb: 1.2128, time: 10.1801\n","epoch 102: train D loss: 0.6643, train F loss: -0.6738, train acc: 0.9610, domain acc: 0.6189, lr: 0.000973, lamb: 1.2154, time: 9.4917\n","epoch 103: train D loss: 0.6637, train F loss: -0.6721, train acc: 0.9566, domain acc: 0.6203, lr: 0.000972, lamb: 1.2179, time: 9.7162\n","epoch 104: train D loss: 0.6547, train F loss: -0.6530, train acc: 0.9590, domain acc: 0.6341, lr: 0.000971, lamb: 1.2205, time: 9.6460\n","epoch 105: train D loss: 0.6643, train F loss: -0.6821, train acc: 0.9626, domain acc: 0.6184, lr: 0.000970, lamb: 1.2230, time: 9.3972\n","epoch 106: train D loss: 0.6601, train F loss: -0.6912, train acc: 0.9662, domain acc: 0.6233, lr: 0.000969, lamb: 1.2255, time: 9.7429\n","epoch 107: train D loss: 0.6542, train F loss: -0.6734, train acc: 0.9602, domain acc: 0.6369, lr: 0.000968, lamb: 1.2280, time: 9.4827\n","epoch 108: train D loss: 0.6684, train F loss: -0.6877, train acc: 0.9598, domain acc: 0.6107, lr: 0.000968, lamb: 1.2304, time: 10.7957\n","epoch 109: train D loss: 0.6536, train F loss: -0.6728, train acc: 0.9622, domain acc: 0.6334, lr: 0.000967, lamb: 1.2328, time: 9.9616\n","epoch 110: train D loss: 0.6713, train F loss: -0.7236, train acc: 0.9702, domain acc: 0.6003, lr: 0.000966, lamb: 1.2352, time: 9.9650\n","epoch 111: train D loss: 0.6600, train F loss: -0.6841, train acc: 0.9590, domain acc: 0.6218, lr: 0.000965, lamb: 1.2376, time: 10.1213\n","epoch 112: train D loss: 0.6700, train F loss: -0.6986, train acc: 0.9584, domain acc: 0.6030, lr: 0.000964, lamb: 1.2400, time: 9.5926\n","epoch 113: train D loss: 0.6608, train F loss: -0.6895, train acc: 0.9602, domain acc: 0.6211, lr: 0.000963, lamb: 1.2423, time: 10.1909\n","epoch 114: train D loss: 0.6625, train F loss: -0.6978, train acc: 0.9650, domain acc: 0.6183, lr: 0.000962, lamb: 1.2446, time: 10.1970\n","epoch 115: train D loss: 0.6568, train F loss: -0.6785, train acc: 0.9592, domain acc: 0.6293, lr: 0.000961, lamb: 1.2469, time: 9.8668\n","epoch 116: train D loss: 0.6645, train F loss: -0.7138, train acc: 0.9682, domain acc: 0.6148, lr: 0.000960, lamb: 1.2492, time: 9.5093\n","epoch 117: train D loss: 0.6618, train F loss: -0.7038, train acc: 0.9658, domain acc: 0.6235, lr: 0.000959, lamb: 1.2514, time: 9.3032\n","epoch 118: train D loss: 0.6583, train F loss: -0.6984, train acc: 0.9620, domain acc: 0.6262, lr: 0.000958, lamb: 1.2537, time: 9.3085\n","epoch 119: train D loss: 0.6611, train F loss: -0.6989, train acc: 0.9622, domain acc: 0.6247, lr: 0.000957, lamb: 1.2559, time: 9.2889\n","epoch 120: train D loss: 0.6641, train F loss: -0.7165, train acc: 0.9634, domain acc: 0.6101, lr: 0.000956, lamb: 1.2581, time: 9.4206\n","epoch 121: train D loss: 0.6679, train F loss: -0.7247, train acc: 0.9686, domain acc: 0.6091, lr: 0.000955, lamb: 1.2603, time: 9.3287\n","epoch 122: train D loss: 0.6664, train F loss: -0.7268, train acc: 0.9676, domain acc: 0.6141, lr: 0.000954, lamb: 1.2624, time: 9.2794\n","epoch 123: train D loss: 0.6650, train F loss: -0.7042, train acc: 0.9584, domain acc: 0.6139, lr: 0.000953, lamb: 1.2646, time: 9.1873\n","epoch 124: train D loss: 0.6630, train F loss: -0.7230, train acc: 0.9632, domain acc: 0.6139, lr: 0.000952, lamb: 1.2667, time: 9.2205\n","epoch 125: train D loss: 0.6589, train F loss: -0.7230, train acc: 0.9686, domain acc: 0.6229, lr: 0.000951, lamb: 1.2688, time: 9.3327\n","epoch 126: train D loss: 0.6612, train F loss: -0.7149, train acc: 0.9634, domain acc: 0.6221, lr: 0.000950, lamb: 1.2709, time: 9.1954\n","epoch 127: train D loss: 0.6647, train F loss: -0.7406, train acc: 0.9686, domain acc: 0.6114, lr: 0.000949, lamb: 1.2730, time: 9.2704\n","epoch 128: train D loss: 0.6621, train F loss: -0.7432, train acc: 0.9758, domain acc: 0.6216, lr: 0.000948, lamb: 1.2750, time: 9.3090\n","epoch 129: train D loss: 0.6551, train F loss: -0.7127, train acc: 0.9620, domain acc: 0.6351, lr: 0.000947, lamb: 1.2771, time: 9.2332\n","epoch 130: train D loss: 0.6658, train F loss: -0.7307, train acc: 0.9656, domain acc: 0.6164, lr: 0.000946, lamb: 1.2791, time: 9.2541\n","epoch 131: train D loss: 0.6597, train F loss: -0.7189, train acc: 0.9622, domain acc: 0.6230, lr: 0.000946, lamb: 1.2811, time: 9.2138\n","epoch 132: train D loss: 0.6563, train F loss: -0.7329, train acc: 0.9718, domain acc: 0.6285, lr: 0.000945, lamb: 1.2831, time: 9.2177\n","epoch 133: train D loss: 0.6623, train F loss: -0.7286, train acc: 0.9658, domain acc: 0.6236, lr: 0.000944, lamb: 1.2851, time: 9.2095\n","epoch 134: train D loss: 0.6661, train F loss: -0.7383, train acc: 0.9638, domain acc: 0.6108, lr: 0.000943, lamb: 1.2871, time: 9.1784\n","epoch 135: train D loss: 0.6651, train F loss: -0.7369, train acc: 0.9674, domain acc: 0.6152, lr: 0.000942, lamb: 1.2890, time: 9.3480\n","epoch 136: train D loss: 0.6615, train F loss: -0.7324, train acc: 0.9684, domain acc: 0.6281, lr: 0.000941, lamb: 1.2910, time: 9.2917\n","epoch 137: train D loss: 0.6602, train F loss: -0.7307, train acc: 0.9610, domain acc: 0.6229, lr: 0.000940, lamb: 1.2929, time: 9.0201\n","epoch 138: train D loss: 0.6659, train F loss: -0.7511, train acc: 0.9694, domain acc: 0.6128, lr: 0.000939, lamb: 1.2948, time: 9.0931\n","epoch 139: train D loss: 0.6660, train F loss: -0.7693, train acc: 0.9736, domain acc: 0.6095, lr: 0.000938, lamb: 1.2967, time: 9.0256\n","epoch 140: train D loss: 0.6610, train F loss: -0.7496, train acc: 0.9698, domain acc: 0.6194, lr: 0.000937, lamb: 1.2986, time: 9.0059\n","epoch 141: train D loss: 0.6611, train F loss: -0.7345, train acc: 0.9668, domain acc: 0.6196, lr: 0.000936, lamb: 1.3005, time: 9.1587\n","epoch 142: train D loss: 0.6617, train F loss: -0.7419, train acc: 0.9678, domain acc: 0.6150, lr: 0.000935, lamb: 1.3023, time: 9.4476\n","epoch 143: train D loss: 0.6670, train F loss: -0.7594, train acc: 0.9670, domain acc: 0.6100, lr: 0.000934, lamb: 1.3042, time: 9.3208\n","epoch 144: train D loss: 0.6645, train F loss: -0.7629, train acc: 0.9690, domain acc: 0.6165, lr: 0.000933, lamb: 1.3060, time: 9.1167\n","epoch 145: train D loss: 0.6584, train F loss: -0.7404, train acc: 0.9640, domain acc: 0.6243, lr: 0.000932, lamb: 1.3078, time: 9.0984\n","epoch 146: train D loss: 0.6722, train F loss: -0.7903, train acc: 0.9734, domain acc: 0.6035, lr: 0.000931, lamb: 1.3096, time: 9.1046\n","epoch 147: train D loss: 0.6587, train F loss: -0.7545, train acc: 0.9708, domain acc: 0.6215, lr: 0.000930, lamb: 1.3114, time: 9.5574\n","epoch 148: train D loss: 0.6654, train F loss: -0.7738, train acc: 0.9724, domain acc: 0.6106, lr: 0.000930, lamb: 1.3132, time: 9.3368\n","epoch 149: train D loss: 0.6615, train F loss: -0.7560, train acc: 0.9650, domain acc: 0.6219, lr: 0.000929, lamb: 1.3150, time: 9.1559\n","epoch 150: train D loss: 0.6631, train F loss: -0.7449, train acc: 0.9636, domain acc: 0.6252, lr: 0.000928, lamb: 1.3167, time: 9.6326\n","epoch 151: train D loss: 0.6656, train F loss: -0.7734, train acc: 0.9728, domain acc: 0.6195, lr: 0.000927, lamb: 1.3185, time: 10.5082\n","epoch 152: train D loss: 0.6662, train F loss: -0.7813, train acc: 0.9718, domain acc: 0.6140, lr: 0.000926, lamb: 1.3202, time: 9.4487\n","epoch 153: train D loss: 0.6630, train F loss: -0.7633, train acc: 0.9648, domain acc: 0.6219, lr: 0.000925, lamb: 1.3219, time: 9.4366\n","epoch 154: train D loss: 0.6718, train F loss: -0.7886, train acc: 0.9698, domain acc: 0.6020, lr: 0.000924, lamb: 1.3236, time: 9.6165\n","epoch 155: train D loss: 0.6685, train F loss: -0.7916, train acc: 0.9724, domain acc: 0.6076, lr: 0.000923, lamb: 1.3253, time: 9.4336\n","epoch 156: train D loss: 0.6669, train F loss: -0.7727, train acc: 0.9688, domain acc: 0.6093, lr: 0.000922, lamb: 1.3270, time: 9.3800\n","epoch 157: train D loss: 0.6713, train F loss: -0.7854, train acc: 0.9686, domain acc: 0.5970, lr: 0.000921, lamb: 1.3287, time: 9.4258\n","epoch 158: train D loss: 0.6577, train F loss: -0.7841, train acc: 0.9750, domain acc: 0.6221, lr: 0.000920, lamb: 1.3304, time: 9.4628\n","epoch 159: train D loss: 0.6661, train F loss: -0.7963, train acc: 0.9756, domain acc: 0.6158, lr: 0.000919, lamb: 1.3320, time: 9.5000\n","epoch 160: train D loss: 0.6622, train F loss: -0.7837, train acc: 0.9728, domain acc: 0.6212, lr: 0.000918, lamb: 1.3337, time: 9.5631\n","epoch 161: train D loss: 0.6601, train F loss: -0.7617, train acc: 0.9674, domain acc: 0.6243, lr: 0.000918, lamb: 1.3353, time: 9.5488\n","epoch 162: train D loss: 0.6732, train F loss: -0.7977, train acc: 0.9684, domain acc: 0.5945, lr: 0.000917, lamb: 1.3369, time: 9.4553\n","epoch 163: train D loss: 0.6686, train F loss: -0.7744, train acc: 0.9704, domain acc: 0.6010, lr: 0.000916, lamb: 1.3386, time: 9.3495\n","epoch 164: train D loss: 0.6671, train F loss: -0.7895, train acc: 0.9668, domain acc: 0.6103, lr: 0.000915, lamb: 1.3402, time: 9.5617\n","epoch 165: train D loss: 0.6662, train F loss: -0.7907, train acc: 0.9694, domain acc: 0.6154, lr: 0.000914, lamb: 1.3418, time: 9.5170\n","epoch 166: train D loss: 0.6627, train F loss: -0.7806, train acc: 0.9680, domain acc: 0.6193, lr: 0.000913, lamb: 1.3434, time: 9.3986\n","epoch 167: train D loss: 0.6639, train F loss: -0.7939, train acc: 0.9750, domain acc: 0.6187, lr: 0.000912, lamb: 1.3449, time: 9.8648\n","epoch 168: train D loss: 0.6662, train F loss: -0.8069, train acc: 0.9742, domain acc: 0.6127, lr: 0.000911, lamb: 1.3465, time: 9.6039\n","epoch 169: train D loss: 0.6604, train F loss: -0.7891, train acc: 0.9746, domain acc: 0.6223, lr: 0.000910, lamb: 1.3481, time: 9.5088\n","epoch 170: train D loss: 0.6731, train F loss: -0.8151, train acc: 0.9730, domain acc: 0.5961, lr: 0.000909, lamb: 1.3496, time: 9.7531\n","epoch 171: train D loss: 0.6698, train F loss: -0.8255, train acc: 0.9772, domain acc: 0.6030, lr: 0.000908, lamb: 1.3512, time: 10.4106\n","epoch 172: train D loss: 0.6678, train F loss: -0.8148, train acc: 0.9732, domain acc: 0.6045, lr: 0.000908, lamb: 1.3527, time: 10.2812\n","epoch 173: train D loss: 0.6699, train F loss: -0.8152, train acc: 0.9728, domain acc: 0.6072, lr: 0.000907, lamb: 1.3542, time: 9.7024\n","epoch 174: train D loss: 0.6696, train F loss: -0.8122, train acc: 0.9724, domain acc: 0.6071, lr: 0.000906, lamb: 1.3557, time: 10.1541\n","epoch 175: train D loss: 0.6641, train F loss: -0.8108, train acc: 0.9746, domain acc: 0.6181, lr: 0.000905, lamb: 1.3572, time: 9.6272\n","epoch 176: train D loss: 0.6617, train F loss: -0.8007, train acc: 0.9712, domain acc: 0.6212, lr: 0.000904, lamb: 1.3587, time: 9.3234\n","epoch 177: train D loss: 0.6688, train F loss: -0.8043, train acc: 0.9714, domain acc: 0.6026, lr: 0.000903, lamb: 1.3602, time: 9.5586\n","epoch 178: train D loss: 0.6617, train F loss: -0.8152, train acc: 0.9740, domain acc: 0.6167, lr: 0.000902, lamb: 1.3617, time: 9.6805\n","epoch 179: train D loss: 0.6703, train F loss: -0.8121, train acc: 0.9706, domain acc: 0.6025, lr: 0.000901, lamb: 1.3632, time: 9.4683\n","epoch 180: train D loss: 0.6585, train F loss: -0.8052, train acc: 0.9780, domain acc: 0.6268, lr: 0.000900, lamb: 1.3646, time: 9.1170\n","epoch 181: train D loss: 0.6651, train F loss: -0.8116, train acc: 0.9742, domain acc: 0.6162, lr: 0.000899, lamb: 1.3661, time: 9.3229\n","epoch 182: train D loss: 0.6666, train F loss: -0.8178, train acc: 0.9720, domain acc: 0.6098, lr: 0.000898, lamb: 1.3675, time: 9.3748\n","epoch 183: train D loss: 0.6677, train F loss: -0.8165, train acc: 0.9718, domain acc: 0.6084, lr: 0.000898, lamb: 1.3690, time: 9.1751\n","epoch 184: train D loss: 0.6692, train F loss: -0.8349, train acc: 0.9754, domain acc: 0.6013, lr: 0.000897, lamb: 1.3704, time: 9.1354\n","epoch 185: train D loss: 0.6730, train F loss: -0.8421, train acc: 0.9778, domain acc: 0.5938, lr: 0.000896, lamb: 1.3718, time: 9.2216\n","epoch 186: train D loss: 0.6647, train F loss: -0.8360, train acc: 0.9788, domain acc: 0.6103, lr: 0.000895, lamb: 1.3733, time: 9.3552\n","epoch 187: train D loss: 0.6775, train F loss: -0.8534, train acc: 0.9780, domain acc: 0.5835, lr: 0.000894, lamb: 1.3747, time: 9.3559\n","epoch 188: train D loss: 0.6725, train F loss: -0.8327, train acc: 0.9746, domain acc: 0.5977, lr: 0.000893, lamb: 1.3761, time: 9.3980\n","epoch 189: train D loss: 0.6655, train F loss: -0.8238, train acc: 0.9732, domain acc: 0.6113, lr: 0.000892, lamb: 1.3775, time: 9.3366\n","epoch 190: train D loss: 0.6706, train F loss: -0.8428, train acc: 0.9762, domain acc: 0.5984, lr: 0.000891, lamb: 1.3788, time: 9.6670\n","epoch 191: train D loss: 0.6726, train F loss: -0.8353, train acc: 0.9710, domain acc: 0.6016, lr: 0.000890, lamb: 1.3802, time: 9.4319\n","epoch 192: train D loss: 0.6651, train F loss: -0.8401, train acc: 0.9772, domain acc: 0.6141, lr: 0.000890, lamb: 1.3816, time: 9.3205\n","epoch 193: train D loss: 0.6603, train F loss: -0.8333, train acc: 0.9806, domain acc: 0.6204, lr: 0.000889, lamb: 1.3830, time: 9.4419\n","epoch 194: train D loss: 0.6715, train F loss: -0.8380, train acc: 0.9738, domain acc: 0.6047, lr: 0.000888, lamb: 1.3843, time: 9.2755\n","epoch 195: train D loss: 0.6724, train F loss: -0.8316, train acc: 0.9722, domain acc: 0.5958, lr: 0.000887, lamb: 1.3857, time: 9.2639\n","epoch 196: train D loss: 0.6669, train F loss: -0.8437, train acc: 0.9762, domain acc: 0.6076, lr: 0.000886, lamb: 1.3870, time: 9.3211\n","epoch 197: train D loss: 0.6664, train F loss: -0.8318, train acc: 0.9746, domain acc: 0.6082, lr: 0.000885, lamb: 1.3884, time: 9.2195\n","epoch 198: train D loss: 0.6698, train F loss: -0.8378, train acc: 0.9716, domain acc: 0.6046, lr: 0.000884, lamb: 1.3897, time: 9.4108\n","epoch 199: train D loss: 0.6749, train F loss: -0.8515, train acc: 0.9738, domain acc: 0.5952, lr: 0.000883, lamb: 1.3910, time: 9.6594\n","epoch 200: train D loss: 0.6699, train F loss: -0.8542, train acc: 0.9770, domain acc: 0.6028, lr: 0.000882, lamb: 1.3923, time: 9.7024\n","epoch 201: train D loss: 0.6646, train F loss: -0.8413, train acc: 0.9746, domain acc: 0.6126, lr: 0.000882, lamb: 1.3936, time: 9.6089\n","epoch 202: train D loss: 0.6659, train F loss: -0.8335, train acc: 0.9738, domain acc: 0.6109, lr: 0.000881, lamb: 1.3949, time: 9.4010\n","epoch 203: train D loss: 0.6706, train F loss: -0.8351, train acc: 0.9704, domain acc: 0.6101, lr: 0.000880, lamb: 1.3962, time: 9.6916\n","epoch 204: train D loss: 0.6676, train F loss: -0.8554, train acc: 0.9800, domain acc: 0.6128, lr: 0.000879, lamb: 1.3975, time: 9.4803\n","epoch 205: train D loss: 0.6702, train F loss: -0.8542, train acc: 0.9774, domain acc: 0.6057, lr: 0.000878, lamb: 1.3988, time: 9.8059\n","epoch 206: train D loss: 0.6657, train F loss: -0.8401, train acc: 0.9710, domain acc: 0.6061, lr: 0.000877, lamb: 1.4001, time: 9.6089\n","epoch 207: train D loss: 0.6703, train F loss: -0.8433, train acc: 0.9736, domain acc: 0.6081, lr: 0.000876, lamb: 1.4014, time: 9.3835\n","epoch 208: train D loss: 0.6756, train F loss: -0.8684, train acc: 0.9786, domain acc: 0.5873, lr: 0.000875, lamb: 1.4026, time: 9.5319\n","epoch 209: train D loss: 0.6679, train F loss: -0.8574, train acc: 0.9788, domain acc: 0.6090, lr: 0.000875, lamb: 1.4039, time: 9.9833\n","epoch 210: train D loss: 0.6669, train F loss: -0.8509, train acc: 0.9754, domain acc: 0.6053, lr: 0.000874, lamb: 1.4051, time: 9.7650\n","epoch 211: train D loss: 0.6675, train F loss: -0.8523, train acc: 0.9782, domain acc: 0.6131, lr: 0.000873, lamb: 1.4064, time: 9.6289\n","epoch 212: train D loss: 0.6749, train F loss: -0.8646, train acc: 0.9730, domain acc: 0.5924, lr: 0.000872, lamb: 1.4076, time: 9.5122\n","epoch 213: train D loss: 0.6702, train F loss: -0.8681, train acc: 0.9780, domain acc: 0.6052, lr: 0.000871, lamb: 1.4089, time: 9.4912\n","epoch 214: train D loss: 0.6602, train F loss: -0.8446, train acc: 0.9770, domain acc: 0.6224, lr: 0.000870, lamb: 1.4101, time: 9.4914\n","epoch 215: train D loss: 0.6747, train F loss: -0.8626, train acc: 0.9756, domain acc: 0.5950, lr: 0.000869, lamb: 1.4113, time: 9.4151\n","epoch 216: train D loss: 0.6705, train F loss: -0.8557, train acc: 0.9732, domain acc: 0.6020, lr: 0.000868, lamb: 1.4125, time: 9.2500\n","epoch 217: train D loss: 0.6733, train F loss: -0.8728, train acc: 0.9768, domain acc: 0.5938, lr: 0.000868, lamb: 1.4138, time: 9.1772\n","epoch 218: train D loss: 0.6670, train F loss: -0.8654, train acc: 0.9804, domain acc: 0.6084, lr: 0.000867, lamb: 1.4150, time: 9.4303\n","epoch 219: train D loss: 0.6661, train F loss: -0.8498, train acc: 0.9720, domain acc: 0.6107, lr: 0.000866, lamb: 1.4162, time: 9.3181\n","epoch 220: train D loss: 0.6715, train F loss: -0.8694, train acc: 0.9760, domain acc: 0.5987, lr: 0.000865, lamb: 1.4174, time: 9.4230\n","epoch 221: train D loss: 0.6673, train F loss: -0.8508, train acc: 0.9734, domain acc: 0.6082, lr: 0.000864, lamb: 1.4186, time: 9.6037\n","epoch 222: train D loss: 0.6705, train F loss: -0.8759, train acc: 0.9790, domain acc: 0.6042, lr: 0.000863, lamb: 1.4197, time: 10.2141\n","epoch 223: train D loss: 0.6752, train F loss: -0.8987, train acc: 0.9828, domain acc: 0.5903, lr: 0.000862, lamb: 1.4209, time: 9.8823\n","epoch 224: train D loss: 0.6656, train F loss: -0.8653, train acc: 0.9788, domain acc: 0.6174, lr: 0.000862, lamb: 1.4221, time: 9.7041\n","epoch 225: train D loss: 0.6746, train F loss: -0.8724, train acc: 0.9742, domain acc: 0.6005, lr: 0.000861, lamb: 1.4233, time: 9.8030\n","epoch 226: train D loss: 0.6681, train F loss: -0.8656, train acc: 0.9764, domain acc: 0.6078, lr: 0.000860, lamb: 1.4244, time: 9.7086\n","epoch 227: train D loss: 0.6689, train F loss: -0.8719, train acc: 0.9772, domain acc: 0.6088, lr: 0.000859, lamb: 1.4256, time: 9.4594\n","epoch 228: train D loss: 0.6638, train F loss: -0.8537, train acc: 0.9784, domain acc: 0.6207, lr: 0.000858, lamb: 1.4268, time: 9.5501\n","epoch 229: train D loss: 0.6721, train F loss: -0.8675, train acc: 0.9752, domain acc: 0.6026, lr: 0.000857, lamb: 1.4279, time: 9.3793\n","epoch 230: train D loss: 0.6712, train F loss: -0.8717, train acc: 0.9740, domain acc: 0.6006, lr: 0.000856, lamb: 1.4291, time: 9.1722\n","epoch 231: train D loss: 0.6689, train F loss: -0.8713, train acc: 0.9768, domain acc: 0.6076, lr: 0.000855, lamb: 1.4302, time: 9.2646\n","epoch 232: train D loss: 0.6673, train F loss: -0.8766, train acc: 0.9786, domain acc: 0.6118, lr: 0.000855, lamb: 1.4313, time: 9.4838\n","epoch 233: train D loss: 0.6649, train F loss: -0.8656, train acc: 0.9774, domain acc: 0.6136, lr: 0.000854, lamb: 1.4325, time: 9.3222\n","epoch 234: train D loss: 0.6697, train F loss: -0.8731, train acc: 0.9762, domain acc: 0.6030, lr: 0.000853, lamb: 1.4336, time: 9.5482\n","epoch 235: train D loss: 0.6702, train F loss: -0.8835, train acc: 0.9778, domain acc: 0.6040, lr: 0.000852, lamb: 1.4347, time: 9.4761\n","epoch 236: train D loss: 0.6664, train F loss: -0.8802, train acc: 0.9790, domain acc: 0.6100, lr: 0.000851, lamb: 1.4358, time: 9.3988\n","epoch 237: train D loss: 0.6710, train F loss: -0.8810, train acc: 0.9764, domain acc: 0.5991, lr: 0.000850, lamb: 1.4369, time: 9.1213\n","epoch 238: train D loss: 0.6641, train F loss: -0.8704, train acc: 0.9784, domain acc: 0.6226, lr: 0.000850, lamb: 1.4380, time: 9.1575\n","epoch 239: train D loss: 0.6657, train F loss: -0.8856, train acc: 0.9816, domain acc: 0.6148, lr: 0.000849, lamb: 1.4391, time: 9.3238\n","epoch 240: train D loss: 0.6768, train F loss: -0.8992, train acc: 0.9782, domain acc: 0.5920, lr: 0.000848, lamb: 1.4402, time: 9.5273\n","epoch 241: train D loss: 0.6689, train F loss: -0.8950, train acc: 0.9812, domain acc: 0.6030, lr: 0.000847, lamb: 1.4413, time: 9.1971\n","epoch 242: train D loss: 0.6733, train F loss: -0.8857, train acc: 0.9778, domain acc: 0.5950, lr: 0.000846, lamb: 1.4424, time: 9.1651\n","epoch 243: train D loss: 0.6697, train F loss: -0.8854, train acc: 0.9782, domain acc: 0.6054, lr: 0.000845, lamb: 1.4435, time: 9.1084\n","epoch 244: train D loss: 0.6758, train F loss: -0.8923, train acc: 0.9750, domain acc: 0.5871, lr: 0.000844, lamb: 1.4446, time: 9.3459\n","epoch 245: train D loss: 0.6650, train F loss: -0.8919, train acc: 0.9808, domain acc: 0.6159, lr: 0.000844, lamb: 1.4457, time: 9.3794\n","epoch 246: train D loss: 0.6692, train F loss: -0.8915, train acc: 0.9818, domain acc: 0.6039, lr: 0.000843, lamb: 1.4467, time: 9.6796\n","epoch 247: train D loss: 0.6737, train F loss: -0.8890, train acc: 0.9766, domain acc: 0.5948, lr: 0.000842, lamb: 1.4478, time: 9.6220\n","epoch 248: train D loss: 0.6686, train F loss: -0.8938, train acc: 0.9804, domain acc: 0.6047, lr: 0.000841, lamb: 1.4489, time: 9.6055\n","epoch 249: train D loss: 0.6709, train F loss: -0.8983, train acc: 0.9796, domain acc: 0.6008, lr: 0.000840, lamb: 1.4499, time: 9.5597\n","epoch 250: train D loss: 0.6724, train F loss: -0.9014, train acc: 0.9804, domain acc: 0.5976, lr: 0.000839, lamb: 1.4510, time: 9.4819\n","epoch 251: train D loss: 0.6732, train F loss: -0.8991, train acc: 0.9768, domain acc: 0.5941, lr: 0.000839, lamb: 1.4520, time: 9.4808\n","epoch 252: train D loss: 0.6632, train F loss: -0.8814, train acc: 0.9782, domain acc: 0.6176, lr: 0.000838, lamb: 1.4531, time: 9.4102\n","epoch 253: train D loss: 0.6709, train F loss: -0.9058, train acc: 0.9810, domain acc: 0.6052, lr: 0.000837, lamb: 1.4541, time: 9.4235\n","epoch 254: train D loss: 0.6742, train F loss: -0.8942, train acc: 0.9758, domain acc: 0.5948, lr: 0.000836, lamb: 1.4551, time: 9.5249\n","epoch 255: train D loss: 0.6697, train F loss: -0.8987, train acc: 0.9798, domain acc: 0.6084, lr: 0.000835, lamb: 1.4562, time: 9.5934\n","epoch 256: train D loss: 0.6662, train F loss: -0.8907, train acc: 0.9788, domain acc: 0.6093, lr: 0.000834, lamb: 1.4572, time: 9.5075\n","epoch 257: train D loss: 0.6755, train F loss: -0.9158, train acc: 0.9808, domain acc: 0.5960, lr: 0.000834, lamb: 1.4582, time: 9.2695\n","epoch 258: train D loss: 0.6683, train F loss: -0.8999, train acc: 0.9792, domain acc: 0.6076, lr: 0.000833, lamb: 1.4592, time: 9.6296\n","epoch 259: train D loss: 0.6720, train F loss: -0.8942, train acc: 0.9768, domain acc: 0.5928, lr: 0.000832, lamb: 1.4603, time: 9.3833\n","epoch 260: train D loss: 0.6687, train F loss: -0.9125, train acc: 0.9840, domain acc: 0.6058, lr: 0.000831, lamb: 1.4613, time: 9.4710\n","epoch 261: train D loss: 0.6704, train F loss: -0.8909, train acc: 0.9738, domain acc: 0.6043, lr: 0.000830, lamb: 1.4623, time: 9.5340\n","epoch 262: train D loss: 0.6755, train F loss: -0.9171, train acc: 0.9826, domain acc: 0.5953, lr: 0.000829, lamb: 1.4633, time: 9.4982\n","epoch 263: train D loss: 0.6716, train F loss: -0.9061, train acc: 0.9768, domain acc: 0.5981, lr: 0.000829, lamb: 1.4643, time: 9.5785\n","epoch 264: train D loss: 0.6678, train F loss: -0.9010, train acc: 0.9816, domain acc: 0.6037, lr: 0.000828, lamb: 1.4653, time: 9.4845\n","epoch 265: train D loss: 0.6736, train F loss: -0.9108, train acc: 0.9808, domain acc: 0.5953, lr: 0.000827, lamb: 1.4663, time: 9.3805\n","epoch 266: train D loss: 0.6678, train F loss: -0.9106, train acc: 0.9820, domain acc: 0.6108, lr: 0.000826, lamb: 1.4673, time: 9.3135\n","epoch 267: train D loss: 0.6717, train F loss: -0.9058, train acc: 0.9764, domain acc: 0.6050, lr: 0.000825, lamb: 1.4683, time: 9.6020\n","epoch 268: train D loss: 0.6713, train F loss: -0.9077, train acc: 0.9764, domain acc: 0.5969, lr: 0.000824, lamb: 1.4692, time: 9.4012\n","epoch 269: train D loss: 0.6691, train F loss: -0.9105, train acc: 0.9780, domain acc: 0.6064, lr: 0.000824, lamb: 1.4702, time: 9.3768\n","epoch 270: train D loss: 0.6679, train F loss: -0.9161, train acc: 0.9820, domain acc: 0.6056, lr: 0.000823, lamb: 1.4712, time: 9.5166\n","epoch 271: train D loss: 0.6740, train F loss: -0.9137, train acc: 0.9804, domain acc: 0.5984, lr: 0.000822, lamb: 1.4722, time: 9.5016\n","epoch 272: train D loss: 0.6722, train F loss: -0.9157, train acc: 0.9788, domain acc: 0.6015, lr: 0.000821, lamb: 1.4731, time: 9.3935\n","epoch 273: train D loss: 0.6728, train F loss: -0.9217, train acc: 0.9788, domain acc: 0.6016, lr: 0.000820, lamb: 1.4741, time: 9.5833\n","epoch 274: train D loss: 0.6700, train F loss: -0.9059, train acc: 0.9786, domain acc: 0.6000, lr: 0.000819, lamb: 1.4751, time: 9.4728\n","epoch 275: train D loss: 0.6733, train F loss: -0.9261, train acc: 0.9826, domain acc: 0.5959, lr: 0.000819, lamb: 1.4760, time: 9.3813\n","epoch 276: train D loss: 0.6724, train F loss: -0.9291, train acc: 0.9826, domain acc: 0.5968, lr: 0.000818, lamb: 1.4770, time: 9.4413\n","epoch 277: train D loss: 0.6709, train F loss: -0.9233, train acc: 0.9816, domain acc: 0.6025, lr: 0.000817, lamb: 1.4779, time: 9.6011\n","epoch 278: train D loss: 0.6701, train F loss: -0.9274, train acc: 0.9828, domain acc: 0.6047, lr: 0.000816, lamb: 1.4789, time: 9.5566\n","epoch 279: train D loss: 0.6693, train F loss: -0.9041, train acc: 0.9780, domain acc: 0.6080, lr: 0.000815, lamb: 1.4798, time: 9.3762\n","epoch 280: train D loss: 0.6695, train F loss: -0.9204, train acc: 0.9806, domain acc: 0.6058, lr: 0.000815, lamb: 1.4807, time: 9.2533\n","epoch 281: train D loss: 0.6719, train F loss: -0.9154, train acc: 0.9762, domain acc: 0.5972, lr: 0.000814, lamb: 1.4817, time: 9.3338\n","epoch 282: train D loss: 0.6719, train F loss: -0.9350, train acc: 0.9822, domain acc: 0.6050, lr: 0.000813, lamb: 1.4826, time: 9.3214\n","epoch 283: train D loss: 0.6755, train F loss: -0.9220, train acc: 0.9798, domain acc: 0.5888, lr: 0.000812, lamb: 1.4835, time: 9.3158\n","epoch 284: train D loss: 0.6736, train F loss: -0.9362, train acc: 0.9816, domain acc: 0.5967, lr: 0.000811, lamb: 1.4845, time: 9.3788\n","epoch 285: train D loss: 0.6677, train F loss: -0.9179, train acc: 0.9804, domain acc: 0.6081, lr: 0.000810, lamb: 1.4854, time: 9.5280\n","epoch 286: train D loss: 0.6762, train F loss: -0.9458, train acc: 0.9832, domain acc: 0.5980, lr: 0.000810, lamb: 1.4863, time: 9.5834\n","epoch 287: train D loss: 0.6725, train F loss: -0.9400, train acc: 0.9844, domain acc: 0.5905, lr: 0.000809, lamb: 1.4872, time: 10.2908\n","epoch 288: train D loss: 0.6675, train F loss: -0.9235, train acc: 0.9792, domain acc: 0.6117, lr: 0.000808, lamb: 1.4881, time: 10.3569\n","epoch 289: train D loss: 0.6755, train F loss: -0.9428, train acc: 0.9850, domain acc: 0.5985, lr: 0.000807, lamb: 1.4891, time: 9.9764\n","epoch 290: train D loss: 0.6735, train F loss: -0.9446, train acc: 0.9834, domain acc: 0.5956, lr: 0.000806, lamb: 1.4900, time: 9.5316\n","epoch 291: train D loss: 0.6758, train F loss: -0.9383, train acc: 0.9808, domain acc: 0.5882, lr: 0.000806, lamb: 1.4909, time: 9.5683\n","epoch 292: train D loss: 0.6699, train F loss: -0.9273, train acc: 0.9806, domain acc: 0.5963, lr: 0.000805, lamb: 1.4918, time: 9.5309\n","epoch 293: train D loss: 0.6701, train F loss: -0.9392, train acc: 0.9834, domain acc: 0.6037, lr: 0.000804, lamb: 1.4927, time: 9.7286\n","epoch 294: train D loss: 0.6723, train F loss: -0.9272, train acc: 0.9800, domain acc: 0.5924, lr: 0.000803, lamb: 1.4936, time: 9.4786\n","epoch 295: train D loss: 0.6693, train F loss: -0.9376, train acc: 0.9818, domain acc: 0.5964, lr: 0.000802, lamb: 1.4945, time: 9.2937\n","epoch 296: train D loss: 0.6701, train F loss: -0.9400, train acc: 0.9840, domain acc: 0.6041, lr: 0.000802, lamb: 1.4953, time: 9.3328\n","epoch 297: train D loss: 0.6676, train F loss: -0.9355, train acc: 0.9806, domain acc: 0.6078, lr: 0.000801, lamb: 1.4962, time: 9.3626\n","epoch 298: train D loss: 0.6747, train F loss: -0.9363, train acc: 0.9774, domain acc: 0.5958, lr: 0.000800, lamb: 1.4971, time: 9.3968\n","epoch 299: train D loss: 0.6737, train F loss: -0.9439, train acc: 0.9816, domain acc: 0.5939, lr: 0.000799, lamb: 1.4980, time: 9.5097\n","epoch 300: train D loss: 0.6688, train F loss: -0.9376, train acc: 0.9832, domain acc: 0.6083, lr: 0.000798, lamb: 1.4989, time: 9.3563\n","epoch 301: train D loss: 0.6732, train F loss: -0.9434, train acc: 0.9824, domain acc: 0.6000, lr: 0.000798, lamb: 1.4997, time: 9.3916\n","epoch 302: train D loss: 0.6726, train F loss: -0.9551, train acc: 0.9864, domain acc: 0.6017, lr: 0.000797, lamb: 1.5006, time: 9.5478\n","epoch 303: train D loss: 0.6737, train F loss: -0.9388, train acc: 0.9782, domain acc: 0.5988, lr: 0.000796, lamb: 1.5015, time: 9.5001\n","epoch 304: train D loss: 0.6733, train F loss: -0.9370, train acc: 0.9806, domain acc: 0.5927, lr: 0.000795, lamb: 1.5024, time: 9.5088\n","epoch 305: train D loss: 0.6690, train F loss: -0.9279, train acc: 0.9778, domain acc: 0.6042, lr: 0.000794, lamb: 1.5032, time: 9.2362\n","epoch 306: train D loss: 0.6719, train F loss: -0.9390, train acc: 0.9806, domain acc: 0.6049, lr: 0.000794, lamb: 1.5041, time: 9.1329\n","epoch 307: train D loss: 0.6682, train F loss: -0.9432, train acc: 0.9858, domain acc: 0.6087, lr: 0.000793, lamb: 1.5049, time: 9.1595\n","epoch 308: train D loss: 0.6766, train F loss: -0.9469, train acc: 0.9800, domain acc: 0.5893, lr: 0.000792, lamb: 1.5058, time: 9.1764\n","epoch 309: train D loss: 0.6728, train F loss: -0.9490, train acc: 0.9824, domain acc: 0.5943, lr: 0.000791, lamb: 1.5066, time: 9.2773\n","epoch 310: train D loss: 0.6736, train F loss: -0.9540, train acc: 0.9824, domain acc: 0.5849, lr: 0.000790, lamb: 1.5075, time: 9.5052\n","epoch 311: train D loss: 0.6753, train F loss: -0.9530, train acc: 0.9812, domain acc: 0.5886, lr: 0.000790, lamb: 1.5083, time: 9.4464\n","epoch 312: train D loss: 0.6768, train F loss: -0.9388, train acc: 0.9790, domain acc: 0.5898, lr: 0.000789, lamb: 1.5092, time: 9.6345\n","epoch 313: train D loss: 0.6684, train F loss: -0.9505, train acc: 0.9840, domain acc: 0.6088, lr: 0.000788, lamb: 1.5100, time: 9.4913\n","epoch 314: train D loss: 0.6690, train F loss: -0.9388, train acc: 0.9796, domain acc: 0.6053, lr: 0.000787, lamb: 1.5109, time: 9.3480\n","epoch 315: train D loss: 0.6693, train F loss: -0.9472, train acc: 0.9846, domain acc: 0.6113, lr: 0.000787, lamb: 1.5117, time: 9.5537\n","epoch 316: train D loss: 0.6729, train F loss: -0.9511, train acc: 0.9804, domain acc: 0.6031, lr: 0.000786, lamb: 1.5125, time: 9.4133\n","epoch 317: train D loss: 0.6703, train F loss: -0.9452, train acc: 0.9822, domain acc: 0.6052, lr: 0.000785, lamb: 1.5134, time: 9.6233\n","epoch 318: train D loss: 0.6747, train F loss: -0.9585, train acc: 0.9826, domain acc: 0.5973, lr: 0.000784, lamb: 1.5142, time: 9.5552\n","epoch 319: train D loss: 0.6749, train F loss: -0.9510, train acc: 0.9780, domain acc: 0.5941, lr: 0.000783, lamb: 1.5150, time: 9.3351\n","epoch 320: train D loss: 0.6761, train F loss: -0.9659, train acc: 0.9830, domain acc: 0.5899, lr: 0.000783, lamb: 1.5158, time: 9.3603\n","epoch 321: train D loss: 0.6716, train F loss: -0.9564, train acc: 0.9816, domain acc: 0.6006, lr: 0.000782, lamb: 1.5167, time: 9.4331\n","epoch 322: train D loss: 0.6742, train F loss: -0.9520, train acc: 0.9804, domain acc: 0.5920, lr: 0.000781, lamb: 1.5175, time: 9.4653\n","epoch 323: train D loss: 0.6740, train F loss: -0.9616, train acc: 0.9848, domain acc: 0.5936, lr: 0.000780, lamb: 1.5183, time: 9.3989\n","epoch 324: train D loss: 0.6749, train F loss: -0.9483, train acc: 0.9806, domain acc: 0.5944, lr: 0.000779, lamb: 1.5191, time: 9.1871\n","epoch 325: train D loss: 0.6729, train F loss: -0.9552, train acc: 0.9794, domain acc: 0.6036, lr: 0.000779, lamb: 1.5199, time: 9.1215\n","epoch 326: train D loss: 0.6688, train F loss: -0.9454, train acc: 0.9804, domain acc: 0.6058, lr: 0.000778, lamb: 1.5207, time: 9.1093\n","epoch 327: train D loss: 0.6729, train F loss: -0.9617, train acc: 0.9834, domain acc: 0.5993, lr: 0.000777, lamb: 1.5215, time: 9.1893\n","epoch 328: train D loss: 0.6689, train F loss: -0.9639, train acc: 0.9860, domain acc: 0.6072, lr: 0.000776, lamb: 1.5223, time: 9.2670\n","epoch 329: train D loss: 0.6751, train F loss: -0.9645, train acc: 0.9840, domain acc: 0.5884, lr: 0.000776, lamb: 1.5231, time: 9.1834\n","epoch 330: train D loss: 0.6753, train F loss: -0.9597, train acc: 0.9808, domain acc: 0.5889, lr: 0.000775, lamb: 1.5239, time: 9.3395\n","epoch 331: train D loss: 0.6681, train F loss: -0.9450, train acc: 0.9794, domain acc: 0.5983, lr: 0.000774, lamb: 1.5247, time: 9.3105\n","epoch 332: train D loss: 0.6711, train F loss: -0.9621, train acc: 0.9844, domain acc: 0.5958, lr: 0.000773, lamb: 1.5255, time: 9.5081\n","epoch 333: train D loss: 0.6722, train F loss: -0.9679, train acc: 0.9858, domain acc: 0.6005, lr: 0.000772, lamb: 1.5263, time: 9.1629\n","epoch 334: train D loss: 0.6707, train F loss: -0.9570, train acc: 0.9810, domain acc: 0.5978, lr: 0.000772, lamb: 1.5271, time: 9.4410\n","epoch 335: train D loss: 0.6730, train F loss: -0.9685, train acc: 0.9850, domain acc: 0.5986, lr: 0.000771, lamb: 1.5279, time: 9.5766\n","epoch 336: train D loss: 0.6800, train F loss: -0.9801, train acc: 0.9836, domain acc: 0.5765, lr: 0.000770, lamb: 1.5287, time: 9.4240\n","epoch 337: train D loss: 0.6757, train F loss: -0.9682, train acc: 0.9810, domain acc: 0.5857, lr: 0.000769, lamb: 1.5294, time: 9.2819\n","epoch 338: train D loss: 0.6694, train F loss: -0.9536, train acc: 0.9810, domain acc: 0.6043, lr: 0.000769, lamb: 1.5302, time: 9.3034\n","epoch 339: train D loss: 0.6755, train F loss: -0.9810, train acc: 0.9864, domain acc: 0.5977, lr: 0.000768, lamb: 1.5310, time: 9.3859\n","epoch 340: train D loss: 0.6719, train F loss: -0.9755, train acc: 0.9864, domain acc: 0.6031, lr: 0.000767, lamb: 1.5318, time: 9.2771\n","epoch 341: train D loss: 0.6759, train F loss: -0.9704, train acc: 0.9824, domain acc: 0.5912, lr: 0.000766, lamb: 1.5325, time: 9.9266\n","epoch 342: train D loss: 0.6680, train F loss: -0.9590, train acc: 0.9848, domain acc: 0.6062, lr: 0.000766, lamb: 1.5333, time: 9.4961\n","epoch 343: train D loss: 0.6762, train F loss: -0.9711, train acc: 0.9812, domain acc: 0.5919, lr: 0.000765, lamb: 1.5341, time: 9.4260\n","epoch 344: train D loss: 0.6740, train F loss: -0.9688, train acc: 0.9842, domain acc: 0.5947, lr: 0.000764, lamb: 1.5348, time: 9.4014\n","epoch 345: train D loss: 0.6756, train F loss: -0.9786, train acc: 0.9850, domain acc: 0.5920, lr: 0.000763, lamb: 1.5356, time: 9.3489\n","epoch 346: train D loss: 0.6718, train F loss: -0.9692, train acc: 0.9836, domain acc: 0.5959, lr: 0.000763, lamb: 1.5364, time: 9.0857\n","epoch 347: train D loss: 0.6809, train F loss: -0.9918, train acc: 0.9840, domain acc: 0.5820, lr: 0.000762, lamb: 1.5371, time: 9.1465\n","epoch 348: train D loss: 0.6741, train F loss: -0.9798, train acc: 0.9834, domain acc: 0.5974, lr: 0.000761, lamb: 1.5379, time: 9.0540\n","epoch 349: train D loss: 0.6717, train F loss: -0.9751, train acc: 0.9830, domain acc: 0.5985, lr: 0.000760, lamb: 1.5386, time: 9.1363\n","epoch 350: train D loss: 0.6727, train F loss: -0.9666, train acc: 0.9828, domain acc: 0.5943, lr: 0.000759, lamb: 1.5394, time: 9.1193\n","epoch 351: train D loss: 0.6730, train F loss: -0.9772, train acc: 0.9828, domain acc: 0.5989, lr: 0.000759, lamb: 1.5401, time: 9.0988\n","epoch 352: train D loss: 0.6736, train F loss: -0.9670, train acc: 0.9814, domain acc: 0.5965, lr: 0.000758, lamb: 1.5409, time: 9.0679\n","epoch 353: train D loss: 0.6763, train F loss: -0.9761, train acc: 0.9796, domain acc: 0.5917, lr: 0.000757, lamb: 1.5416, time: 9.0972\n","epoch 354: train D loss: 0.6713, train F loss: -0.9657, train acc: 0.9838, domain acc: 0.6025, lr: 0.000756, lamb: 1.5424, time: 9.0183\n","epoch 355: train D loss: 0.6744, train F loss: -0.9694, train acc: 0.9796, domain acc: 0.5998, lr: 0.000756, lamb: 1.5431, time: 9.0195\n","epoch 356: train D loss: 0.6741, train F loss: -0.9920, train acc: 0.9884, domain acc: 0.5925, lr: 0.000755, lamb: 1.5438, time: 9.1091\n","epoch 357: train D loss: 0.6730, train F loss: -0.9883, train acc: 0.9848, domain acc: 0.5936, lr: 0.000754, lamb: 1.5446, time: 9.0723\n","epoch 358: train D loss: 0.6761, train F loss: -0.9760, train acc: 0.9818, domain acc: 0.5905, lr: 0.000753, lamb: 1.5453, time: 9.0186\n","epoch 359: train D loss: 0.6725, train F loss: -0.9770, train acc: 0.9836, domain acc: 0.5968, lr: 0.000753, lamb: 1.5461, time: 9.0358\n","epoch 360: train D loss: 0.6707, train F loss: -0.9729, train acc: 0.9828, domain acc: 0.6012, lr: 0.000752, lamb: 1.5468, time: 9.0449\n","epoch 361: train D loss: 0.6715, train F loss: -0.9748, train acc: 0.9820, domain acc: 0.6021, lr: 0.000751, lamb: 1.5475, time: 9.0810\n","epoch 362: train D loss: 0.6763, train F loss: -0.9790, train acc: 0.9808, domain acc: 0.5947, lr: 0.000750, lamb: 1.5482, time: 8.9955\n","epoch 363: train D loss: 0.6734, train F loss: -0.9732, train acc: 0.9808, domain acc: 0.5914, lr: 0.000750, lamb: 1.5490, time: 9.0255\n","epoch 364: train D loss: 0.6730, train F loss: -0.9828, train acc: 0.9862, domain acc: 0.6006, lr: 0.000749, lamb: 1.5497, time: 9.0371\n","epoch 365: train D loss: 0.6724, train F loss: -0.9816, train acc: 0.9836, domain acc: 0.5992, lr: 0.000748, lamb: 1.5504, time: 9.0248\n","epoch 366: train D loss: 0.6753, train F loss: -0.9895, train acc: 0.9840, domain acc: 0.5951, lr: 0.000747, lamb: 1.5511, time: 8.9967\n","epoch 367: train D loss: 0.6734, train F loss: -0.9834, train acc: 0.9822, domain acc: 0.5945, lr: 0.000747, lamb: 1.5518, time: 9.0950\n","epoch 368: train D loss: 0.6762, train F loss: -0.9847, train acc: 0.9826, domain acc: 0.5861, lr: 0.000746, lamb: 1.5526, time: 9.0685\n","epoch 369: train D loss: 0.6720, train F loss: -0.9792, train acc: 0.9850, domain acc: 0.5994, lr: 0.000745, lamb: 1.5533, time: 9.0318\n","epoch 370: train D loss: 0.6754, train F loss: -0.9790, train acc: 0.9818, domain acc: 0.5946, lr: 0.000744, lamb: 1.5540, time: 9.0447\n","epoch 371: train D loss: 0.6796, train F loss: -0.9900, train acc: 0.9804, domain acc: 0.5783, lr: 0.000744, lamb: 1.5547, time: 9.0140\n","epoch 372: train D loss: 0.6717, train F loss: -0.9938, train acc: 0.9860, domain acc: 0.5987, lr: 0.000743, lamb: 1.5554, time: 9.0002\n","epoch 373: train D loss: 0.6738, train F loss: -0.9910, train acc: 0.9840, domain acc: 0.5930, lr: 0.000742, lamb: 1.5561, time: 9.0975\n","epoch 374: train D loss: 0.6728, train F loss: -0.9920, train acc: 0.9842, domain acc: 0.5903, lr: 0.000741, lamb: 1.5568, time: 9.0633\n","epoch 375: train D loss: 0.6757, train F loss: -0.9910, train acc: 0.9812, domain acc: 0.5898, lr: 0.000741, lamb: 1.5575, time: 9.0663\n","epoch 376: train D loss: 0.6770, train F loss: -1.0017, train acc: 0.9854, domain acc: 0.5895, lr: 0.000740, lamb: 1.5582, time: 8.9707\n","epoch 377: train D loss: 0.6738, train F loss: -1.0061, train acc: 0.9884, domain acc: 0.5913, lr: 0.000739, lamb: 1.5589, time: 9.2068\n","epoch 378: train D loss: 0.6786, train F loss: -1.0033, train acc: 0.9824, domain acc: 0.5867, lr: 0.000738, lamb: 1.5596, time: 8.9793\n","epoch 379: train D loss: 0.6738, train F loss: -0.9901, train acc: 0.9840, domain acc: 0.5932, lr: 0.000738, lamb: 1.5603, time: 9.0654\n","epoch 380: train D loss: 0.6741, train F loss: -0.9888, train acc: 0.9838, domain acc: 0.6041, lr: 0.000737, lamb: 1.5610, time: 9.0562\n","epoch 381: train D loss: 0.6779, train F loss: -0.9957, train acc: 0.9842, domain acc: 0.5854, lr: 0.000736, lamb: 1.5617, time: 9.0818\n","epoch 382: train D loss: 0.6733, train F loss: -0.9945, train acc: 0.9846, domain acc: 0.6015, lr: 0.000736, lamb: 1.5624, time: 9.0472\n","epoch 383: train D loss: 0.6726, train F loss: -0.9897, train acc: 0.9834, domain acc: 0.6021, lr: 0.000735, lamb: 1.5631, time: 9.2296\n","epoch 384: train D loss: 0.6732, train F loss: -0.9905, train acc: 0.9838, domain acc: 0.5953, lr: 0.000734, lamb: 1.5637, time: 9.1095\n","epoch 385: train D loss: 0.6777, train F loss: -1.0040, train acc: 0.9858, domain acc: 0.5874, lr: 0.000733, lamb: 1.5644, time: 9.0482\n","epoch 386: train D loss: 0.6750, train F loss: -0.9957, train acc: 0.9876, domain acc: 0.5918, lr: 0.000733, lamb: 1.5651, time: 8.9943\n","epoch 387: train D loss: 0.6746, train F loss: -0.9909, train acc: 0.9816, domain acc: 0.5918, lr: 0.000732, lamb: 1.5658, time: 9.0515\n","epoch 388: train D loss: 0.6751, train F loss: -1.0088, train acc: 0.9882, domain acc: 0.5889, lr: 0.000731, lamb: 1.5665, time: 9.0447\n","epoch 389: train D loss: 0.6784, train F loss: -0.9950, train acc: 0.9804, domain acc: 0.5814, lr: 0.000730, lamb: 1.5671, time: 9.0127\n","epoch 390: train D loss: 0.6752, train F loss: -1.0063, train acc: 0.9850, domain acc: 0.5911, lr: 0.000730, lamb: 1.5678, time: 9.0580\n","epoch 391: train D loss: 0.6745, train F loss: -0.9912, train acc: 0.9814, domain acc: 0.5935, lr: 0.000729, lamb: 1.5685, time: 9.0342\n","epoch 392: train D loss: 0.6751, train F loss: -1.0038, train acc: 0.9870, domain acc: 0.5916, lr: 0.000728, lamb: 1.5692, time: 9.0590\n","epoch 393: train D loss: 0.6783, train F loss: -1.0033, train acc: 0.9840, domain acc: 0.5834, lr: 0.000727, lamb: 1.5698, time: 9.0237\n","epoch 394: train D loss: 0.6749, train F loss: -0.9948, train acc: 0.9828, domain acc: 0.5878, lr: 0.000727, lamb: 1.5705, time: 9.0279\n","epoch 395: train D loss: 0.6781, train F loss: -1.0133, train acc: 0.9866, domain acc: 0.5820, lr: 0.000726, lamb: 1.5712, time: 9.0710\n","epoch 396: train D loss: 0.6706, train F loss: -0.9996, train acc: 0.9836, domain acc: 0.5988, lr: 0.000725, lamb: 1.5718, time: 9.0613\n","epoch 397: train D loss: 0.6732, train F loss: -0.9984, train acc: 0.9830, domain acc: 0.5962, lr: 0.000725, lamb: 1.5725, time: 9.0925\n","epoch 398: train D loss: 0.6777, train F loss: -1.0063, train acc: 0.9832, domain acc: 0.5839, lr: 0.000724, lamb: 1.5732, time: 9.0472\n","epoch 399: train D loss: 0.6733, train F loss: -1.0042, train acc: 0.9846, domain acc: 0.5961, lr: 0.000723, lamb: 1.5738, time: 9.0183\n","epoch 400: train D loss: 0.6771, train F loss: -1.0196, train acc: 0.9870, domain acc: 0.5877, lr: 0.000722, lamb: 1.5745, time: 9.0885\n","epoch 401: train D loss: 0.6760, train F loss: -1.0118, train acc: 0.9864, domain acc: 0.5872, lr: 0.000722, lamb: 1.5751, time: 9.0221\n","epoch 402: train D loss: 0.6742, train F loss: -1.0029, train acc: 0.9860, domain acc: 0.5899, lr: 0.000721, lamb: 1.5758, time: 9.0151\n","epoch 403: train D loss: 0.6791, train F loss: -1.0168, train acc: 0.9870, domain acc: 0.5822, lr: 0.000720, lamb: 1.5764, time: 9.0041\n","epoch 404: train D loss: 0.6775, train F loss: -1.0146, train acc: 0.9870, domain acc: 0.5846, lr: 0.000720, lamb: 1.5771, time: 9.0368\n","epoch 405: train D loss: 0.6760, train F loss: -1.0189, train acc: 0.9886, domain acc: 0.5905, lr: 0.000719, lamb: 1.5777, time: 9.0440\n","epoch 406: train D loss: 0.6756, train F loss: -1.0086, train acc: 0.9844, domain acc: 0.5918, lr: 0.000718, lamb: 1.5784, time: 9.0300\n","epoch 407: train D loss: 0.6745, train F loss: -1.0100, train acc: 0.9850, domain acc: 0.5907, lr: 0.000717, lamb: 1.5790, time: 9.0107\n","epoch 408: train D loss: 0.6742, train F loss: -1.0017, train acc: 0.9844, domain acc: 0.5924, lr: 0.000717, lamb: 1.5797, time: 9.0894\n","epoch 409: train D loss: 0.6741, train F loss: -1.0062, train acc: 0.9848, domain acc: 0.5961, lr: 0.000716, lamb: 1.5803, time: 9.1109\n","epoch 410: train D loss: 0.6733, train F loss: -1.0052, train acc: 0.9860, domain acc: 0.5957, lr: 0.000715, lamb: 1.5810, time: 9.4184\n","epoch 411: train D loss: 0.6735, train F loss: -1.0063, train acc: 0.9840, domain acc: 0.5913, lr: 0.000715, lamb: 1.5816, time: 9.5410\n","epoch 412: train D loss: 0.6744, train F loss: -1.0066, train acc: 0.9858, domain acc: 0.5939, lr: 0.000714, lamb: 1.5822, time: 9.8375\n","epoch 413: train D loss: 0.6744, train F loss: -1.0063, train acc: 0.9832, domain acc: 0.5961, lr: 0.000713, lamb: 1.5829, time: 9.4224\n","epoch 414: train D loss: 0.6752, train F loss: -1.0107, train acc: 0.9838, domain acc: 0.5961, lr: 0.000712, lamb: 1.5835, time: 9.9109\n","epoch 415: train D loss: 0.6708, train F loss: -0.9996, train acc: 0.9834, domain acc: 0.6045, lr: 0.000712, lamb: 1.5841, time: 10.2864\n","epoch 416: train D loss: 0.6735, train F loss: -1.0144, train acc: 0.9854, domain acc: 0.5955, lr: 0.000711, lamb: 1.5848, time: 9.8931\n","epoch 417: train D loss: 0.6704, train F loss: -1.0116, train acc: 0.9888, domain acc: 0.6029, lr: 0.000710, lamb: 1.5854, time: 9.3740\n","epoch 418: train D loss: 0.6773, train F loss: -1.0213, train acc: 0.9872, domain acc: 0.5893, lr: 0.000710, lamb: 1.5860, time: 9.5776\n","epoch 419: train D loss: 0.6790, train F loss: -1.0144, train acc: 0.9832, domain acc: 0.5797, lr: 0.000709, lamb: 1.5867, time: 9.4851\n","epoch 420: train D loss: 0.6727, train F loss: -1.0197, train acc: 0.9878, domain acc: 0.5940, lr: 0.000708, lamb: 1.5873, time: 9.3327\n","epoch 421: train D loss: 0.6776, train F loss: -1.0294, train acc: 0.9894, domain acc: 0.5860, lr: 0.000707, lamb: 1.5879, time: 9.3793\n","epoch 422: train D loss: 0.6752, train F loss: -1.0180, train acc: 0.9862, domain acc: 0.5932, lr: 0.000707, lamb: 1.5885, time: 9.3164\n","epoch 423: train D loss: 0.6717, train F loss: -1.0140, train acc: 0.9872, domain acc: 0.5986, lr: 0.000706, lamb: 1.5892, time: 9.3545\n","epoch 424: train D loss: 0.6760, train F loss: -1.0069, train acc: 0.9812, domain acc: 0.5906, lr: 0.000705, lamb: 1.5898, time: 9.4528\n","epoch 425: train D loss: 0.6799, train F loss: -1.0277, train acc: 0.9834, domain acc: 0.5786, lr: 0.000705, lamb: 1.5904, time: 9.4782\n","epoch 426: train D loss: 0.6785, train F loss: -1.0250, train acc: 0.9838, domain acc: 0.5841, lr: 0.000704, lamb: 1.5910, time: 9.3479\n","epoch 427: train D loss: 0.6753, train F loss: -1.0279, train acc: 0.9858, domain acc: 0.5900, lr: 0.000703, lamb: 1.5916, time: 9.7068\n","epoch 428: train D loss: 0.6749, train F loss: -1.0189, train acc: 0.9866, domain acc: 0.5893, lr: 0.000702, lamb: 1.5923, time: 10.5910\n","epoch 429: train D loss: 0.6744, train F loss: -1.0174, train acc: 0.9840, domain acc: 0.5984, lr: 0.000702, lamb: 1.5929, time: 9.6704\n","epoch 430: train D loss: 0.6739, train F loss: -1.0174, train acc: 0.9870, domain acc: 0.5911, lr: 0.000701, lamb: 1.5935, time: 9.4286\n","epoch 431: train D loss: 0.6750, train F loss: -1.0230, train acc: 0.9862, domain acc: 0.5908, lr: 0.000700, lamb: 1.5941, time: 9.2585\n","epoch 432: train D loss: 0.6773, train F loss: -1.0297, train acc: 0.9872, domain acc: 0.5871, lr: 0.000700, lamb: 1.5947, time: 9.2846\n","epoch 433: train D loss: 0.6755, train F loss: -1.0202, train acc: 0.9850, domain acc: 0.5953, lr: 0.000699, lamb: 1.5953, time: 9.1073\n","epoch 434: train D loss: 0.6811, train F loss: -1.0315, train acc: 0.9864, domain acc: 0.5762, lr: 0.000698, lamb: 1.5959, time: 9.3639\n","epoch 435: train D loss: 0.6734, train F loss: -1.0241, train acc: 0.9848, domain acc: 0.5936, lr: 0.000698, lamb: 1.5965, time: 9.4106\n","epoch 436: train D loss: 0.6734, train F loss: -1.0300, train acc: 0.9890, domain acc: 0.5935, lr: 0.000697, lamb: 1.5971, time: 9.4686\n","epoch 437: train D loss: 0.6800, train F loss: -1.0354, train acc: 0.9868, domain acc: 0.5753, lr: 0.000696, lamb: 1.5977, time: 9.3628\n","epoch 438: train D loss: 0.6743, train F loss: -1.0347, train acc: 0.9886, domain acc: 0.5946, lr: 0.000695, lamb: 1.5983, time: 9.3789\n","epoch 439: train D loss: 0.6768, train F loss: -1.0225, train acc: 0.9852, domain acc: 0.5828, lr: 0.000695, lamb: 1.5989, time: 9.3995\n","epoch 440: train D loss: 0.6771, train F loss: -1.0406, train acc: 0.9894, domain acc: 0.5878, lr: 0.000694, lamb: 1.5995, time: 9.4076\n","epoch 441: train D loss: 0.6715, train F loss: -1.0188, train acc: 0.9842, domain acc: 0.6015, lr: 0.000693, lamb: 1.6001, time: 9.3295\n","epoch 442: train D loss: 0.6692, train F loss: -1.0162, train acc: 0.9838, domain acc: 0.5984, lr: 0.000693, lamb: 1.6007, time: 9.3201\n","epoch 443: train D loss: 0.6719, train F loss: -1.0025, train acc: 0.9818, domain acc: 0.5960, lr: 0.000692, lamb: 1.6013, time: 9.1029\n","epoch 444: train D loss: 0.6748, train F loss: -1.0183, train acc: 0.9840, domain acc: 0.5892, lr: 0.000691, lamb: 1.6019, time: 9.0803\n","epoch 445: train D loss: 0.6735, train F loss: -1.0191, train acc: 0.9840, domain acc: 0.5929, lr: 0.000691, lamb: 1.6025, time: 9.1094\n","epoch 446: train D loss: 0.6738, train F loss: -1.0289, train acc: 0.9864, domain acc: 0.5992, lr: 0.000690, lamb: 1.6031, time: 9.0563\n","epoch 447: train D loss: 0.6783, train F loss: -1.0309, train acc: 0.9844, domain acc: 0.5865, lr: 0.000689, lamb: 1.6037, time: 9.0161\n","epoch 448: train D loss: 0.6764, train F loss: -1.0354, train acc: 0.9880, domain acc: 0.5876, lr: 0.000689, lamb: 1.6043, time: 9.0164\n","epoch 449: train D loss: 0.6729, train F loss: -1.0260, train acc: 0.9886, domain acc: 0.5942, lr: 0.000688, lamb: 1.6048, time: 9.0578\n","epoch 450: train D loss: 0.6768, train F loss: -1.0330, train acc: 0.9846, domain acc: 0.5880, lr: 0.000687, lamb: 1.6054, time: 9.1017\n","epoch 451: train D loss: 0.6716, train F loss: -1.0154, train acc: 0.9850, domain acc: 0.5942, lr: 0.000686, lamb: 1.6060, time: 9.1023\n","epoch 452: train D loss: 0.6734, train F loss: -1.0287, train acc: 0.9868, domain acc: 0.5969, lr: 0.000686, lamb: 1.6066, time: 9.0295\n","epoch 453: train D loss: 0.6734, train F loss: -1.0287, train acc: 0.9866, domain acc: 0.5917, lr: 0.000685, lamb: 1.6072, time: 9.0246\n","epoch 454: train D loss: 0.6739, train F loss: -1.0189, train acc: 0.9868, domain acc: 0.5927, lr: 0.000684, lamb: 1.6078, time: 9.0225\n","epoch 455: train D loss: 0.6779, train F loss: -1.0120, train acc: 0.9794, domain acc: 0.5866, lr: 0.000684, lamb: 1.6083, time: 9.0236\n","epoch 456: train D loss: 0.6748, train F loss: -1.0338, train acc: 0.9872, domain acc: 0.5927, lr: 0.000683, lamb: 1.6089, time: 9.0141\n","epoch 457: train D loss: 0.6752, train F loss: -1.0363, train acc: 0.9860, domain acc: 0.5929, lr: 0.000682, lamb: 1.6095, time: 9.0290\n","epoch 458: train D loss: 0.6737, train F loss: -1.0399, train acc: 0.9898, domain acc: 0.5952, lr: 0.000682, lamb: 1.6101, time: 9.0519\n","epoch 459: train D loss: 0.6752, train F loss: -1.0375, train acc: 0.9864, domain acc: 0.5898, lr: 0.000681, lamb: 1.6106, time: 9.0458\n","epoch 460: train D loss: 0.6732, train F loss: -1.0212, train acc: 0.9846, domain acc: 0.5955, lr: 0.000680, lamb: 1.6112, time: 9.0719\n","epoch 461: train D loss: 0.6754, train F loss: -1.0337, train acc: 0.9864, domain acc: 0.5902, lr: 0.000680, lamb: 1.6118, time: 8.9967\n","epoch 462: train D loss: 0.6790, train F loss: -1.0417, train acc: 0.9864, domain acc: 0.5848, lr: 0.000679, lamb: 1.6123, time: 9.0021\n","epoch 463: train D loss: 0.6790, train F loss: -1.0378, train acc: 0.9824, domain acc: 0.5810, lr: 0.000678, lamb: 1.6129, time: 8.9827\n","epoch 464: train D loss: 0.6769, train F loss: -1.0458, train acc: 0.9870, domain acc: 0.5881, lr: 0.000678, lamb: 1.6135, time: 9.0347\n","epoch 465: train D loss: 0.6743, train F loss: -1.0414, train acc: 0.9886, domain acc: 0.5891, lr: 0.000677, lamb: 1.6140, time: 9.0558\n","epoch 466: train D loss: 0.6708, train F loss: -1.0220, train acc: 0.9846, domain acc: 0.6017, lr: 0.000676, lamb: 1.6146, time: 9.0570\n","epoch 467: train D loss: 0.6733, train F loss: -1.0335, train acc: 0.9866, domain acc: 0.5947, lr: 0.000676, lamb: 1.6152, time: 9.0124\n","epoch 468: train D loss: 0.6785, train F loss: -1.0401, train acc: 0.9850, domain acc: 0.5855, lr: 0.000675, lamb: 1.6157, time: 8.9918\n","epoch 469: train D loss: 0.6784, train F loss: -1.0473, train acc: 0.9864, domain acc: 0.5833, lr: 0.000674, lamb: 1.6163, time: 9.0017\n","epoch 470: train D loss: 0.6760, train F loss: -1.0480, train acc: 0.9880, domain acc: 0.5838, lr: 0.000674, lamb: 1.6169, time: 9.0133\n","epoch 471: train D loss: 0.6769, train F loss: -1.0379, train acc: 0.9876, domain acc: 0.5840, lr: 0.000673, lamb: 1.6174, time: 9.0235\n","epoch 472: train D loss: 0.6761, train F loss: -1.0498, train acc: 0.9894, domain acc: 0.5896, lr: 0.000672, lamb: 1.6180, time: 8.9987\n","epoch 473: train D loss: 0.6779, train F loss: -1.0353, train acc: 0.9862, domain acc: 0.5929, lr: 0.000672, lamb: 1.6185, time: 8.9964\n","epoch 474: train D loss: 0.6746, train F loss: -1.0417, train acc: 0.9850, domain acc: 0.5920, lr: 0.000671, lamb: 1.6191, time: 9.0169\n","epoch 475: train D loss: 0.6780, train F loss: -1.0510, train acc: 0.9878, domain acc: 0.5788, lr: 0.000670, lamb: 1.6196, time: 9.1403\n","epoch 476: train D loss: 0.6733, train F loss: -1.0432, train acc: 0.9892, domain acc: 0.5914, lr: 0.000670, lamb: 1.6202, time: 8.9996\n","epoch 477: train D loss: 0.6763, train F loss: -1.0303, train acc: 0.9824, domain acc: 0.5871, lr: 0.000669, lamb: 1.6207, time: 8.9996\n","epoch 478: train D loss: 0.6802, train F loss: -1.0468, train acc: 0.9844, domain acc: 0.5759, lr: 0.000668, lamb: 1.6213, time: 9.0551\n","epoch 479: train D loss: 0.6748, train F loss: -1.0434, train acc: 0.9868, domain acc: 0.5918, lr: 0.000668, lamb: 1.6218, time: 9.0510\n","epoch 480: train D loss: 0.6802, train F loss: -1.0564, train acc: 0.9882, domain acc: 0.5821, lr: 0.000667, lamb: 1.6224, time: 9.0399\n","epoch 481: train D loss: 0.6756, train F loss: -1.0487, train acc: 0.9862, domain acc: 0.5897, lr: 0.000666, lamb: 1.6229, time: 9.0319\n","epoch 482: train D loss: 0.6781, train F loss: -1.0368, train acc: 0.9832, domain acc: 0.5873, lr: 0.000666, lamb: 1.6235, time: 9.0513\n","epoch 483: train D loss: 0.6767, train F loss: -1.0463, train acc: 0.9874, domain acc: 0.5836, lr: 0.000665, lamb: 1.6240, time: 9.0854\n","epoch 484: train D loss: 0.6760, train F loss: -1.0499, train acc: 0.9868, domain acc: 0.5876, lr: 0.000664, lamb: 1.6246, time: 9.0307\n","epoch 485: train D loss: 0.6748, train F loss: -1.0542, train acc: 0.9890, domain acc: 0.5907, lr: 0.000664, lamb: 1.6251, time: 9.0397\n","epoch 486: train D loss: 0.6764, train F loss: -1.0374, train acc: 0.9842, domain acc: 0.5843, lr: 0.000663, lamb: 1.6256, time: 9.0971\n","epoch 487: train D loss: 0.6803, train F loss: -1.0573, train acc: 0.9876, domain acc: 0.5817, lr: 0.000662, lamb: 1.6262, time: 9.0416\n","epoch 488: train D loss: 0.6720, train F loss: -1.0431, train acc: 0.9872, domain acc: 0.5937, lr: 0.000662, lamb: 1.6267, time: 9.0272\n","epoch 489: train D loss: 0.6733, train F loss: -1.0358, train acc: 0.9838, domain acc: 0.5947, lr: 0.000661, lamb: 1.6273, time: 9.0511\n","epoch 490: train D loss: 0.6758, train F loss: -1.0487, train acc: 0.9876, domain acc: 0.5874, lr: 0.000660, lamb: 1.6278, time: 9.0810\n","epoch 491: train D loss: 0.6807, train F loss: -1.0620, train acc: 0.9876, domain acc: 0.5772, lr: 0.000660, lamb: 1.6283, time: 9.0553\n","epoch 492: train D loss: 0.6763, train F loss: -1.0610, train acc: 0.9894, domain acc: 0.5853, lr: 0.000659, lamb: 1.6289, time: 9.0568\n","epoch 493: train D loss: 0.6764, train F loss: -1.0544, train acc: 0.9880, domain acc: 0.5819, lr: 0.000658, lamb: 1.6294, time: 8.9822\n","epoch 494: train D loss: 0.6756, train F loss: -1.0535, train acc: 0.9864, domain acc: 0.5917, lr: 0.000658, lamb: 1.6299, time: 9.0985\n","epoch 495: train D loss: 0.6760, train F loss: -1.0453, train acc: 0.9864, domain acc: 0.5874, lr: 0.000657, lamb: 1.6305, time: 9.0133\n","epoch 496: train D loss: 0.6748, train F loss: -1.0476, train acc: 0.9862, domain acc: 0.5893, lr: 0.000656, lamb: 1.6310, time: 8.9529\n","epoch 497: train D loss: 0.6819, train F loss: -1.0544, train acc: 0.9870, domain acc: 0.5777, lr: 0.000656, lamb: 1.6315, time: 9.0799\n","epoch 498: train D loss: 0.6733, train F loss: -1.0486, train acc: 0.9866, domain acc: 0.5942, lr: 0.000655, lamb: 1.6321, time: 9.1075\n","epoch 499: train D loss: 0.6771, train F loss: -1.0587, train acc: 0.9892, domain acc: 0.5871, lr: 0.000654, lamb: 1.6326, time: 9.0204\n","epoch 500: train D loss: 0.6731, train F loss: -1.0530, train acc: 0.9888, domain acc: 0.5967, lr: 0.000654, lamb: 1.6331, time: 9.0318\n","epoch 501: train D loss: 0.6765, train F loss: -1.0541, train acc: 0.9872, domain acc: 0.5888, lr: 0.000653, lamb: 1.6336, time: 8.9802\n","epoch 502: train D loss: 0.6786, train F loss: -1.0610, train acc: 0.9868, domain acc: 0.5804, lr: 0.000652, lamb: 1.6342, time: 9.0008\n","epoch 503: train D loss: 0.6714, train F loss: -1.0504, train acc: 0.9864, domain acc: 0.5947, lr: 0.000652, lamb: 1.6347, time: 9.0980\n","epoch 504: train D loss: 0.6737, train F loss: -1.0417, train acc: 0.9862, domain acc: 0.5938, lr: 0.000651, lamb: 1.6352, time: 9.2307\n","epoch 505: train D loss: 0.6786, train F loss: -1.0602, train acc: 0.9870, domain acc: 0.5874, lr: 0.000650, lamb: 1.6357, time: 9.0448\n","epoch 506: train D loss: 0.6807, train F loss: -1.0750, train acc: 0.9904, domain acc: 0.5792, lr: 0.000650, lamb: 1.6362, time: 9.2886\n","epoch 507: train D loss: 0.6751, train F loss: -1.0549, train acc: 0.9882, domain acc: 0.5898, lr: 0.000649, lamb: 1.6368, time: 9.2103\n","epoch 508: train D loss: 0.6767, train F loss: -1.0585, train acc: 0.9874, domain acc: 0.5882, lr: 0.000648, lamb: 1.6373, time: 9.3584\n","epoch 509: train D loss: 0.6768, train F loss: -1.0643, train acc: 0.9868, domain acc: 0.5886, lr: 0.000648, lamb: 1.6378, time: 9.5331\n","epoch 510: train D loss: 0.6800, train F loss: -1.0583, train acc: 0.9866, domain acc: 0.5830, lr: 0.000647, lamb: 1.6383, time: 9.5727\n","epoch 511: train D loss: 0.6754, train F loss: -1.0615, train acc: 0.9862, domain acc: 0.5905, lr: 0.000646, lamb: 1.6388, time: 9.4718\n","epoch 512: train D loss: 0.6777, train F loss: -1.0651, train acc: 0.9872, domain acc: 0.5829, lr: 0.000646, lamb: 1.6393, time: 9.6958\n","epoch 513: train D loss: 0.6765, train F loss: -1.0587, train acc: 0.9854, domain acc: 0.5896, lr: 0.000645, lamb: 1.6399, time: 9.4431\n","epoch 514: train D loss: 0.6732, train F loss: -1.0557, train acc: 0.9870, domain acc: 0.5922, lr: 0.000645, lamb: 1.6404, time: 9.3811\n","epoch 515: train D loss: 0.6732, train F loss: -1.0578, train acc: 0.9882, domain acc: 0.6034, lr: 0.000644, lamb: 1.6409, time: 9.0611\n","epoch 516: train D loss: 0.6777, train F loss: -1.0560, train acc: 0.9860, domain acc: 0.5852, lr: 0.000643, lamb: 1.6414, time: 9.2255\n","epoch 517: train D loss: 0.6734, train F loss: -1.0550, train acc: 0.9878, domain acc: 0.5952, lr: 0.000643, lamb: 1.6419, time: 9.1289\n","epoch 518: train D loss: 0.6769, train F loss: -1.0667, train acc: 0.9894, domain acc: 0.5820, lr: 0.000642, lamb: 1.6424, time: 9.1989\n","epoch 519: train D loss: 0.6806, train F loss: -1.0690, train acc: 0.9876, domain acc: 0.5704, lr: 0.000641, lamb: 1.6429, time: 9.3330\n","epoch 520: train D loss: 0.6768, train F loss: -1.0685, train acc: 0.9866, domain acc: 0.5828, lr: 0.000641, lamb: 1.6434, time: 9.3411\n","epoch 521: train D loss: 0.6772, train F loss: -1.0640, train acc: 0.9862, domain acc: 0.5854, lr: 0.000640, lamb: 1.6439, time: 9.3439\n","epoch 522: train D loss: 0.6751, train F loss: -1.0631, train acc: 0.9886, domain acc: 0.5931, lr: 0.000639, lamb: 1.6444, time: 9.3538\n","epoch 523: train D loss: 0.6761, train F loss: -1.0583, train acc: 0.9868, domain acc: 0.5866, lr: 0.000639, lamb: 1.6449, time: 9.6339\n","epoch 524: train D loss: 0.6806, train F loss: -1.0739, train acc: 0.9874, domain acc: 0.5761, lr: 0.000638, lamb: 1.6454, time: 9.6023\n","epoch 525: train D loss: 0.6805, train F loss: -1.0739, train acc: 0.9878, domain acc: 0.5761, lr: 0.000637, lamb: 1.6459, time: 9.5923\n","epoch 526: train D loss: 0.6757, train F loss: -1.0673, train acc: 0.9900, domain acc: 0.5914, lr: 0.000637, lamb: 1.6464, time: 9.5065\n","epoch 527: train D loss: 0.6726, train F loss: -1.0569, train acc: 0.9876, domain acc: 0.5929, lr: 0.000636, lamb: 1.6469, time: 9.3232\n","epoch 528: train D loss: 0.6718, train F loss: -1.0455, train acc: 0.9856, domain acc: 0.6020, lr: 0.000636, lamb: 1.6474, time: 9.4316\n","epoch 529: train D loss: 0.6788, train F loss: -1.0625, train acc: 0.9842, domain acc: 0.5815, lr: 0.000635, lamb: 1.6479, time: 9.9941\n","epoch 530: train D loss: 0.6755, train F loss: -1.0634, train acc: 0.9886, domain acc: 0.5895, lr: 0.000634, lamb: 1.6484, time: 10.2802\n","epoch 531: train D loss: 0.6805, train F loss: -1.0771, train acc: 0.9876, domain acc: 0.5752, lr: 0.000634, lamb: 1.6489, time: 9.7193\n","epoch 532: train D loss: 0.6773, train F loss: -1.0708, train acc: 0.9876, domain acc: 0.5907, lr: 0.000633, lamb: 1.6494, time: 9.6214\n","epoch 533: train D loss: 0.6737, train F loss: -1.0646, train acc: 0.9880, domain acc: 0.5880, lr: 0.000632, lamb: 1.6499, time: 9.4889\n","epoch 534: train D loss: 0.6787, train F loss: -1.0735, train acc: 0.9878, domain acc: 0.5803, lr: 0.000632, lamb: 1.6504, time: 9.5170\n","epoch 535: train D loss: 0.6823, train F loss: -1.0878, train acc: 0.9898, domain acc: 0.5721, lr: 0.000631, lamb: 1.6509, time: 9.3356\n","epoch 536: train D loss: 0.6844, train F loss: -1.0820, train acc: 0.9868, domain acc: 0.5612, lr: 0.000631, lamb: 1.6514, time: 9.3239\n","epoch 537: train D loss: 0.6784, train F loss: -1.0792, train acc: 0.9880, domain acc: 0.5834, lr: 0.000630, lamb: 1.6519, time: 9.2564\n","epoch 538: train D loss: 0.6754, train F loss: -1.0763, train acc: 0.9902, domain acc: 0.5887, lr: 0.000629, lamb: 1.6524, time: 9.3197\n","epoch 539: train D loss: 0.6812, train F loss: -1.0879, train acc: 0.9906, domain acc: 0.5738, lr: 0.000629, lamb: 1.6528, time: 9.1694\n","epoch 540: train D loss: 0.6775, train F loss: -1.0797, train acc: 0.9890, domain acc: 0.5792, lr: 0.000628, lamb: 1.6533, time: 9.4413\n","epoch 541: train D loss: 0.6798, train F loss: -1.0828, train acc: 0.9882, domain acc: 0.5790, lr: 0.000627, lamb: 1.6538, time: 9.2351\n","epoch 542: train D loss: 0.6764, train F loss: -1.0794, train acc: 0.9898, domain acc: 0.5859, lr: 0.000627, lamb: 1.6543, time: 9.2103\n","epoch 543: train D loss: 0.6801, train F loss: -1.0763, train acc: 0.9862, domain acc: 0.5730, lr: 0.000626, lamb: 1.6548, time: 9.1941\n","epoch 544: train D loss: 0.6773, train F loss: -1.0763, train acc: 0.9872, domain acc: 0.5841, lr: 0.000625, lamb: 1.6553, time: 9.1957\n","epoch 545: train D loss: 0.6797, train F loss: -1.0698, train acc: 0.9864, domain acc: 0.5838, lr: 0.000625, lamb: 1.6558, time: 9.1510\n","epoch 546: train D loss: 0.6765, train F loss: -1.0690, train acc: 0.9856, domain acc: 0.5856, lr: 0.000624, lamb: 1.6562, time: 9.2233\n","epoch 547: train D loss: 0.6812, train F loss: -1.0848, train acc: 0.9894, domain acc: 0.5761, lr: 0.000624, lamb: 1.6567, time: 9.1984\n","epoch 548: train D loss: 0.6756, train F loss: -1.0736, train acc: 0.9882, domain acc: 0.5938, lr: 0.000623, lamb: 1.6572, time: 9.1529\n","epoch 549: train D loss: 0.6792, train F loss: -1.0733, train acc: 0.9862, domain acc: 0.5776, lr: 0.000622, lamb: 1.6577, time: 9.1479\n","epoch 550: train D loss: 0.6773, train F loss: -1.0754, train acc: 0.9878, domain acc: 0.5894, lr: 0.000622, lamb: 1.6582, time: 9.2882\n","epoch 551: train D loss: 0.6808, train F loss: -1.0802, train acc: 0.9868, domain acc: 0.5785, lr: 0.000621, lamb: 1.6586, time: 9.2366\n","epoch 552: train D loss: 0.6773, train F loss: -1.0793, train acc: 0.9874, domain acc: 0.5890, lr: 0.000620, lamb: 1.6591, time: 9.7841\n","epoch 553: train D loss: 0.6788, train F loss: -1.0786, train acc: 0.9890, domain acc: 0.5815, lr: 0.000620, lamb: 1.6596, time: 9.7359\n","epoch 554: train D loss: 0.6790, train F loss: -1.0838, train acc: 0.9882, domain acc: 0.5807, lr: 0.000619, lamb: 1.6601, time: 9.5534\n","epoch 555: train D loss: 0.6742, train F loss: -1.0722, train acc: 0.9884, domain acc: 0.5937, lr: 0.000619, lamb: 1.6605, time: 9.7008\n","epoch 556: train D loss: 0.6791, train F loss: -1.0794, train acc: 0.9880, domain acc: 0.5800, lr: 0.000618, lamb: 1.6610, time: 9.1486\n","epoch 557: train D loss: 0.6788, train F loss: -1.0824, train acc: 0.9878, domain acc: 0.5842, lr: 0.000617, lamb: 1.6615, time: 9.0909\n","epoch 558: train D loss: 0.6755, train F loss: -1.0759, train acc: 0.9884, domain acc: 0.5870, lr: 0.000617, lamb: 1.6620, time: 9.1649\n","epoch 559: train D loss: 0.6786, train F loss: -1.0815, train acc: 0.9882, domain acc: 0.5845, lr: 0.000616, lamb: 1.6624, time: 9.4045\n","epoch 560: train D loss: 0.6794, train F loss: -1.0807, train acc: 0.9876, domain acc: 0.5819, lr: 0.000616, lamb: 1.6629, time: 9.4678\n","epoch 561: train D loss: 0.6783, train F loss: -1.0768, train acc: 0.9874, domain acc: 0.5812, lr: 0.000615, lamb: 1.6634, time: 9.9978\n","epoch 562: train D loss: 0.6765, train F loss: -1.0786, train acc: 0.9870, domain acc: 0.5822, lr: 0.000614, lamb: 1.6638, time: 9.8172\n","epoch 563: train D loss: 0.6795, train F loss: -1.0934, train acc: 0.9914, domain acc: 0.5790, lr: 0.000614, lamb: 1.6643, time: 9.2400\n","epoch 564: train D loss: 0.6798, train F loss: -1.0872, train acc: 0.9884, domain acc: 0.5775, lr: 0.000613, lamb: 1.6648, time: 9.3429\n","epoch 565: train D loss: 0.6787, train F loss: -1.0889, train acc: 0.9876, domain acc: 0.5846, lr: 0.000612, lamb: 1.6652, time: 9.2845\n","epoch 566: train D loss: 0.6766, train F loss: -1.0845, train acc: 0.9886, domain acc: 0.5868, lr: 0.000612, lamb: 1.6657, time: 9.1993\n","epoch 567: train D loss: 0.6788, train F loss: -1.0852, train acc: 0.9862, domain acc: 0.5827, lr: 0.000611, lamb: 1.6662, time: 9.3561\n","epoch 568: train D loss: 0.6802, train F loss: -1.0920, train acc: 0.9900, domain acc: 0.5779, lr: 0.000611, lamb: 1.6666, time: 9.4291\n","epoch 569: train D loss: 0.6796, train F loss: -1.0861, train acc: 0.9880, domain acc: 0.5828, lr: 0.000610, lamb: 1.6671, time: 9.3241\n","epoch 570: train D loss: 0.6769, train F loss: -1.0828, train acc: 0.9886, domain acc: 0.5823, lr: 0.000609, lamb: 1.6675, time: 9.4132\n","epoch 571: train D loss: 0.6784, train F loss: -1.0882, train acc: 0.9892, domain acc: 0.5837, lr: 0.000609, lamb: 1.6680, time: 9.4740\n","epoch 572: train D loss: 0.6775, train F loss: -1.0874, train acc: 0.9892, domain acc: 0.5822, lr: 0.000608, lamb: 1.6685, time: 9.4500\n","epoch 573: train D loss: 0.6780, train F loss: -1.0886, train acc: 0.9892, domain acc: 0.5824, lr: 0.000608, lamb: 1.6689, time: 9.4141\n","epoch 574: train D loss: 0.6810, train F loss: -1.0941, train acc: 0.9892, domain acc: 0.5749, lr: 0.000607, lamb: 1.6694, time: 9.6223\n","epoch 575: train D loss: 0.6801, train F loss: -1.0946, train acc: 0.9868, domain acc: 0.5756, lr: 0.000606, lamb: 1.6698, time: 9.8004\n","epoch 576: train D loss: 0.6782, train F loss: -1.0889, train acc: 0.9888, domain acc: 0.5822, lr: 0.000606, lamb: 1.6703, time: 9.4753\n","epoch 577: train D loss: 0.6806, train F loss: -1.0970, train acc: 0.9890, domain acc: 0.5694, lr: 0.000605, lamb: 1.6708, time: 9.6767\n","epoch 578: train D loss: 0.6780, train F loss: -1.0856, train acc: 0.9882, domain acc: 0.5834, lr: 0.000605, lamb: 1.6712, time: 9.2133\n","epoch 579: train D loss: 0.6766, train F loss: -1.0923, train acc: 0.9898, domain acc: 0.5839, lr: 0.000604, lamb: 1.6717, time: 9.1379\n","epoch 580: train D loss: 0.6790, train F loss: -1.0846, train acc: 0.9874, domain acc: 0.5806, lr: 0.000603, lamb: 1.6721, time: 9.1403\n","epoch 581: train D loss: 0.6789, train F loss: -1.0894, train acc: 0.9880, domain acc: 0.5870, lr: 0.000603, lamb: 1.6726, time: 9.0931\n","epoch 582: train D loss: 0.6775, train F loss: -1.0830, train acc: 0.9878, domain acc: 0.5867, lr: 0.000602, lamb: 1.6730, time: 9.1286\n","epoch 583: train D loss: 0.6779, train F loss: -1.0868, train acc: 0.9884, domain acc: 0.5854, lr: 0.000602, lamb: 1.6735, time: 9.0747\n","epoch 584: train D loss: 0.6763, train F loss: -1.0867, train acc: 0.9884, domain acc: 0.5892, lr: 0.000601, lamb: 1.6739, time: 9.0428\n","epoch 585: train D loss: 0.6773, train F loss: -1.0868, train acc: 0.9878, domain acc: 0.5854, lr: 0.000600, lamb: 1.6744, time: 9.1100\n","epoch 586: train D loss: 0.6805, train F loss: -1.0902, train acc: 0.9868, domain acc: 0.5799, lr: 0.000600, lamb: 1.6748, time: 9.0730\n","epoch 587: train D loss: 0.6790, train F loss: -1.0966, train acc: 0.9896, domain acc: 0.5784, lr: 0.000599, lamb: 1.6753, time: 9.0266\n","epoch 588: train D loss: 0.6788, train F loss: -1.0901, train acc: 0.9886, domain acc: 0.5852, lr: 0.000599, lamb: 1.6757, time: 9.0177\n","epoch 589: train D loss: 0.6788, train F loss: -1.0940, train acc: 0.9896, domain acc: 0.5834, lr: 0.000598, lamb: 1.6762, time: 8.9129\n","epoch 590: train D loss: 0.6808, train F loss: -1.0986, train acc: 0.9892, domain acc: 0.5777, lr: 0.000597, lamb: 1.6766, time: 9.0270\n","epoch 591: train D loss: 0.6780, train F loss: -1.0930, train acc: 0.9890, domain acc: 0.5858, lr: 0.000597, lamb: 1.6771, time: 9.0076\n","epoch 592: train D loss: 0.6785, train F loss: -1.0949, train acc: 0.9882, domain acc: 0.5831, lr: 0.000596, lamb: 1.6775, time: 9.0305\n","epoch 593: train D loss: 0.6788, train F loss: -1.0978, train acc: 0.9886, domain acc: 0.5762, lr: 0.000596, lamb: 1.6779, time: 9.1588\n","epoch 594: train D loss: 0.6793, train F loss: -1.1032, train acc: 0.9890, domain acc: 0.5862, lr: 0.000595, lamb: 1.6784, time: 9.2799\n","epoch 595: train D loss: 0.6776, train F loss: -1.0908, train acc: 0.9880, domain acc: 0.5806, lr: 0.000594, lamb: 1.6788, time: 9.8383\n","epoch 596: train D loss: 0.6740, train F loss: -1.0944, train acc: 0.9900, domain acc: 0.5940, lr: 0.000594, lamb: 1.6793, time: 9.2164\n","epoch 597: train D loss: 0.6797, train F loss: -1.0887, train acc: 0.9858, domain acc: 0.5829, lr: 0.000593, lamb: 1.6797, time: 9.6218\n","epoch 598: train D loss: 0.6795, train F loss: -1.1057, train acc: 0.9906, domain acc: 0.5792, lr: 0.000593, lamb: 1.6801, time: 9.4607\n","epoch 599: train D loss: 0.6741, train F loss: -1.0858, train acc: 0.9882, domain acc: 0.5866, lr: 0.000592, lamb: 1.6806, time: 9.4449\n","epoch 600: train D loss: 0.6791, train F loss: -1.0895, train acc: 0.9866, domain acc: 0.5819, lr: 0.000591, lamb: 1.6810, time: 9.6577\n","epoch 601: train D loss: 0.6793, train F loss: -1.0925, train acc: 0.9876, domain acc: 0.5838, lr: 0.000591, lamb: 1.6815, time: 9.2920\n","epoch 602: train D loss: 0.6790, train F loss: -1.1024, train acc: 0.9894, domain acc: 0.5791, lr: 0.000590, lamb: 1.6819, time: 9.6925\n","epoch 603: train D loss: 0.6802, train F loss: -1.0927, train acc: 0.9872, domain acc: 0.5774, lr: 0.000590, lamb: 1.6823, time: 9.2475\n","epoch 604: train D loss: 0.6794, train F loss: -1.1014, train acc: 0.9878, domain acc: 0.5762, lr: 0.000589, lamb: 1.6828, time: 9.6067\n","epoch 605: train D loss: 0.6792, train F loss: -1.1027, train acc: 0.9892, domain acc: 0.5831, lr: 0.000588, lamb: 1.6832, time: 9.4116\n","epoch 606: train D loss: 0.6763, train F loss: -1.0920, train acc: 0.9894, domain acc: 0.5866, lr: 0.000588, lamb: 1.6836, time: 9.3816\n","epoch 607: train D loss: 0.6725, train F loss: -1.0929, train acc: 0.9910, domain acc: 0.5951, lr: 0.000587, lamb: 1.6841, time: 9.5953\n","epoch 608: train D loss: 0.6779, train F loss: -1.1024, train acc: 0.9908, domain acc: 0.5858, lr: 0.000587, lamb: 1.6845, time: 9.2082\n","epoch 609: train D loss: 0.6812, train F loss: -1.1019, train acc: 0.9878, domain acc: 0.5699, lr: 0.000586, lamb: 1.6849, time: 9.7774\n","epoch 610: train D loss: 0.6829, train F loss: -1.1115, train acc: 0.9906, domain acc: 0.5704, lr: 0.000586, lamb: 1.6854, time: 9.3306\n","epoch 611: train D loss: 0.6794, train F loss: -1.1096, train acc: 0.9904, domain acc: 0.5793, lr: 0.000585, lamb: 1.6858, time: 9.7546\n","epoch 612: train D loss: 0.6786, train F loss: -1.1073, train acc: 0.9904, domain acc: 0.5799, lr: 0.000584, lamb: 1.6862, time: 10.6506\n","epoch 613: train D loss: 0.6778, train F loss: -1.1045, train acc: 0.9914, domain acc: 0.5872, lr: 0.000584, lamb: 1.6867, time: 9.6705\n","epoch 614: train D loss: 0.6773, train F loss: -1.0947, train acc: 0.9886, domain acc: 0.5845, lr: 0.000583, lamb: 1.6871, time: 10.0990\n","epoch 615: train D loss: 0.6743, train F loss: -1.0935, train acc: 0.9892, domain acc: 0.5917, lr: 0.000583, lamb: 1.6875, time: 9.4039\n","epoch 616: train D loss: 0.6789, train F loss: -1.1001, train acc: 0.9888, domain acc: 0.5828, lr: 0.000582, lamb: 1.6879, time: 9.7017\n","epoch 617: train D loss: 0.6804, train F loss: -1.1046, train acc: 0.9884, domain acc: 0.5711, lr: 0.000581, lamb: 1.6884, time: 9.4873\n","epoch 618: train D loss: 0.6769, train F loss: -1.0954, train acc: 0.9882, domain acc: 0.5844, lr: 0.000581, lamb: 1.6888, time: 9.3823\n","epoch 619: train D loss: 0.6801, train F loss: -1.1079, train acc: 0.9906, domain acc: 0.5764, lr: 0.000580, lamb: 1.6892, time: 9.7375\n","epoch 620: train D loss: 0.6756, train F loss: -1.0977, train acc: 0.9890, domain acc: 0.5891, lr: 0.000580, lamb: 1.6896, time: 9.2088\n","epoch 621: train D loss: 0.6836, train F loss: -1.1105, train acc: 0.9892, domain acc: 0.5683, lr: 0.000579, lamb: 1.6901, time: 9.6978\n","epoch 622: train D loss: 0.6745, train F loss: -1.1028, train acc: 0.9898, domain acc: 0.5935, lr: 0.000579, lamb: 1.6905, time: 9.3039\n","epoch 623: train D loss: 0.6749, train F loss: -1.1002, train acc: 0.9890, domain acc: 0.5925, lr: 0.000578, lamb: 1.6909, time: 9.5642\n","epoch 624: train D loss: 0.6796, train F loss: -1.1044, train acc: 0.9886, domain acc: 0.5785, lr: 0.000577, lamb: 1.6913, time: 9.5128\n","epoch 625: train D loss: 0.6791, train F loss: -1.1052, train acc: 0.9896, domain acc: 0.5809, lr: 0.000577, lamb: 1.6918, time: 9.3161\n","epoch 626: train D loss: 0.6808, train F loss: -1.1050, train acc: 0.9876, domain acc: 0.5742, lr: 0.000576, lamb: 1.6922, time: 9.7136\n","epoch 627: train D loss: 0.6742, train F loss: -1.0986, train acc: 0.9904, domain acc: 0.5918, lr: 0.000576, lamb: 1.6926, time: 9.1447\n","epoch 628: train D loss: 0.6764, train F loss: -1.1035, train acc: 0.9896, domain acc: 0.5857, lr: 0.000575, lamb: 1.6930, time: 9.6573\n","epoch 629: train D loss: 0.6783, train F loss: -1.1035, train acc: 0.9882, domain acc: 0.5881, lr: 0.000574, lamb: 1.6934, time: 9.2965\n","epoch 630: train D loss: 0.6796, train F loss: -1.1041, train acc: 0.9878, domain acc: 0.5829, lr: 0.000574, lamb: 1.6938, time: 9.4215\n","epoch 631: train D loss: 0.6805, train F loss: -1.1091, train acc: 0.9892, domain acc: 0.5756, lr: 0.000573, lamb: 1.6943, time: 9.5025\n","epoch 632: train D loss: 0.6835, train F loss: -1.1171, train acc: 0.9900, domain acc: 0.5712, lr: 0.000573, lamb: 1.6947, time: 9.2556\n","epoch 633: train D loss: 0.6781, train F loss: -1.1082, train acc: 0.9894, domain acc: 0.5811, lr: 0.000572, lamb: 1.6951, time: 9.7184\n","epoch 634: train D loss: 0.6782, train F loss: -1.1059, train acc: 0.9874, domain acc: 0.5888, lr: 0.000572, lamb: 1.6955, time: 9.1374\n","epoch 635: train D loss: 0.6793, train F loss: -1.1120, train acc: 0.9896, domain acc: 0.5811, lr: 0.000571, lamb: 1.6959, time: 9.6410\n","epoch 636: train D loss: 0.6784, train F loss: -1.1102, train acc: 0.9894, domain acc: 0.5789, lr: 0.000570, lamb: 1.6963, time: 9.2439\n","epoch 637: train D loss: 0.6819, train F loss: -1.1205, train acc: 0.9908, domain acc: 0.5701, lr: 0.000570, lamb: 1.6967, time: 9.4566\n","epoch 638: train D loss: 0.6757, train F loss: -1.1064, train acc: 0.9898, domain acc: 0.5870, lr: 0.000569, lamb: 1.6972, time: 9.4714\n","epoch 639: train D loss: 0.6760, train F loss: -1.1004, train acc: 0.9880, domain acc: 0.5896, lr: 0.000569, lamb: 1.6976, time: 9.2491\n","epoch 640: train D loss: 0.6777, train F loss: -1.1041, train acc: 0.9886, domain acc: 0.5833, lr: 0.000568, lamb: 1.6980, time: 9.6594\n","epoch 641: train D loss: 0.6812, train F loss: -1.1090, train acc: 0.9894, domain acc: 0.5761, lr: 0.000568, lamb: 1.6984, time: 9.2258\n","epoch 642: train D loss: 0.6776, train F loss: -1.1150, train acc: 0.9904, domain acc: 0.5818, lr: 0.000567, lamb: 1.6988, time: 10.2132\n","epoch 643: train D loss: 0.6797, train F loss: -1.0875, train acc: 0.9840, domain acc: 0.5816, lr: 0.000566, lamb: 1.6992, time: 10.1119\n","epoch 644: train D loss: 0.6798, train F loss: -1.1144, train acc: 0.9884, domain acc: 0.5785, lr: 0.000566, lamb: 1.6996, time: 71.2609\n","epoch 645: train D loss: 0.6751, train F loss: -1.1084, train acc: 0.9910, domain acc: 0.5855, lr: 0.000565, lamb: 1.7000, time: 47.5782\n","epoch 646: train D loss: 0.6781, train F loss: -1.1088, train acc: 0.9890, domain acc: 0.5840, lr: 0.000565, lamb: 1.7004, time: 43.5495\n","epoch 647: train D loss: 0.6774, train F loss: -1.1114, train acc: 0.9900, domain acc: 0.5841, lr: 0.000564, lamb: 1.7008, time: 40.2987\n","epoch 648: train D loss: 0.6782, train F loss: -1.1113, train acc: 0.9898, domain acc: 0.5820, lr: 0.000564, lamb: 1.7012, time: 38.5272\n","epoch 649: train D loss: 0.6800, train F loss: -1.1112, train acc: 0.9872, domain acc: 0.5753, lr: 0.000563, lamb: 1.7017, time: 36.3053\n","epoch 650: train D loss: 0.6765, train F loss: -1.1122, train acc: 0.9906, domain acc: 0.5869, lr: 0.000563, lamb: 1.7021, time: 36.5559\n","epoch 651: train D loss: 0.6795, train F loss: -1.1204, train acc: 0.9916, domain acc: 0.5772, lr: 0.000562, lamb: 1.7025, time: 34.1564\n","epoch 652: train D loss: 0.6789, train F loss: -1.1169, train acc: 0.9892, domain acc: 0.5764, lr: 0.000561, lamb: 1.7029, time: 33.7276\n","epoch 653: train D loss: 0.6802, train F loss: -1.1216, train acc: 0.9898, domain acc: 0.5745, lr: 0.000561, lamb: 1.7033, time: 32.5405\n","epoch 654: train D loss: 0.6797, train F loss: -1.1215, train acc: 0.9902, domain acc: 0.5759, lr: 0.000560, lamb: 1.7037, time: 30.7334\n","epoch 655: train D loss: 0.6815, train F loss: -1.1178, train acc: 0.9890, domain acc: 0.5761, lr: 0.000560, lamb: 1.7041, time: 28.8172\n","epoch 656: train D loss: 0.6816, train F loss: -1.1198, train acc: 0.9878, domain acc: 0.5775, lr: 0.000559, lamb: 1.7045, time: 27.6804\n","epoch 657: train D loss: 0.6785, train F loss: -1.1178, train acc: 0.9896, domain acc: 0.5795, lr: 0.000559, lamb: 1.7049, time: 27.1517\n","epoch 658: train D loss: 0.6761, train F loss: -1.1052, train acc: 0.9870, domain acc: 0.5864, lr: 0.000558, lamb: 1.7053, time: 26.3124\n","epoch 659: train D loss: 0.6782, train F loss: -1.1187, train acc: 0.9902, domain acc: 0.5778, lr: 0.000558, lamb: 1.7057, time: 25.6223\n","epoch 660: train D loss: 0.6812, train F loss: -1.1231, train acc: 0.9898, domain acc: 0.5701, lr: 0.000557, lamb: 1.7061, time: 24.6433\n","epoch 661: train D loss: 0.6786, train F loss: -1.1162, train acc: 0.9878, domain acc: 0.5811, lr: 0.000556, lamb: 1.7065, time: 24.0560\n","epoch 662: train D loss: 0.6827, train F loss: -1.1300, train acc: 0.9906, domain acc: 0.5675, lr: 0.000556, lamb: 1.7069, time: 23.6049\n","epoch 663: train D loss: 0.6795, train F loss: -1.1224, train acc: 0.9906, domain acc: 0.5822, lr: 0.000555, lamb: 1.7073, time: 22.8532\n","epoch 664: train D loss: 0.6841, train F loss: -1.1211, train acc: 0.9884, domain acc: 0.5617, lr: 0.000555, lamb: 1.7077, time: 22.4969\n","epoch 665: train D loss: 0.6784, train F loss: -1.1152, train acc: 0.9888, domain acc: 0.5769, lr: 0.000554, lamb: 1.7081, time: 21.5758\n","epoch 666: train D loss: 0.6828, train F loss: -1.1312, train acc: 0.9908, domain acc: 0.5646, lr: 0.000554, lamb: 1.7084, time: 21.9649\n","epoch 667: train D loss: 0.6799, train F loss: -1.1236, train acc: 0.9894, domain acc: 0.5700, lr: 0.000553, lamb: 1.7088, time: 21.1633\n","epoch 668: train D loss: 0.6832, train F loss: -1.1341, train acc: 0.9908, domain acc: 0.5679, lr: 0.000553, lamb: 1.7092, time: 20.5163\n","epoch 669: train D loss: 0.6809, train F loss: -1.1260, train acc: 0.9898, domain acc: 0.5773, lr: 0.000552, lamb: 1.7096, time: 20.2304\n","epoch 670: train D loss: 0.6792, train F loss: -1.1159, train acc: 0.9894, domain acc: 0.5858, lr: 0.000551, lamb: 1.7100, time: 19.3556\n","epoch 671: train D loss: 0.6809, train F loss: -1.1246, train acc: 0.9912, domain acc: 0.5735, lr: 0.000551, lamb: 1.7104, time: 19.1488\n","epoch 672: train D loss: 0.6801, train F loss: -1.1238, train acc: 0.9906, domain acc: 0.5763, lr: 0.000550, lamb: 1.7108, time: 19.5574\n","epoch 673: train D loss: 0.6786, train F loss: -1.1168, train acc: 0.9880, domain acc: 0.5806, lr: 0.000550, lamb: 1.7112, time: 19.4109\n","epoch 674: train D loss: 0.6787, train F loss: -1.1113, train acc: 0.9874, domain acc: 0.5828, lr: 0.000549, lamb: 1.7116, time: 19.2121\n","epoch 675: train D loss: 0.6814, train F loss: -1.1238, train acc: 0.9890, domain acc: 0.5812, lr: 0.000549, lamb: 1.7120, time: 17.8816\n","epoch 676: train D loss: 0.6812, train F loss: -1.1266, train acc: 0.9894, domain acc: 0.5739, lr: 0.000548, lamb: 1.7124, time: 17.0762\n","epoch 677: train D loss: 0.6747, train F loss: -1.1156, train acc: 0.9914, domain acc: 0.5939, lr: 0.000548, lamb: 1.7128, time: 17.4752\n","epoch 678: train D loss: 0.6780, train F loss: -1.1199, train acc: 0.9912, domain acc: 0.5876, lr: 0.000547, lamb: 1.7131, time: 16.1417\n","epoch 679: train D loss: 0.6780, train F loss: -1.1180, train acc: 0.9896, domain acc: 0.5839, lr: 0.000546, lamb: 1.7135, time: 15.5314\n","epoch 680: train D loss: 0.6793, train F loss: -1.1192, train acc: 0.9890, domain acc: 0.5772, lr: 0.000546, lamb: 1.7139, time: 15.7142\n","epoch 681: train D loss: 0.6763, train F loss: -1.1202, train acc: 0.9900, domain acc: 0.5851, lr: 0.000545, lamb: 1.7143, time: 15.0910\n","epoch 682: train D loss: 0.6782, train F loss: -1.1221, train acc: 0.9894, domain acc: 0.5840, lr: 0.000545, lamb: 1.7147, time: 14.9448\n","epoch 683: train D loss: 0.6799, train F loss: -1.1255, train acc: 0.9906, domain acc: 0.5826, lr: 0.000544, lamb: 1.7151, time: 14.1595\n","epoch 684: train D loss: 0.6802, train F loss: -1.1221, train acc: 0.9892, domain acc: 0.5769, lr: 0.000544, lamb: 1.7155, time: 13.7375\n","epoch 685: train D loss: 0.6804, train F loss: -1.1278, train acc: 0.9894, domain acc: 0.5691, lr: 0.000543, lamb: 1.7158, time: 14.1636\n","epoch 686: train D loss: 0.6808, train F loss: -1.1272, train acc: 0.9904, domain acc: 0.5767, lr: 0.000543, lamb: 1.7162, time: 13.4064\n","epoch 687: train D loss: 0.6823, train F loss: -1.1354, train acc: 0.9908, domain acc: 0.5729, lr: 0.000542, lamb: 1.7166, time: 13.3147\n","epoch 688: train D loss: 0.6749, train F loss: -1.1230, train acc: 0.9906, domain acc: 0.5922, lr: 0.000542, lamb: 1.7170, time: 13.2093\n","epoch 689: train D loss: 0.6800, train F loss: -1.1249, train acc: 0.9894, domain acc: 0.5819, lr: 0.000541, lamb: 1.7174, time: 13.1296\n","epoch 690: train D loss: 0.6798, train F loss: -1.1317, train acc: 0.9900, domain acc: 0.5803, lr: 0.000540, lamb: 1.7178, time: 13.3842\n","epoch 691: train D loss: 0.6824, train F loss: -1.1311, train acc: 0.9882, domain acc: 0.5720, lr: 0.000540, lamb: 1.7181, time: 12.3461\n","epoch 692: train D loss: 0.6802, train F loss: -1.1303, train acc: 0.9908, domain acc: 0.5773, lr: 0.000539, lamb: 1.7185, time: 12.5290\n","epoch 693: train D loss: 0.6803, train F loss: -1.1231, train acc: 0.9882, domain acc: 0.5780, lr: 0.000539, lamb: 1.7189, time: 13.4308\n","epoch 694: train D loss: 0.6791, train F loss: -1.1290, train acc: 0.9902, domain acc: 0.5744, lr: 0.000538, lamb: 1.7193, time: 13.4617\n","epoch 695: train D loss: 0.6819, train F loss: -1.1369, train acc: 0.9910, domain acc: 0.5716, lr: 0.000538, lamb: 1.7196, time: 14.4005\n","epoch 696: train D loss: 0.6797, train F loss: -1.1302, train acc: 0.9914, domain acc: 0.5753, lr: 0.000537, lamb: 1.7200, time: 15.1219\n","epoch 697: train D loss: 0.6818, train F loss: -1.1353, train acc: 0.9886, domain acc: 0.5737, lr: 0.000537, lamb: 1.7204, time: 11.9916\n","epoch 698: train D loss: 0.6813, train F loss: -1.1287, train acc: 0.9886, domain acc: 0.5754, lr: 0.000536, lamb: 1.7208, time: 11.9599\n","epoch 699: train D loss: 0.6779, train F loss: -1.1257, train acc: 0.9904, domain acc: 0.5820, lr: 0.000536, lamb: 1.7212, time: 11.7744\n","epoch 700: train D loss: 0.6813, train F loss: -1.1281, train acc: 0.9908, domain acc: 0.5786, lr: 0.000535, lamb: 1.7215, time: 11.6671\n","epoch 701: train D loss: 0.6826, train F loss: -1.1347, train acc: 0.9890, domain acc: 0.5716, lr: 0.000535, lamb: 1.7219, time: 11.7455\n","epoch 702: train D loss: 0.6786, train F loss: -1.1303, train acc: 0.9904, domain acc: 0.5745, lr: 0.000534, lamb: 1.7223, time: 11.3899\n","epoch 703: train D loss: 0.6758, train F loss: -1.1262, train acc: 0.9916, domain acc: 0.5867, lr: 0.000533, lamb: 1.7227, time: 11.7311\n","epoch 704: train D loss: 0.6773, train F loss: -1.1256, train acc: 0.9898, domain acc: 0.5874, lr: 0.000533, lamb: 1.7230, time: 11.4845\n","epoch 705: train D loss: 0.6821, train F loss: -1.1417, train acc: 0.9912, domain acc: 0.5736, lr: 0.000532, lamb: 1.7234, time: 11.4398\n","epoch 706: train D loss: 0.6843, train F loss: -1.1427, train acc: 0.9900, domain acc: 0.5702, lr: 0.000532, lamb: 1.7238, time: 11.4432\n","epoch 707: train D loss: 0.6804, train F loss: -1.1348, train acc: 0.9884, domain acc: 0.5734, lr: 0.000531, lamb: 1.7241, time: 11.2832\n","epoch 708: train D loss: 0.6789, train F loss: -1.1371, train acc: 0.9914, domain acc: 0.5821, lr: 0.000531, lamb: 1.7245, time: 11.3383\n","epoch 709: train D loss: 0.6793, train F loss: -1.1339, train acc: 0.9930, domain acc: 0.5721, lr: 0.000530, lamb: 1.7249, time: 11.1809\n","epoch 710: train D loss: 0.6791, train F loss: -1.1278, train acc: 0.9892, domain acc: 0.5786, lr: 0.000530, lamb: 1.7253, time: 11.3099\n","epoch 711: train D loss: 0.6827, train F loss: -1.1401, train acc: 0.9908, domain acc: 0.5747, lr: 0.000529, lamb: 1.7256, time: 11.4132\n","epoch 712: train D loss: 0.6806, train F loss: -1.1383, train acc: 0.9900, domain acc: 0.5712, lr: 0.000529, lamb: 1.7260, time: 10.9829\n","epoch 713: train D loss: 0.6779, train F loss: -1.1336, train acc: 0.9916, domain acc: 0.5825, lr: 0.000528, lamb: 1.7264, time: 11.0688\n","epoch 714: train D loss: 0.6819, train F loss: -1.1367, train acc: 0.9902, domain acc: 0.5799, lr: 0.000528, lamb: 1.7267, time: 11.0905\n","epoch 715: train D loss: 0.6782, train F loss: -1.1232, train acc: 0.9878, domain acc: 0.5852, lr: 0.000527, lamb: 1.7271, time: 10.9523\n","epoch 716: train D loss: 0.6828, train F loss: -1.1408, train acc: 0.9904, domain acc: 0.5709, lr: 0.000527, lamb: 1.7275, time: 10.5957\n","epoch 717: train D loss: 0.6778, train F loss: -1.1335, train acc: 0.9914, domain acc: 0.5849, lr: 0.000526, lamb: 1.7278, time: 10.8480\n","epoch 718: train D loss: 0.6832, train F loss: -1.1445, train acc: 0.9904, domain acc: 0.5640, lr: 0.000526, lamb: 1.7282, time: 10.8891\n","epoch 719: train D loss: 0.6775, train F loss: -1.1330, train acc: 0.9904, domain acc: 0.5860, lr: 0.000525, lamb: 1.7286, time: 11.6253\n","epoch 720: train D loss: 0.6781, train F loss: -1.1344, train acc: 0.9902, domain acc: 0.5825, lr: 0.000524, lamb: 1.7289, time: 11.3907\n","epoch 721: train D loss: 0.6795, train F loss: -1.1405, train acc: 0.9920, domain acc: 0.5790, lr: 0.000524, lamb: 1.7293, time: 11.0573\n","epoch 722: train D loss: 0.6805, train F loss: -1.1417, train acc: 0.9906, domain acc: 0.5815, lr: 0.000523, lamb: 1.7297, time: 10.4050\n","epoch 723: train D loss: 0.6836, train F loss: -1.1480, train acc: 0.9916, domain acc: 0.5712, lr: 0.000523, lamb: 1.7300, time: 10.4575\n","epoch 724: train D loss: 0.6767, train F loss: -1.1360, train acc: 0.9902, domain acc: 0.5880, lr: 0.000522, lamb: 1.7304, time: 10.1566\n","epoch 725: train D loss: 0.6823, train F loss: -1.1322, train acc: 0.9890, domain acc: 0.5700, lr: 0.000522, lamb: 1.7308, time: 10.3987\n","epoch 726: train D loss: 0.6834, train F loss: -1.1413, train acc: 0.9884, domain acc: 0.5738, lr: 0.000521, lamb: 1.7311, time: 10.2536\n","epoch 727: train D loss: 0.6846, train F loss: -1.1502, train acc: 0.9920, domain acc: 0.5653, lr: 0.000521, lamb: 1.7315, time: 10.0217\n","epoch 728: train D loss: 0.6793, train F loss: -1.1424, train acc: 0.9908, domain acc: 0.5784, lr: 0.000520, lamb: 1.7318, time: 10.0788\n","epoch 729: train D loss: 0.6826, train F loss: -1.1482, train acc: 0.9908, domain acc: 0.5721, lr: 0.000520, lamb: 1.7322, time: 9.7949\n","epoch 730: train D loss: 0.6822, train F loss: -1.1466, train acc: 0.9914, domain acc: 0.5727, lr: 0.000519, lamb: 1.7326, time: 10.2768\n","epoch 731: train D loss: 0.6809, train F loss: -1.1395, train acc: 0.9896, domain acc: 0.5776, lr: 0.000519, lamb: 1.7329, time: 9.7789\n","epoch 732: train D loss: 0.6814, train F loss: -1.1524, train acc: 0.9928, domain acc: 0.5723, lr: 0.000518, lamb: 1.7333, time: 9.9459\n","epoch 733: train D loss: 0.6783, train F loss: -1.1390, train acc: 0.9908, domain acc: 0.5828, lr: 0.000518, lamb: 1.7336, time: 9.9474\n","epoch 734: train D loss: 0.6791, train F loss: -1.1357, train acc: 0.9892, domain acc: 0.5825, lr: 0.000517, lamb: 1.7340, time: 9.7995\n","epoch 735: train D loss: 0.6807, train F loss: -1.1392, train acc: 0.9904, domain acc: 0.5746, lr: 0.000517, lamb: 1.7344, time: 10.1672\n","epoch 736: train D loss: 0.6815, train F loss: -1.1499, train acc: 0.9912, domain acc: 0.5748, lr: 0.000516, lamb: 1.7347, time: 9.6355\n","epoch 737: train D loss: 0.6809, train F loss: -1.1437, train acc: 0.9916, domain acc: 0.5716, lr: 0.000516, lamb: 1.7351, time: 9.9035\n","epoch 738: train D loss: 0.6772, train F loss: -1.1340, train acc: 0.9914, domain acc: 0.5817, lr: 0.000515, lamb: 1.7354, time: 9.7433\n","epoch 739: train D loss: 0.6781, train F loss: -1.1290, train acc: 0.9884, domain acc: 0.5846, lr: 0.000515, lamb: 1.7358, time: 9.7957\n","epoch 740: train D loss: 0.6813, train F loss: -1.1488, train acc: 0.9918, domain acc: 0.5756, lr: 0.000514, lamb: 1.7361, time: 9.9039\n","epoch 741: train D loss: 0.6799, train F loss: -1.1435, train acc: 0.9892, domain acc: 0.5796, lr: 0.000514, lamb: 1.7365, time: 9.4397\n","epoch 742: train D loss: 0.6768, train F loss: -1.1375, train acc: 0.9898, domain acc: 0.5795, lr: 0.000513, lamb: 1.7368, time: 10.1234\n","epoch 743: train D loss: 0.6777, train F loss: -1.1313, train acc: 0.9890, domain acc: 0.5848, lr: 0.000513, lamb: 1.7372, time: 9.4790\n","epoch 744: train D loss: 0.6839, train F loss: -1.1504, train acc: 0.9898, domain acc: 0.5674, lr: 0.000512, lamb: 1.7376, time: 9.7132\n","epoch 745: train D loss: 0.6800, train F loss: -1.1490, train acc: 0.9906, domain acc: 0.5730, lr: 0.000512, lamb: 1.7379, time: 9.6318\n","epoch 746: train D loss: 0.6797, train F loss: -1.1495, train acc: 0.9902, domain acc: 0.5809, lr: 0.000511, lamb: 1.7383, time: 9.6138\n","epoch 747: train D loss: 0.6759, train F loss: -1.1420, train acc: 0.9926, domain acc: 0.5872, lr: 0.000511, lamb: 1.7386, time: 9.8453\n","epoch 748: train D loss: 0.6808, train F loss: -1.1423, train acc: 0.9900, domain acc: 0.5773, lr: 0.000510, lamb: 1.7390, time: 9.4249\n","epoch 749: train D loss: 0.6846, train F loss: -1.1548, train acc: 0.9894, domain acc: 0.5669, lr: 0.000509, lamb: 1.7393, time: 9.9803\n","epoch 750: train D loss: 0.6824, train F loss: -1.1475, train acc: 0.9892, domain acc: 0.5758, lr: 0.000509, lamb: 1.7397, time: 9.5365\n","epoch 751: train D loss: 0.6805, train F loss: -1.1450, train acc: 0.9914, domain acc: 0.5747, lr: 0.000508, lamb: 1.7400, time: 9.7806\n","epoch 752: train D loss: 0.6832, train F loss: -1.1543, train acc: 0.9918, domain acc: 0.5717, lr: 0.000508, lamb: 1.7404, time: 9.7891\n","epoch 753: train D loss: 0.6836, train F loss: -1.1499, train acc: 0.9914, domain acc: 0.5715, lr: 0.000507, lamb: 1.7407, time: 9.3616\n","epoch 754: train D loss: 0.6804, train F loss: -1.1469, train acc: 0.9902, domain acc: 0.5764, lr: 0.000507, lamb: 1.7411, time: 9.8028\n","epoch 755: train D loss: 0.6842, train F loss: -1.1590, train acc: 0.9922, domain acc: 0.5675, lr: 0.000506, lamb: 1.7414, time: 9.2392\n","epoch 756: train D loss: 0.6822, train F loss: -1.1472, train acc: 0.9866, domain acc: 0.5712, lr: 0.000506, lamb: 1.7418, time: 9.6507\n","epoch 757: train D loss: 0.6837, train F loss: -1.1577, train acc: 0.9916, domain acc: 0.5755, lr: 0.000505, lamb: 1.7421, time: 9.5154\n","epoch 758: train D loss: 0.6778, train F loss: -1.1448, train acc: 0.9908, domain acc: 0.5824, lr: 0.000505, lamb: 1.7425, time: 9.5378\n","epoch 759: train D loss: 0.6820, train F loss: -1.1532, train acc: 0.9908, domain acc: 0.5685, lr: 0.000504, lamb: 1.7428, time: 9.5915\n","epoch 760: train D loss: 0.6771, train F loss: -1.1408, train acc: 0.9892, domain acc: 0.5874, lr: 0.000504, lamb: 1.7431, time: 9.3454\n","epoch 761: train D loss: 0.6793, train F loss: -1.1431, train acc: 0.9884, domain acc: 0.5769, lr: 0.000503, lamb: 1.7435, time: 9.7449\n","epoch 762: train D loss: 0.6782, train F loss: -1.1447, train acc: 0.9902, domain acc: 0.5776, lr: 0.000503, lamb: 1.7438, time: 9.3328\n","epoch 763: train D loss: 0.6801, train F loss: -1.1440, train acc: 0.9892, domain acc: 0.5762, lr: 0.000502, lamb: 1.7442, time: 9.6412\n","epoch 764: train D loss: 0.6777, train F loss: -1.1501, train acc: 0.9922, domain acc: 0.5843, lr: 0.000502, lamb: 1.7445, time: 9.4154\n","epoch 765: train D loss: 0.6790, train F loss: -1.1539, train acc: 0.9926, domain acc: 0.5830, lr: 0.000501, lamb: 1.7449, time: 9.4607\n","epoch 766: train D loss: 0.6798, train F loss: -1.1513, train acc: 0.9918, domain acc: 0.5791, lr: 0.000501, lamb: 1.7452, time: 9.6121\n","epoch 767: train D loss: 0.6797, train F loss: -1.1435, train acc: 0.9894, domain acc: 0.5855, lr: 0.000500, lamb: 1.7456, time: 10.6045\n","epoch 768: train D loss: 0.6814, train F loss: -1.1526, train acc: 0.9914, domain acc: 0.5722, lr: 0.000500, lamb: 1.7459, time: 10.3262\n","epoch 769: train D loss: 0.6807, train F loss: -1.1518, train acc: 0.9918, domain acc: 0.5796, lr: 0.000499, lamb: 1.7462, time: 9.5504\n","epoch 770: train D loss: 0.6811, train F loss: -1.1544, train acc: 0.9910, domain acc: 0.5791, lr: 0.000499, lamb: 1.7466, time: 9.4051\n","epoch 771: train D loss: 0.6810, train F loss: -1.1527, train acc: 0.9904, domain acc: 0.5700, lr: 0.000498, lamb: 1.7469, time: 9.5526\n","epoch 772: train D loss: 0.6809, train F loss: -1.1537, train acc: 0.9930, domain acc: 0.5724, lr: 0.000498, lamb: 1.7473, time: 9.3294\n","epoch 773: train D loss: 0.6843, train F loss: -1.1564, train acc: 0.9902, domain acc: 0.5698, lr: 0.000497, lamb: 1.7476, time: 9.7662\n","epoch 774: train D loss: 0.6793, train F loss: -1.1492, train acc: 0.9904, domain acc: 0.5823, lr: 0.000497, lamb: 1.7479, time: 9.2063\n","epoch 775: train D loss: 0.6801, train F loss: -1.1582, train acc: 0.9918, domain acc: 0.5796, lr: 0.000496, lamb: 1.7483, time: 9.5952\n","epoch 776: train D loss: 0.6821, train F loss: -1.1576, train acc: 0.9900, domain acc: 0.5737, lr: 0.000496, lamb: 1.7486, time: 9.5409\n","epoch 777: train D loss: 0.6835, train F loss: -1.1573, train acc: 0.9892, domain acc: 0.5697, lr: 0.000495, lamb: 1.7490, time: 9.4229\n","epoch 778: train D loss: 0.6804, train F loss: -1.1581, train acc: 0.9920, domain acc: 0.5774, lr: 0.000495, lamb: 1.7493, time: 9.5787\n","epoch 779: train D loss: 0.6816, train F loss: -1.1515, train acc: 0.9902, domain acc: 0.5777, lr: 0.000494, lamb: 1.7496, time: 9.1934\n","epoch 780: train D loss: 0.6794, train F loss: -1.1389, train acc: 0.9848, domain acc: 0.5799, lr: 0.000494, lamb: 1.7500, time: 9.7269\n","epoch 781: train D loss: 0.6808, train F loss: -1.1525, train acc: 0.9902, domain acc: 0.5808, lr: 0.000493, lamb: 1.7503, time: 9.1963\n","epoch 782: train D loss: 0.6809, train F loss: -1.1567, train acc: 0.9924, domain acc: 0.5719, lr: 0.000493, lamb: 1.7506, time: 9.4883\n","epoch 783: train D loss: 0.6822, train F loss: -1.1607, train acc: 0.9920, domain acc: 0.5678, lr: 0.000492, lamb: 1.7510, time: 9.4532\n","epoch 784: train D loss: 0.6821, train F loss: -1.1534, train acc: 0.9902, domain acc: 0.5769, lr: 0.000492, lamb: 1.7513, time: 9.3033\n","epoch 785: train D loss: 0.6808, train F loss: -1.1579, train acc: 0.9912, domain acc: 0.5753, lr: 0.000491, lamb: 1.7516, time: 9.4464\n","epoch 786: train D loss: 0.6778, train F loss: -1.1565, train acc: 0.9922, domain acc: 0.5833, lr: 0.000491, lamb: 1.7520, time: 9.1276\n","epoch 787: train D loss: 0.6817, train F loss: -1.1590, train acc: 0.9920, domain acc: 0.5713, lr: 0.000490, lamb: 1.7523, time: 9.6089\n","epoch 788: train D loss: 0.6826, train F loss: -1.1572, train acc: 0.9910, domain acc: 0.5663, lr: 0.000490, lamb: 1.7527, time: 9.1615\n","epoch 789: train D loss: 0.6812, train F loss: -1.1556, train acc: 0.9900, domain acc: 0.5783, lr: 0.000490, lamb: 1.7530, time: 9.5271\n","epoch 790: train D loss: 0.6811, train F loss: -1.1580, train acc: 0.9904, domain acc: 0.5760, lr: 0.000489, lamb: 1.7533, time: 9.3755\n","epoch 791: train D loss: 0.6774, train F loss: -1.1518, train acc: 0.9920, domain acc: 0.5863, lr: 0.000489, lamb: 1.7536, time: 9.3627\n","epoch 792: train D loss: 0.6820, train F loss: -1.1669, train acc: 0.9926, domain acc: 0.5744, lr: 0.000488, lamb: 1.7540, time: 9.5066\n","epoch 793: train D loss: 0.6822, train F loss: -1.1607, train acc: 0.9918, domain acc: 0.5694, lr: 0.000488, lamb: 1.7543, time: 9.2696\n","epoch 794: train D loss: 0.6816, train F loss: -1.1575, train acc: 0.9912, domain acc: 0.5749, lr: 0.000487, lamb: 1.7546, time: 9.5982\n","epoch 795: train D loss: 0.6823, train F loss: -1.1606, train acc: 0.9898, domain acc: 0.5711, lr: 0.000487, lamb: 1.7550, time: 9.1596\n","epoch 796: train D loss: 0.6844, train F loss: -1.1702, train acc: 0.9930, domain acc: 0.5685, lr: 0.000486, lamb: 1.7553, time: 9.4690\n","epoch 797: train D loss: 0.6827, train F loss: -1.1653, train acc: 0.9906, domain acc: 0.5733, lr: 0.000486, lamb: 1.7556, time: 9.3143\n","epoch 798: train D loss: 0.6807, train F loss: -1.1621, train acc: 0.9930, domain acc: 0.5765, lr: 0.000485, lamb: 1.7560, time: 9.3793\n","epoch 799: train D loss: 0.6832, train F loss: -1.1523, train acc: 0.9908, domain acc: 0.5691, lr: 0.000485, lamb: 1.7563, time: 9.4435\n","epoch 800: train D loss: 0.6832, train F loss: -1.1610, train acc: 0.9896, domain acc: 0.5709, lr: 0.000484, lamb: 1.7566, time: 9.2674\n","epoch 801: train D loss: 0.6809, train F loss: -1.1531, train acc: 0.9890, domain acc: 0.5806, lr: 0.000484, lamb: 1.7570, time: 9.6668\n","epoch 802: train D loss: 0.6753, train F loss: -1.1454, train acc: 0.9898, domain acc: 0.5880, lr: 0.000483, lamb: 1.7573, time: 9.2134\n","epoch 803: train D loss: 0.6805, train F loss: -1.1572, train acc: 0.9904, domain acc: 0.5759, lr: 0.000483, lamb: 1.7576, time: 9.5899\n","epoch 804: train D loss: 0.6813, train F loss: -1.1637, train acc: 0.9906, domain acc: 0.5756, lr: 0.000482, lamb: 1.7579, time: 9.3464\n","epoch 805: train D loss: 0.6805, train F loss: -1.1649, train acc: 0.9926, domain acc: 0.5735, lr: 0.000482, lamb: 1.7583, time: 9.3854\n","epoch 806: train D loss: 0.6770, train F loss: -1.0930, train acc: 0.9868, domain acc: 0.5852, lr: 0.000481, lamb: 1.7586, time: 9.4832\n","epoch 807: train D loss: 0.6805, train F loss: -1.1545, train acc: 0.9902, domain acc: 0.5781, lr: 0.000481, lamb: 1.7589, time: 9.2667\n","epoch 808: train D loss: 0.6791, train F loss: -1.1590, train acc: 0.9912, domain acc: 0.5782, lr: 0.000480, lamb: 1.7592, time: 9.6780\n","epoch 809: train D loss: 0.6743, train F loss: -1.1484, train acc: 0.9914, domain acc: 0.5927, lr: 0.000480, lamb: 1.7596, time: 9.2390\n","epoch 810: train D loss: 0.6795, train F loss: -1.1601, train acc: 0.9910, domain acc: 0.5839, lr: 0.000479, lamb: 1.7599, time: 9.5831\n","epoch 811: train D loss: 0.6808, train F loss: -1.1595, train acc: 0.9916, domain acc: 0.5782, lr: 0.000479, lamb: 1.7602, time: 9.3100\n","epoch 812: train D loss: 0.6805, train F loss: -1.1614, train acc: 0.9908, domain acc: 0.5771, lr: 0.000478, lamb: 1.7605, time: 9.4702\n","epoch 813: train D loss: 0.6799, train F loss: -1.1610, train acc: 0.9900, domain acc: 0.5790, lr: 0.000478, lamb: 1.7609, time: 9.5550\n","epoch 814: train D loss: 0.6782, train F loss: -1.1569, train acc: 0.9930, domain acc: 0.5814, lr: 0.000477, lamb: 1.7612, time: 9.2663\n","epoch 815: train D loss: 0.6802, train F loss: -1.1663, train acc: 0.9922, domain acc: 0.5818, lr: 0.000477, lamb: 1.7615, time: 9.6332\n","epoch 816: train D loss: 0.6787, train F loss: -1.1602, train acc: 0.9924, domain acc: 0.5808, lr: 0.000476, lamb: 1.7618, time: 9.1651\n","epoch 817: train D loss: 0.6786, train F loss: -1.1546, train acc: 0.9900, domain acc: 0.5801, lr: 0.000476, lamb: 1.7621, time: 9.6410\n","epoch 818: train D loss: 0.6806, train F loss: -1.1568, train acc: 0.9880, domain acc: 0.5769, lr: 0.000476, lamb: 1.7625, time: 9.3601\n","epoch 819: train D loss: 0.6804, train F loss: -1.1575, train acc: 0.9898, domain acc: 0.5794, lr: 0.000475, lamb: 1.7628, time: 9.4115\n","epoch 820: train D loss: 0.6811, train F loss: -1.1603, train acc: 0.9892, domain acc: 0.5757, lr: 0.000475, lamb: 1.7631, time: 9.4657\n","epoch 821: train D loss: 0.6797, train F loss: -1.1678, train acc: 0.9936, domain acc: 0.5792, lr: 0.000474, lamb: 1.7634, time: 9.1658\n","epoch 822: train D loss: 0.6822, train F loss: -1.1680, train acc: 0.9932, domain acc: 0.5734, lr: 0.000474, lamb: 1.7638, time: 9.6026\n","epoch 823: train D loss: 0.6819, train F loss: -1.1671, train acc: 0.9910, domain acc: 0.5732, lr: 0.000473, lamb: 1.7641, time: 9.1819\n","epoch 824: train D loss: 0.6817, train F loss: -1.1717, train acc: 0.9928, domain acc: 0.5757, lr: 0.000473, lamb: 1.7644, time: 9.6006\n","epoch 825: train D loss: 0.6807, train F loss: -1.1699, train acc: 0.9920, domain acc: 0.5711, lr: 0.000472, lamb: 1.7647, time: 9.2388\n","epoch 826: train D loss: 0.6834, train F loss: -1.1708, train acc: 0.9910, domain acc: 0.5686, lr: 0.000472, lamb: 1.7650, time: 9.4466\n","epoch 827: train D loss: 0.6819, train F loss: -1.1682, train acc: 0.9908, domain acc: 0.5745, lr: 0.000471, lamb: 1.7653, time: 9.5591\n","epoch 828: train D loss: 0.6802, train F loss: -1.1718, train acc: 0.9940, domain acc: 0.5729, lr: 0.000471, lamb: 1.7657, time: 9.2879\n","epoch 829: train D loss: 0.6804, train F loss: -1.1634, train acc: 0.9912, domain acc: 0.5763, lr: 0.000470, lamb: 1.7660, time: 9.6079\n","epoch 830: train D loss: 0.6813, train F loss: -1.1621, train acc: 0.9898, domain acc: 0.5714, lr: 0.000470, lamb: 1.7663, time: 9.1513\n","epoch 831: train D loss: 0.6822, train F loss: -1.1718, train acc: 0.9916, domain acc: 0.5746, lr: 0.000469, lamb: 1.7666, time: 9.5756\n","epoch 832: train D loss: 0.6845, train F loss: -1.1655, train acc: 0.9902, domain acc: 0.5688, lr: 0.000469, lamb: 1.7669, time: 9.5157\n","epoch 833: train D loss: 0.6803, train F loss: -1.1662, train acc: 0.9904, domain acc: 0.5757, lr: 0.000468, lamb: 1.7672, time: 9.3941\n","epoch 834: train D loss: 0.6786, train F loss: -1.1634, train acc: 0.9906, domain acc: 0.5769, lr: 0.000468, lamb: 1.7676, time: 9.5046\n","epoch 835: train D loss: 0.6803, train F loss: -1.1722, train acc: 0.9930, domain acc: 0.5724, lr: 0.000467, lamb: 1.7679, time: 9.2792\n","epoch 836: train D loss: 0.6809, train F loss: -1.1727, train acc: 0.9926, domain acc: 0.5782, lr: 0.000467, lamb: 1.7682, time: 9.6335\n","epoch 837: train D loss: 0.6827, train F loss: -1.1721, train acc: 0.9912, domain acc: 0.5694, lr: 0.000467, lamb: 1.7685, time: 9.1722\n","epoch 838: train D loss: 0.6822, train F loss: -1.1732, train acc: 0.9902, domain acc: 0.5687, lr: 0.000466, lamb: 1.7688, time: 9.5399\n","epoch 839: train D loss: 0.6840, train F loss: -1.1728, train acc: 0.9914, domain acc: 0.5645, lr: 0.000466, lamb: 1.7691, time: 9.4046\n","epoch 840: train D loss: 0.6798, train F loss: -1.1729, train acc: 0.9940, domain acc: 0.5796, lr: 0.000465, lamb: 1.7694, time: 9.4075\n","epoch 841: train D loss: 0.6848, train F loss: -1.1797, train acc: 0.9914, domain acc: 0.5672, lr: 0.000465, lamb: 1.7698, time: 9.5721\n","epoch 842: train D loss: 0.6812, train F loss: -1.1706, train acc: 0.9894, domain acc: 0.5702, lr: 0.000464, lamb: 1.7701, time: 9.1766\n","epoch 843: train D loss: 0.6793, train F loss: -1.1716, train acc: 0.9932, domain acc: 0.5789, lr: 0.000464, lamb: 1.7704, time: 9.6338\n","epoch 844: train D loss: 0.6851, train F loss: -1.1746, train acc: 0.9886, domain acc: 0.5646, lr: 0.000463, lamb: 1.7707, time: 9.1707\n","epoch 845: train D loss: 0.6826, train F loss: -1.1815, train acc: 0.9930, domain acc: 0.5713, lr: 0.000463, lamb: 1.7710, time: 9.6278\n","epoch 846: train D loss: 0.6796, train F loss: -1.1720, train acc: 0.9916, domain acc: 0.5742, lr: 0.000462, lamb: 1.7713, time: 9.4090\n","epoch 847: train D loss: 0.6822, train F loss: -1.1774, train acc: 0.9930, domain acc: 0.5681, lr: 0.000462, lamb: 1.7716, time: 9.4388\n","epoch 848: train D loss: 0.6814, train F loss: -1.1719, train acc: 0.9916, domain acc: 0.5740, lr: 0.000461, lamb: 1.7719, time: 9.4741\n","epoch 849: train D loss: 0.6791, train F loss: -1.1652, train acc: 0.9908, domain acc: 0.5836, lr: 0.000461, lamb: 1.7722, time: 9.1179\n","epoch 850: train D loss: 0.6800, train F loss: -1.1642, train acc: 0.9910, domain acc: 0.5772, lr: 0.000461, lamb: 1.7726, time: 9.6444\n","epoch 851: train D loss: 0.6794, train F loss: -1.1659, train acc: 0.9876, domain acc: 0.5756, lr: 0.000460, lamb: 1.7729, time: 9.1903\n","epoch 852: train D loss: 0.6791, train F loss: -1.1658, train acc: 0.9900, domain acc: 0.5809, lr: 0.000460, lamb: 1.7732, time: 9.5392\n","epoch 853: train D loss: 0.6803, train F loss: -1.1699, train acc: 0.9916, domain acc: 0.5763, lr: 0.000459, lamb: 1.7735, time: 9.2851\n","epoch 854: train D loss: 0.6792, train F loss: -1.1649, train acc: 0.9890, domain acc: 0.5782, lr: 0.000459, lamb: 1.7738, time: 9.3864\n","epoch 855: train D loss: 0.6823, train F loss: -1.1732, train acc: 0.9910, domain acc: 0.5743, lr: 0.000458, lamb: 1.7741, time: 9.5319\n","epoch 856: train D loss: 0.6833, train F loss: -1.1790, train acc: 0.9906, domain acc: 0.5625, lr: 0.000458, lamb: 1.7744, time: 9.2322\n","epoch 857: train D loss: 0.6788, train F loss: -1.1664, train acc: 0.9908, domain acc: 0.5774, lr: 0.000457, lamb: 1.7747, time: 9.7241\n","epoch 858: train D loss: 0.6807, train F loss: -1.1721, train acc: 0.9922, domain acc: 0.5706, lr: 0.000457, lamb: 1.7750, time: 9.2631\n","epoch 859: train D loss: 0.6786, train F loss: -1.1617, train acc: 0.9914, domain acc: 0.5844, lr: 0.000456, lamb: 1.7753, time: 9.5981\n","epoch 860: train D loss: 0.6795, train F loss: -1.1636, train acc: 0.9892, domain acc: 0.5780, lr: 0.000456, lamb: 1.7756, time: 9.3283\n","epoch 861: train D loss: 0.6816, train F loss: -1.1716, train acc: 0.9904, domain acc: 0.5693, lr: 0.000455, lamb: 1.7759, time: 9.3535\n","epoch 862: train D loss: 0.6830, train F loss: -1.1824, train acc: 0.9924, domain acc: 0.5713, lr: 0.000455, lamb: 1.7762, time: 9.5504\n","epoch 863: train D loss: 0.6839, train F loss: -1.1834, train acc: 0.9918, domain acc: 0.5683, lr: 0.000455, lamb: 1.7765, time: 9.2146\n","epoch 864: train D loss: 0.6785, train F loss: -1.1618, train acc: 0.9908, domain acc: 0.5764, lr: 0.000454, lamb: 1.7768, time: 9.5850\n","epoch 865: train D loss: 0.6799, train F loss: -1.1746, train acc: 0.9924, domain acc: 0.5695, lr: 0.000454, lamb: 1.7772, time: 9.1801\n","epoch 866: train D loss: 0.6806, train F loss: -1.1721, train acc: 0.9914, domain acc: 0.5801, lr: 0.000453, lamb: 1.7775, time: 9.5603\n","epoch 867: train D loss: 0.6787, train F loss: -1.1598, train acc: 0.9884, domain acc: 0.5748, lr: 0.000453, lamb: 1.7778, time: 9.3357\n","epoch 868: train D loss: 0.6789, train F loss: -1.1729, train acc: 0.9920, domain acc: 0.5843, lr: 0.000452, lamb: 1.7781, time: 9.3165\n","epoch 869: train D loss: 0.6824, train F loss: -1.1786, train acc: 0.9920, domain acc: 0.5713, lr: 0.000452, lamb: 1.7784, time: 9.5564\n","epoch 870: train D loss: 0.6775, train F loss: -1.1713, train acc: 0.9904, domain acc: 0.5864, lr: 0.000451, lamb: 1.7787, time: 9.2246\n","epoch 871: train D loss: 0.6779, train F loss: -1.1773, train acc: 0.9938, domain acc: 0.5839, lr: 0.000451, lamb: 1.7790, time: 9.6496\n","epoch 872: train D loss: 0.6803, train F loss: -1.1771, train acc: 0.9914, domain acc: 0.5736, lr: 0.000450, lamb: 1.7793, time: 9.2556\n","epoch 873: train D loss: 0.6795, train F loss: -1.1770, train acc: 0.9916, domain acc: 0.5791, lr: 0.000450, lamb: 1.7796, time: 9.5425\n","epoch 874: train D loss: 0.6827, train F loss: -1.1820, train acc: 0.9922, domain acc: 0.5726, lr: 0.000450, lamb: 1.7799, time: 9.3862\n","epoch 875: train D loss: 0.6800, train F loss: -1.1810, train acc: 0.9932, domain acc: 0.5726, lr: 0.000449, lamb: 1.7802, time: 9.3751\n","epoch 876: train D loss: 0.6842, train F loss: -1.1836, train acc: 0.9922, domain acc: 0.5683, lr: 0.000449, lamb: 1.7805, time: 9.5811\n","epoch 877: train D loss: 0.6835, train F loss: -1.1844, train acc: 0.9920, domain acc: 0.5723, lr: 0.000448, lamb: 1.7808, time: 9.2679\n","epoch 878: train D loss: 0.6822, train F loss: -1.1760, train acc: 0.9890, domain acc: 0.5743, lr: 0.000448, lamb: 1.7811, time: 9.8051\n","epoch 879: train D loss: 0.6825, train F loss: -1.1773, train acc: 0.9898, domain acc: 0.5679, lr: 0.000447, lamb: 1.7814, time: 9.1691\n","epoch 880: train D loss: 0.6790, train F loss: -1.1744, train acc: 0.9906, domain acc: 0.5787, lr: 0.000447, lamb: 1.7817, time: 9.5515\n","epoch 881: train D loss: 0.6801, train F loss: -1.1786, train acc: 0.9910, domain acc: 0.5761, lr: 0.000446, lamb: 1.7820, time: 9.3491\n","epoch 882: train D loss: 0.6827, train F loss: -1.1792, train acc: 0.9904, domain acc: 0.5675, lr: 0.000446, lamb: 1.7823, time: 9.4076\n","epoch 883: train D loss: 0.6774, train F loss: -1.1770, train acc: 0.9924, domain acc: 0.5837, lr: 0.000446, lamb: 1.7826, time: 9.5346\n","epoch 884: train D loss: 0.6841, train F loss: -1.1869, train acc: 0.9928, domain acc: 0.5698, lr: 0.000445, lamb: 1.7829, time: 9.1663\n","epoch 885: train D loss: 0.6803, train F loss: -1.1851, train acc: 0.9932, domain acc: 0.5756, lr: 0.000445, lamb: 1.7832, time: 9.6647\n","epoch 886: train D loss: 0.6833, train F loss: -1.1827, train acc: 0.9914, domain acc: 0.5688, lr: 0.000444, lamb: 1.7835, time: 9.2065\n","epoch 887: train D loss: 0.6822, train F loss: -1.1855, train acc: 0.9940, domain acc: 0.5678, lr: 0.000444, lamb: 1.7838, time: 9.5140\n","epoch 888: train D loss: 0.6832, train F loss: -1.1781, train acc: 0.9898, domain acc: 0.5681, lr: 0.000443, lamb: 1.7840, time: 9.3020\n","epoch 889: train D loss: 0.6799, train F loss: -1.1743, train acc: 0.9906, domain acc: 0.5784, lr: 0.000443, lamb: 1.7843, time: 9.3420\n","epoch 890: train D loss: 0.6806, train F loss: -1.1775, train acc: 0.9914, domain acc: 0.5787, lr: 0.000442, lamb: 1.7846, time: 9.4371\n","epoch 891: train D loss: 0.6842, train F loss: -1.1911, train acc: 0.9930, domain acc: 0.5625, lr: 0.000442, lamb: 1.7849, time: 9.1420\n","epoch 892: train D loss: 0.6804, train F loss: -1.1819, train acc: 0.9914, domain acc: 0.5780, lr: 0.000442, lamb: 1.7852, time: 9.6539\n","epoch 893: train D loss: 0.6809, train F loss: -1.1857, train acc: 0.9928, domain acc: 0.5737, lr: 0.000441, lamb: 1.7855, time: 9.2117\n","epoch 894: train D loss: 0.6806, train F loss: -1.1813, train acc: 0.9916, domain acc: 0.5753, lr: 0.000441, lamb: 1.7858, time: 9.5733\n","epoch 895: train D loss: 0.6805, train F loss: -1.1794, train acc: 0.9914, domain acc: 0.5747, lr: 0.000440, lamb: 1.7861, time: 9.3597\n","epoch 896: train D loss: 0.6834, train F loss: -1.1873, train acc: 0.9918, domain acc: 0.5709, lr: 0.000440, lamb: 1.7864, time: 9.3589\n","epoch 897: train D loss: 0.6822, train F loss: -1.1779, train acc: 0.9900, domain acc: 0.5707, lr: 0.000439, lamb: 1.7867, time: 9.5622\n","epoch 898: train D loss: 0.6833, train F loss: -1.1925, train acc: 0.9936, domain acc: 0.5703, lr: 0.000439, lamb: 1.7870, time: 9.2611\n","epoch 899: train D loss: 0.6811, train F loss: -1.1872, train acc: 0.9922, domain acc: 0.5745, lr: 0.000438, lamb: 1.7873, time: 9.6285\n","epoch 900: train D loss: 0.6830, train F loss: -1.1921, train acc: 0.9922, domain acc: 0.5694, lr: 0.000438, lamb: 1.7876, time: 9.2487\n","epoch 901: train D loss: 0.6812, train F loss: -1.1771, train acc: 0.9906, domain acc: 0.5701, lr: 0.000438, lamb: 1.7879, time: 9.6131\n","epoch 902: train D loss: 0.6808, train F loss: -1.1836, train acc: 0.9920, domain acc: 0.5707, lr: 0.000437, lamb: 1.7882, time: 9.4661\n","epoch 903: train D loss: 0.6809, train F loss: -1.1802, train acc: 0.9906, domain acc: 0.5741, lr: 0.000437, lamb: 1.7884, time: 9.4072\n","epoch 904: train D loss: 0.6824, train F loss: -1.1828, train acc: 0.9890, domain acc: 0.5703, lr: 0.000436, lamb: 1.7887, time: 9.5327\n","epoch 905: train D loss: 0.6830, train F loss: -1.1894, train acc: 0.9918, domain acc: 0.5596, lr: 0.000436, lamb: 1.7890, time: 9.2049\n","epoch 906: train D loss: 0.6780, train F loss: -1.1821, train acc: 0.9928, domain acc: 0.5766, lr: 0.000435, lamb: 1.7893, time: 9.7110\n","epoch 907: train D loss: 0.6812, train F loss: -1.1815, train acc: 0.9932, domain acc: 0.5736, lr: 0.000435, lamb: 1.7896, time: 9.1689\n","epoch 908: train D loss: 0.6793, train F loss: -1.1806, train acc: 0.9922, domain acc: 0.5806, lr: 0.000435, lamb: 1.7899, time: 9.5279\n","epoch 909: train D loss: 0.6833, train F loss: -1.1878, train acc: 0.9920, domain acc: 0.5677, lr: 0.000434, lamb: 1.7902, time: 9.4112\n","epoch 910: train D loss: 0.6822, train F loss: -1.1928, train acc: 0.9928, domain acc: 0.5733, lr: 0.000434, lamb: 1.7905, time: 9.4060\n","epoch 911: train D loss: 0.6832, train F loss: -1.1904, train acc: 0.9920, domain acc: 0.5683, lr: 0.000433, lamb: 1.7908, time: 9.6442\n","epoch 912: train D loss: 0.6822, train F loss: -1.1906, train acc: 0.9920, domain acc: 0.5666, lr: 0.000433, lamb: 1.7911, time: 9.2853\n","epoch 913: train D loss: 0.6809, train F loss: -1.1867, train acc: 0.9924, domain acc: 0.5688, lr: 0.000432, lamb: 1.7913, time: 9.6992\n","epoch 914: train D loss: 0.6816, train F loss: -1.1898, train acc: 0.9916, domain acc: 0.5700, lr: 0.000432, lamb: 1.7916, time: 9.2487\n","epoch 915: train D loss: 0.6797, train F loss: -1.1800, train acc: 0.9902, domain acc: 0.5785, lr: 0.000432, lamb: 1.7919, time: 9.5096\n","epoch 916: train D loss: 0.6807, train F loss: -1.1880, train acc: 0.9912, domain acc: 0.5732, lr: 0.000431, lamb: 1.7922, time: 9.3860\n","epoch 917: train D loss: 0.6780, train F loss: -1.1840, train acc: 0.9916, domain acc: 0.5795, lr: 0.000431, lamb: 1.7925, time: 9.2943\n","epoch 918: train D loss: 0.6841, train F loss: -1.1944, train acc: 0.9914, domain acc: 0.5610, lr: 0.000430, lamb: 1.7928, time: 9.5877\n","epoch 919: train D loss: 0.6859, train F loss: -1.1982, train acc: 0.9932, domain acc: 0.5538, lr: 0.000430, lamb: 1.7931, time: 9.1357\n","epoch 920: train D loss: 0.6822, train F loss: -1.1923, train acc: 0.9930, domain acc: 0.5727, lr: 0.000429, lamb: 1.7933, time: 9.6820\n","epoch 921: train D loss: 0.6796, train F loss: -1.1902, train acc: 0.9926, domain acc: 0.5753, lr: 0.000429, lamb: 1.7936, time: 9.3134\n","epoch 922: train D loss: 0.6820, train F loss: -1.1890, train acc: 0.9922, domain acc: 0.5694, lr: 0.000429, lamb: 1.7939, time: 9.5268\n","epoch 923: train D loss: 0.6824, train F loss: -1.1897, train acc: 0.9912, domain acc: 0.5725, lr: 0.000428, lamb: 1.7942, time: 9.3431\n","epoch 924: train D loss: 0.6813, train F loss: -1.1938, train acc: 0.9922, domain acc: 0.5769, lr: 0.000428, lamb: 1.7945, time: 9.3315\n","epoch 925: train D loss: 0.6810, train F loss: -1.1870, train acc: 0.9918, domain acc: 0.5723, lr: 0.000427, lamb: 1.7948, time: 9.4831\n","epoch 926: train D loss: 0.6834, train F loss: -1.1955, train acc: 0.9914, domain acc: 0.5688, lr: 0.000427, lamb: 1.7951, time: 9.1289\n","epoch 927: train D loss: 0.6823, train F loss: -1.1974, train acc: 0.9926, domain acc: 0.5716, lr: 0.000426, lamb: 1.7953, time: 9.6365\n","epoch 928: train D loss: 0.6815, train F loss: -1.1889, train acc: 0.9906, domain acc: 0.5720, lr: 0.000426, lamb: 1.7956, time: 9.2417\n","epoch 929: train D loss: 0.6818, train F loss: -1.1877, train acc: 0.9916, domain acc: 0.5729, lr: 0.000426, lamb: 1.7959, time: 9.5923\n","epoch 930: train D loss: 0.6827, train F loss: -1.1963, train acc: 0.9926, domain acc: 0.5744, lr: 0.000425, lamb: 1.7962, time: 9.4337\n","epoch 931: train D loss: 0.6797, train F loss: -1.1897, train acc: 0.9900, domain acc: 0.5751, lr: 0.000425, lamb: 1.7965, time: 9.3312\n","epoch 932: train D loss: 0.6817, train F loss: -1.1905, train acc: 0.9922, domain acc: 0.5739, lr: 0.000424, lamb: 1.7968, time: 9.5525\n","epoch 933: train D loss: 0.6777, train F loss: -1.1801, train acc: 0.9906, domain acc: 0.5794, lr: 0.000424, lamb: 1.7970, time: 9.1678\n","epoch 934: train D loss: 0.6808, train F loss: -1.1920, train acc: 0.9908, domain acc: 0.5735, lr: 0.000423, lamb: 1.7973, time: 9.6363\n","epoch 935: train D loss: 0.6838, train F loss: -1.1928, train acc: 0.9912, domain acc: 0.5701, lr: 0.000423, lamb: 1.7976, time: 9.2468\n","epoch 936: train D loss: 0.6817, train F loss: -1.1932, train acc: 0.9906, domain acc: 0.5665, lr: 0.000423, lamb: 1.7979, time: 9.4933\n","epoch 937: train D loss: 0.6793, train F loss: -1.1882, train acc: 0.9926, domain acc: 0.5777, lr: 0.000422, lamb: 1.7982, time: 9.4938\n","epoch 938: train D loss: 0.6847, train F loss: -1.1985, train acc: 0.9906, domain acc: 0.5651, lr: 0.000422, lamb: 1.7984, time: 9.3716\n","epoch 939: train D loss: 0.6809, train F loss: -1.1840, train acc: 0.9912, domain acc: 0.5768, lr: 0.000421, lamb: 1.7987, time: 9.6426\n","epoch 940: train D loss: 0.6815, train F loss: -1.1946, train acc: 0.9934, domain acc: 0.5709, lr: 0.000421, lamb: 1.7990, time: 9.2020\n","epoch 941: train D loss: 0.6839, train F loss: -1.1999, train acc: 0.9920, domain acc: 0.5676, lr: 0.000420, lamb: 1.7993, time: 9.6728\n","epoch 942: train D loss: 0.6809, train F loss: -1.1949, train acc: 0.9906, domain acc: 0.5689, lr: 0.000420, lamb: 1.7996, time: 9.3511\n","epoch 943: train D loss: 0.6799, train F loss: -1.1952, train acc: 0.9926, domain acc: 0.5772, lr: 0.000420, lamb: 1.7998, time: 9.6225\n","epoch 944: train D loss: 0.6823, train F loss: -1.1933, train acc: 0.9918, domain acc: 0.5665, lr: 0.000419, lamb: 1.8001, time: 9.5427\n","epoch 945: train D loss: 0.6794, train F loss: -1.1851, train acc: 0.9900, domain acc: 0.5787, lr: 0.000419, lamb: 1.8004, time: 9.3009\n","epoch 946: train D loss: 0.6762, train F loss: -1.1882, train acc: 0.9938, domain acc: 0.5882, lr: 0.000418, lamb: 1.8007, time: 9.6148\n","epoch 947: train D loss: 0.6776, train F loss: -1.1722, train acc: 0.9906, domain acc: 0.5782, lr: 0.000418, lamb: 1.8010, time: 9.2038\n","epoch 948: train D loss: 0.6795, train F loss: -1.1900, train acc: 0.9912, domain acc: 0.5741, lr: 0.000418, lamb: 1.8012, time: 9.6007\n","epoch 949: train D loss: 0.6807, train F loss: -1.1933, train acc: 0.9924, domain acc: 0.5779, lr: 0.000417, lamb: 1.8015, time: 9.3653\n","epoch 950: train D loss: 0.6828, train F loss: -1.1983, train acc: 0.9932, domain acc: 0.5731, lr: 0.000417, lamb: 1.8018, time: 9.5659\n","epoch 951: train D loss: 0.6820, train F loss: -1.1901, train acc: 0.9908, domain acc: 0.5717, lr: 0.000416, lamb: 1.8021, time: 9.4043\n","epoch 952: train D loss: 0.6835, train F loss: -1.2017, train acc: 0.9924, domain acc: 0.5641, lr: 0.000416, lamb: 1.8023, time: 9.2761\n","epoch 953: train D loss: 0.6825, train F loss: -1.2001, train acc: 0.9922, domain acc: 0.5675, lr: 0.000415, lamb: 1.8026, time: 9.5315\n","epoch 954: train D loss: 0.6812, train F loss: -1.1974, train acc: 0.9928, domain acc: 0.5727, lr: 0.000415, lamb: 1.8029, time: 9.3301\n","epoch 955: train D loss: 0.6806, train F loss: -1.1989, train acc: 0.9926, domain acc: 0.5825, lr: 0.000415, lamb: 1.8032, time: 9.7321\n","epoch 956: train D loss: 0.6810, train F loss: -1.1907, train acc: 0.9914, domain acc: 0.5709, lr: 0.000414, lamb: 1.8034, time: 9.3112\n","epoch 957: train D loss: 0.6786, train F loss: -1.1927, train acc: 0.9920, domain acc: 0.5771, lr: 0.000414, lamb: 1.8037, time: 9.4993\n","epoch 958: train D loss: 0.6790, train F loss: -1.1904, train acc: 0.9916, domain acc: 0.5787, lr: 0.000413, lamb: 1.8040, time: 9.5493\n","epoch 959: train D loss: 0.6812, train F loss: -1.1932, train acc: 0.9920, domain acc: 0.5764, lr: 0.000413, lamb: 1.8043, time: 9.3058\n","epoch 960: train D loss: 0.6802, train F loss: -1.1920, train acc: 0.9928, domain acc: 0.5733, lr: 0.000413, lamb: 1.8045, time: 9.7258\n","epoch 961: train D loss: 0.6869, train F loss: -1.2042, train acc: 0.9928, domain acc: 0.5582, lr: 0.000412, lamb: 1.8048, time: 9.1492\n","epoch 962: train D loss: 0.6836, train F loss: -1.1922, train acc: 0.9884, domain acc: 0.5637, lr: 0.000412, lamb: 1.8051, time: 9.5905\n","epoch 963: train D loss: 0.6811, train F loss: -1.2007, train acc: 0.9934, domain acc: 0.5730, lr: 0.000411, lamb: 1.8054, time: 9.2811\n","epoch 964: train D loss: 0.6828, train F loss: -1.2010, train acc: 0.9910, domain acc: 0.5716, lr: 0.000411, lamb: 1.8056, time: 9.4424\n","epoch 965: train D loss: 0.6837, train F loss: -1.2049, train acc: 0.9934, domain acc: 0.5671, lr: 0.000410, lamb: 1.8059, time: 9.5766\n","epoch 966: train D loss: 0.6819, train F loss: -1.1996, train acc: 0.9920, domain acc: 0.5726, lr: 0.000410, lamb: 1.8062, time: 9.2681\n","epoch 967: train D loss: 0.6783, train F loss: -1.1929, train acc: 0.9930, domain acc: 0.5826, lr: 0.000410, lamb: 1.8064, time: 9.6409\n","epoch 968: train D loss: 0.6805, train F loss: -1.1951, train acc: 0.9920, domain acc: 0.5786, lr: 0.000409, lamb: 1.8067, time: 9.1275\n","epoch 969: train D loss: 0.6845, train F loss: -1.2080, train acc: 0.9930, domain acc: 0.5644, lr: 0.000409, lamb: 1.8070, time: 9.6864\n","epoch 970: train D loss: 0.6814, train F loss: -1.2031, train acc: 0.9928, domain acc: 0.5704, lr: 0.000408, lamb: 1.8073, time: 9.2800\n","epoch 971: train D loss: 0.6826, train F loss: -1.2070, train acc: 0.9936, domain acc: 0.5713, lr: 0.000408, lamb: 1.8075, time: 9.3746\n","epoch 972: train D loss: 0.6835, train F loss: -1.2075, train acc: 0.9934, domain acc: 0.5619, lr: 0.000408, lamb: 1.8078, time: 9.5387\n","epoch 973: train D loss: 0.6812, train F loss: -1.1974, train acc: 0.9910, domain acc: 0.5714, lr: 0.000407, lamb: 1.8081, time: 9.2245\n","epoch 974: train D loss: 0.6854, train F loss: -1.2030, train acc: 0.9908, domain acc: 0.5604, lr: 0.000407, lamb: 1.8083, time: 9.6194\n","epoch 975: train D loss: 0.6859, train F loss: -1.2106, train acc: 0.9916, domain acc: 0.5512, lr: 0.000406, lamb: 1.8086, time: 9.2077\n","epoch 976: train D loss: 0.6838, train F loss: -1.2034, train acc: 0.9902, domain acc: 0.5588, lr: 0.000406, lamb: 1.8089, time: 9.5678\n","epoch 977: train D loss: 0.6798, train F loss: -1.1899, train acc: 0.9910, domain acc: 0.5765, lr: 0.000406, lamb: 1.8091, time: 9.3138\n","epoch 978: train D loss: 0.6829, train F loss: -1.1974, train acc: 0.9896, domain acc: 0.5683, lr: 0.000405, lamb: 1.8094, time: 9.4010\n","epoch 979: train D loss: 0.6807, train F loss: -1.1959, train acc: 0.9920, domain acc: 0.5780, lr: 0.000405, lamb: 1.8097, time: 9.4922\n","epoch 980: train D loss: 0.6791, train F loss: -1.1964, train acc: 0.9920, domain acc: 0.5808, lr: 0.000404, lamb: 1.8100, time: 9.2104\n","epoch 981: train D loss: 0.6805, train F loss: -1.2016, train acc: 0.9926, domain acc: 0.5726, lr: 0.000404, lamb: 1.8102, time: 9.6804\n","epoch 982: train D loss: 0.6852, train F loss: -1.2009, train acc: 0.9908, domain acc: 0.5632, lr: 0.000404, lamb: 1.8105, time: 9.2126\n","epoch 983: train D loss: 0.6836, train F loss: -1.2050, train acc: 0.9908, domain acc: 0.5684, lr: 0.000403, lamb: 1.8108, time: 9.6853\n","epoch 984: train D loss: 0.6839, train F loss: -1.2045, train acc: 0.9918, domain acc: 0.5660, lr: 0.000403, lamb: 1.8110, time: 9.3643\n","epoch 985: train D loss: 0.6805, train F loss: -1.2015, train acc: 0.9924, domain acc: 0.5757, lr: 0.000402, lamb: 1.8113, time: 9.4021\n","epoch 986: train D loss: 0.6795, train F loss: -1.2010, train acc: 0.9944, domain acc: 0.5757, lr: 0.000402, lamb: 1.8116, time: 9.4837\n","epoch 987: train D loss: 0.6827, train F loss: -1.2080, train acc: 0.9928, domain acc: 0.5628, lr: 0.000402, lamb: 1.8118, time: 9.2322\n","epoch 988: train D loss: 0.6831, train F loss: -1.2074, train acc: 0.9928, domain acc: 0.5669, lr: 0.000401, lamb: 1.8121, time: 9.6423\n","epoch 989: train D loss: 0.6860, train F loss: -1.2146, train acc: 0.9926, domain acc: 0.5626, lr: 0.000401, lamb: 1.8124, time: 9.2309\n","epoch 990: train D loss: 0.6839, train F loss: -1.2029, train acc: 0.9914, domain acc: 0.5594, lr: 0.000400, lamb: 1.8126, time: 9.6100\n","epoch 991: train D loss: 0.6815, train F loss: -1.2075, train acc: 0.9930, domain acc: 0.5669, lr: 0.000400, lamb: 1.8129, time: 9.3898\n","epoch 992: train D loss: 0.6830, train F loss: -1.2103, train acc: 0.9922, domain acc: 0.5686, lr: 0.000400, lamb: 1.8132, time: 9.4077\n","epoch 993: train D loss: 0.6839, train F loss: -1.2081, train acc: 0.9916, domain acc: 0.5663, lr: 0.000399, lamb: 1.8134, time: 9.5729\n","epoch 994: train D loss: 0.6834, train F loss: -1.2110, train acc: 0.9926, domain acc: 0.5672, lr: 0.000399, lamb: 1.8137, time: 9.3023\n","epoch 995: train D loss: 0.6846, train F loss: -1.2144, train acc: 0.9918, domain acc: 0.5644, lr: 0.000398, lamb: 1.8139, time: 9.6892\n","epoch 996: train D loss: 0.6836, train F loss: -1.2068, train acc: 0.9912, domain acc: 0.5686, lr: 0.000398, lamb: 1.8142, time: 9.2803\n","epoch 997: train D loss: 0.6811, train F loss: -1.2081, train acc: 0.9940, domain acc: 0.5767, lr: 0.000398, lamb: 1.8145, time: 9.5969\n","epoch 998: train D loss: 0.6848, train F loss: -1.2021, train acc: 0.9908, domain acc: 0.5685, lr: 0.000397, lamb: 1.8147, time: 9.4215\n","epoch 999: train D loss: 0.6833, train F loss: -1.2088, train acc: 0.9930, domain acc: 0.5658, lr: 0.000397, lamb: 1.8150, time: 9.3858\n","epoch 1000: train D loss: 0.6862, train F loss: -1.2082, train acc: 0.9924, domain acc: 0.5601, lr: 0.000396, lamb: 1.8153, time: 9.6446\n","epoch 1001: train D loss: 0.6832, train F loss: -1.2057, train acc: 0.9918, domain acc: 0.5672, lr: 0.000396, lamb: 1.8155, time: 9.3171\n","epoch 1002: train D loss: 0.6834, train F loss: -1.2135, train acc: 0.9934, domain acc: 0.5669, lr: 0.000396, lamb: 1.8158, time: 9.7472\n","epoch 1003: train D loss: 0.6838, train F loss: -1.2157, train acc: 0.9938, domain acc: 0.5657, lr: 0.000395, lamb: 1.8160, time: 9.2353\n","epoch 1004: train D loss: 0.6841, train F loss: -1.2208, train acc: 0.9954, domain acc: 0.5658, lr: 0.000395, lamb: 1.8163, time: 9.5389\n","epoch 1005: train D loss: 0.6846, train F loss: -1.2155, train acc: 0.9944, domain acc: 0.5667, lr: 0.000394, lamb: 1.8166, time: 9.4596\n","epoch 1006: train D loss: 0.6796, train F loss: -1.1967, train acc: 0.9924, domain acc: 0.5799, lr: 0.000394, lamb: 1.8168, time: 9.3953\n","epoch 1007: train D loss: 0.6837, train F loss: -1.2066, train acc: 0.9922, domain acc: 0.5668, lr: 0.000394, lamb: 1.8171, time: 9.6202\n","epoch 1008: train D loss: 0.6817, train F loss: -1.2047, train acc: 0.9906, domain acc: 0.5755, lr: 0.000393, lamb: 1.8174, time: 9.1963\n","epoch 1009: train D loss: 0.6834, train F loss: -1.2160, train acc: 0.9946, domain acc: 0.5679, lr: 0.000393, lamb: 1.8176, time: 9.6201\n","epoch 1010: train D loss: 0.6812, train F loss: -1.2058, train acc: 0.9920, domain acc: 0.5690, lr: 0.000392, lamb: 1.8179, time: 9.3109\n","epoch 1011: train D loss: 0.6800, train F loss: -1.1939, train acc: 0.9894, domain acc: 0.5762, lr: 0.000392, lamb: 1.8181, time: 9.5565\n","epoch 1012: train D loss: 0.6822, train F loss: -1.2137, train acc: 0.9926, domain acc: 0.5705, lr: 0.000392, lamb: 1.8184, time: 9.5147\n","epoch 1013: train D loss: 0.6808, train F loss: -1.2067, train acc: 0.9928, domain acc: 0.5773, lr: 0.000391, lamb: 1.8187, time: 9.3218\n","epoch 1014: train D loss: 0.6825, train F loss: -1.2042, train acc: 0.9904, domain acc: 0.5730, lr: 0.000391, lamb: 1.8189, time: 9.6828\n","epoch 1015: train D loss: 0.6829, train F loss: -1.2103, train acc: 0.9916, domain acc: 0.5670, lr: 0.000390, lamb: 1.8192, time: 9.2163\n","epoch 1016: train D loss: 0.6800, train F loss: -1.2116, train acc: 0.9934, domain acc: 0.5745, lr: 0.000390, lamb: 1.8194, time: 9.7385\n","epoch 1017: train D loss: 0.6829, train F loss: -1.2128, train acc: 0.9934, domain acc: 0.5715, lr: 0.000390, lamb: 1.8197, time: 9.3554\n","epoch 1018: train D loss: 0.6827, train F loss: -1.2142, train acc: 0.9934, domain acc: 0.5674, lr: 0.000389, lamb: 1.8199, time: 9.4739\n","epoch 1019: train D loss: 0.6816, train F loss: -1.2156, train acc: 0.9942, domain acc: 0.5696, lr: 0.000389, lamb: 1.8202, time: 9.4938\n","epoch 1020: train D loss: 0.6798, train F loss: -1.2038, train acc: 0.9914, domain acc: 0.5752, lr: 0.000388, lamb: 1.8205, time: 9.2818\n","epoch 1021: train D loss: 0.6798, train F loss: -1.2044, train acc: 0.9930, domain acc: 0.5765, lr: 0.000388, lamb: 1.8207, time: 9.7330\n","epoch 1022: train D loss: 0.6769, train F loss: -1.1991, train acc: 0.9918, domain acc: 0.5906, lr: 0.000388, lamb: 1.8210, time: 9.8834\n","epoch 1023: train D loss: 0.6821, train F loss: -1.2072, train acc: 0.9930, domain acc: 0.5695, lr: 0.000387, lamb: 1.8212, time: 10.1759\n","epoch 1024: train D loss: 0.6791, train F loss: -1.2078, train acc: 0.9936, domain acc: 0.5778, lr: 0.000387, lamb: 1.8215, time: 9.4597\n","epoch 1025: train D loss: 0.6791, train F loss: -1.2119, train acc: 0.9952, domain acc: 0.5814, lr: 0.000387, lamb: 1.8218, time: 9.5046\n","epoch 1026: train D loss: 0.6834, train F loss: -1.2137, train acc: 0.9922, domain acc: 0.5698, lr: 0.000386, lamb: 1.8220, time: 9.6091\n","epoch 1027: train D loss: 0.6820, train F loss: -1.2128, train acc: 0.9920, domain acc: 0.5757, lr: 0.000386, lamb: 1.8223, time: 9.2271\n","epoch 1028: train D loss: 0.6821, train F loss: -1.2109, train acc: 0.9920, domain acc: 0.5718, lr: 0.000385, lamb: 1.8225, time: 9.6265\n","epoch 1029: train D loss: 0.6838, train F loss: -1.2131, train acc: 0.9916, domain acc: 0.5653, lr: 0.000385, lamb: 1.8228, time: 9.2588\n","epoch 1030: train D loss: 0.6779, train F loss: -1.2058, train acc: 0.9912, domain acc: 0.5791, lr: 0.000385, lamb: 1.8230, time: 9.4771\n","epoch 1031: train D loss: 0.6829, train F loss: -1.2134, train acc: 0.9918, domain acc: 0.5636, lr: 0.000384, lamb: 1.8233, time: 9.3892\n","epoch 1032: train D loss: 0.6827, train F loss: -1.2161, train acc: 0.9932, domain acc: 0.5683, lr: 0.000384, lamb: 1.8235, time: 9.3620\n","epoch 1033: train D loss: 0.6809, train F loss: -1.2089, train acc: 0.9924, domain acc: 0.5726, lr: 0.000383, lamb: 1.8238, time: 9.6188\n","epoch 1034: train D loss: 0.6832, train F loss: -1.2140, train acc: 0.9922, domain acc: 0.5662, lr: 0.000383, lamb: 1.8240, time: 9.1731\n","epoch 1035: train D loss: 0.6804, train F loss: -1.2130, train acc: 0.9928, domain acc: 0.5739, lr: 0.000383, lamb: 1.8243, time: 9.6718\n","epoch 1036: train D loss: 0.6852, train F loss: -1.2247, train acc: 0.9948, domain acc: 0.5603, lr: 0.000382, lamb: 1.8246, time: 9.2973\n","epoch 1037: train D loss: 0.6846, train F loss: -1.2240, train acc: 0.9936, domain acc: 0.5650, lr: 0.000382, lamb: 1.8248, time: 9.5403\n","epoch 1038: train D loss: 0.6836, train F loss: -1.2258, train acc: 0.9942, domain acc: 0.5619, lr: 0.000382, lamb: 1.8251, time: 9.4497\n","epoch 1039: train D loss: 0.6856, train F loss: -1.2186, train acc: 0.9916, domain acc: 0.5643, lr: 0.000381, lamb: 1.8253, time: 9.3133\n","epoch 1040: train D loss: 0.6854, train F loss: -1.2219, train acc: 0.9922, domain acc: 0.5594, lr: 0.000381, lamb: 1.8256, time: 9.9511\n","epoch 1041: train D loss: 0.6805, train F loss: -1.2178, train acc: 0.9942, domain acc: 0.5743, lr: 0.000380, lamb: 1.8258, time: 9.5957\n","epoch 1042: train D loss: 0.6841, train F loss: -1.2236, train acc: 0.9934, domain acc: 0.5657, lr: 0.000380, lamb: 1.8261, time: 9.6720\n","epoch 1043: train D loss: 0.6866, train F loss: -1.2206, train acc: 0.9906, domain acc: 0.5606, lr: 0.000380, lamb: 1.8263, time: 9.4745\n","epoch 1044: train D loss: 0.6841, train F loss: -1.2198, train acc: 0.9922, domain acc: 0.5691, lr: 0.000379, lamb: 1.8266, time: 9.4335\n","epoch 1045: train D loss: 0.6836, train F loss: -1.2079, train acc: 0.9910, domain acc: 0.5674, lr: 0.000379, lamb: 1.8268, time: 9.5000\n","epoch 1046: train D loss: 0.6839, train F loss: -1.2212, train acc: 0.9934, domain acc: 0.5699, lr: 0.000379, lamb: 1.8271, time: 9.2705\n","epoch 1047: train D loss: 0.6821, train F loss: -1.2213, train acc: 0.9932, domain acc: 0.5647, lr: 0.000378, lamb: 1.8273, time: 9.6129\n","epoch 1048: train D loss: 0.6829, train F loss: -1.2125, train acc: 0.9916, domain acc: 0.5687, lr: 0.000378, lamb: 1.8276, time: 9.5731\n","epoch 1049: train D loss: 0.6834, train F loss: -1.2201, train acc: 0.9928, domain acc: 0.5679, lr: 0.000377, lamb: 1.8278, time: 10.5426\n","epoch 1050: train D loss: 0.6820, train F loss: -1.2185, train acc: 0.9934, domain acc: 0.5730, lr: 0.000377, lamb: 1.8281, time: 10.3169\n","epoch 1051: train D loss: 0.6822, train F loss: -1.2194, train acc: 0.9938, domain acc: 0.5715, lr: 0.000377, lamb: 1.8283, time: 9.6727\n","epoch 1052: train D loss: 0.6833, train F loss: -1.2197, train acc: 0.9926, domain acc: 0.5666, lr: 0.000376, lamb: 1.8286, time: 9.7112\n","epoch 1053: train D loss: 0.6837, train F loss: -1.2270, train acc: 0.9954, domain acc: 0.5670, lr: 0.000376, lamb: 1.8288, time: 9.2442\n","epoch 1054: train D loss: 0.6814, train F loss: -1.2119, train acc: 0.9918, domain acc: 0.5742, lr: 0.000376, lamb: 1.8291, time: 9.5933\n","epoch 1055: train D loss: 0.6841, train F loss: -1.2169, train acc: 0.9918, domain acc: 0.5731, lr: 0.000375, lamb: 1.8293, time: 9.4397\n","epoch 1056: train D loss: 0.6830, train F loss: -1.2212, train acc: 0.9934, domain acc: 0.5656, lr: 0.000375, lamb: 1.8296, time: 9.3886\n","epoch 1057: train D loss: 0.6818, train F loss: -1.2196, train acc: 0.9928, domain acc: 0.5724, lr: 0.000374, lamb: 1.8298, time: 9.5951\n","epoch 1058: train D loss: 0.6840, train F loss: -1.2306, train acc: 0.9952, domain acc: 0.5606, lr: 0.000374, lamb: 1.8301, time: 9.0868\n","epoch 1059: train D loss: 0.6862, train F loss: -1.2268, train acc: 0.9932, domain acc: 0.5597, lr: 0.000374, lamb: 1.8303, time: 9.6975\n","epoch 1060: train D loss: 0.6834, train F loss: -1.2258, train acc: 0.9930, domain acc: 0.5665, lr: 0.000373, lamb: 1.8306, time: 9.3273\n","epoch 1061: train D loss: 0.6833, train F loss: -1.2123, train acc: 0.9930, domain acc: 0.5681, lr: 0.000373, lamb: 1.8308, time: 9.7842\n","epoch 1062: train D loss: 0.6806, train F loss: -1.2242, train acc: 0.9952, domain acc: 0.5771, lr: 0.000373, lamb: 1.8311, time: 10.2981\n","epoch 1063: train D loss: 0.6836, train F loss: -1.2145, train acc: 0.9910, domain acc: 0.5677, lr: 0.000372, lamb: 1.8313, time: 10.6765\n","epoch 1064: train D loss: 0.6809, train F loss: -1.2188, train acc: 0.9936, domain acc: 0.5687, lr: 0.000372, lamb: 1.8316, time: 9.9039\n","epoch 1065: train D loss: 0.6820, train F loss: -1.2147, train acc: 0.9920, domain acc: 0.5672, lr: 0.000371, lamb: 1.8318, time: 9.8143\n","epoch 1066: train D loss: 0.6807, train F loss: -1.2196, train acc: 0.9930, domain acc: 0.5783, lr: 0.000371, lamb: 1.8321, time: 9.6344\n","epoch 1067: train D loss: 0.6807, train F loss: -1.2090, train acc: 0.9922, domain acc: 0.5777, lr: 0.000371, lamb: 1.8323, time: 9.3492\n","epoch 1068: train D loss: 0.6830, train F loss: -1.2160, train acc: 0.9900, domain acc: 0.5697, lr: 0.000370, lamb: 1.8325, time: 9.2848\n","epoch 1069: train D loss: 0.6817, train F loss: -1.2213, train acc: 0.9932, domain acc: 0.5702, lr: 0.000370, lamb: 1.8328, time: 9.3001\n","epoch 1070: train D loss: 0.6810, train F loss: -1.2202, train acc: 0.9926, domain acc: 0.5673, lr: 0.000370, lamb: 1.8330, time: 9.6152\n","epoch 1071: train D loss: 0.6832, train F loss: -1.2262, train acc: 0.9944, domain acc: 0.5658, lr: 0.000369, lamb: 1.8333, time: 10.0205\n","epoch 1072: train D loss: 0.6821, train F loss: -1.2207, train acc: 0.9920, domain acc: 0.5715, lr: 0.000369, lamb: 1.8335, time: 10.2896\n","epoch 1073: train D loss: 0.6807, train F loss: -1.2228, train acc: 0.9928, domain acc: 0.5749, lr: 0.000368, lamb: 1.8338, time: 9.5334\n","epoch 1074: train D loss: 0.6819, train F loss: -1.2149, train acc: 0.9918, domain acc: 0.5752, lr: 0.000368, lamb: 1.8340, time: 10.1481\n","epoch 1075: train D loss: 0.6834, train F loss: -1.2252, train acc: 0.9932, domain acc: 0.5672, lr: 0.000368, lamb: 1.8343, time: 9.5843\n","epoch 1076: train D loss: 0.6793, train F loss: -1.2143, train acc: 0.9928, domain acc: 0.5769, lr: 0.000367, lamb: 1.8345, time: 9.5044\n","epoch 1077: train D loss: 0.6816, train F loss: -1.2177, train acc: 0.9926, domain acc: 0.5752, lr: 0.000367, lamb: 1.8348, time: 9.4923\n","epoch 1078: train D loss: 0.6826, train F loss: -1.2198, train acc: 0.9918, domain acc: 0.5700, lr: 0.000367, lamb: 1.8350, time: 9.5554\n","epoch 1079: train D loss: 0.6821, train F loss: -1.2201, train acc: 0.9932, domain acc: 0.5697, lr: 0.000366, lamb: 1.8352, time: 9.8056\n","epoch 1080: train D loss: 0.6827, train F loss: -1.2166, train acc: 0.9904, domain acc: 0.5682, lr: 0.000366, lamb: 1.8355, time: 9.6411\n","epoch 1081: train D loss: 0.6775, train F loss: -1.2142, train acc: 0.9934, domain acc: 0.5842, lr: 0.000365, lamb: 1.8357, time: 9.9118\n","epoch 1082: train D loss: 0.6827, train F loss: -1.2058, train acc: 0.9916, domain acc: 0.5620, lr: 0.000365, lamb: 1.8360, time: 9.8523\n","epoch 1083: train D loss: 0.6795, train F loss: -1.2089, train acc: 0.9912, domain acc: 0.5784, lr: 0.000365, lamb: 1.8362, time: 9.3096\n","epoch 1084: train D loss: 0.6821, train F loss: -1.2219, train acc: 0.9924, domain acc: 0.5691, lr: 0.000364, lamb: 1.8365, time: 9.2341\n","epoch 1085: train D loss: 0.6827, train F loss: -1.2240, train acc: 0.9926, domain acc: 0.5719, lr: 0.000364, lamb: 1.8367, time: 9.2761\n","epoch 1086: train D loss: 0.6838, train F loss: -1.2284, train acc: 0.9934, domain acc: 0.5613, lr: 0.000364, lamb: 1.8369, time: 9.2887\n","epoch 1087: train D loss: 0.6830, train F loss: -1.2259, train acc: 0.9926, domain acc: 0.5684, lr: 0.000363, lamb: 1.8372, time: 9.2297\n","epoch 1088: train D loss: 0.6798, train F loss: -1.2171, train acc: 0.9926, domain acc: 0.5752, lr: 0.000363, lamb: 1.8374, time: 9.2187\n","epoch 1089: train D loss: 0.6809, train F loss: -1.2217, train acc: 0.9924, domain acc: 0.5678, lr: 0.000363, lamb: 1.8377, time: 9.1979\n","epoch 1090: train D loss: 0.6815, train F loss: -1.2206, train acc: 0.9924, domain acc: 0.5756, lr: 0.000362, lamb: 1.8379, time: 9.3114\n","epoch 1091: train D loss: 0.6825, train F loss: -1.2221, train acc: 0.9924, domain acc: 0.5683, lr: 0.000362, lamb: 1.8381, time: 9.2003\n","epoch 1092: train D loss: 0.6854, train F loss: -1.2330, train acc: 0.9934, domain acc: 0.5692, lr: 0.000361, lamb: 1.8384, time: 9.2042\n","epoch 1093: train D loss: 0.6838, train F loss: -1.2304, train acc: 0.9942, domain acc: 0.5683, lr: 0.000361, lamb: 1.8386, time: 9.1547\n","epoch 1094: train D loss: 0.6837, train F loss: -1.2273, train acc: 0.9934, domain acc: 0.5679, lr: 0.000361, lamb: 1.8389, time: 9.6276\n","epoch 1095: train D loss: 0.6830, train F loss: -1.2231, train acc: 0.9920, domain acc: 0.5654, lr: 0.000360, lamb: 1.8391, time: 9.5018\n","epoch 1096: train D loss: 0.6849, train F loss: -1.2326, train acc: 0.9932, domain acc: 0.5608, lr: 0.000360, lamb: 1.8394, time: 9.3743\n","epoch 1097: train D loss: 0.6833, train F loss: -1.2251, train acc: 0.9922, domain acc: 0.5667, lr: 0.000360, lamb: 1.8396, time: 9.1933\n","epoch 1098: train D loss: 0.6835, train F loss: -1.2266, train acc: 0.9932, domain acc: 0.5689, lr: 0.000359, lamb: 1.8398, time: 9.2184\n","epoch 1099: train D loss: 0.6817, train F loss: -1.2306, train acc: 0.9944, domain acc: 0.5692, lr: 0.000359, lamb: 1.8401, time: 9.2550\n","epoch 1100: train D loss: 0.6815, train F loss: -1.2285, train acc: 0.9940, domain acc: 0.5736, lr: 0.000359, lamb: 1.8403, time: 9.3909\n","epoch 1101: train D loss: 0.6844, train F loss: -1.2295, train acc: 0.9932, domain acc: 0.5600, lr: 0.000358, lamb: 1.8405, time: 9.3172\n","epoch 1102: train D loss: 0.6843, train F loss: -1.2307, train acc: 0.9940, domain acc: 0.5595, lr: 0.000358, lamb: 1.8408, time: 9.1803\n","epoch 1103: train D loss: 0.6827, train F loss: -1.2333, train acc: 0.9940, domain acc: 0.5670, lr: 0.000358, lamb: 1.8410, time: 9.2415\n","epoch 1104: train D loss: 0.6849, train F loss: -1.2337, train acc: 0.9934, domain acc: 0.5655, lr: 0.000357, lamb: 1.8413, time: 9.4360\n","epoch 1105: train D loss: 0.6861, train F loss: -1.2366, train acc: 0.9938, domain acc: 0.5586, lr: 0.000357, lamb: 1.8415, time: 9.3561\n","epoch 1106: train D loss: 0.6838, train F loss: -1.2332, train acc: 0.9932, domain acc: 0.5667, lr: 0.000356, lamb: 1.8417, time: 9.8336\n","epoch 1107: train D loss: 0.6840, train F loss: -1.2284, train acc: 0.9916, domain acc: 0.5603, lr: 0.000356, lamb: 1.8420, time: 9.2958\n","epoch 1108: train D loss: 0.6836, train F loss: -1.2359, train acc: 0.9946, domain acc: 0.5695, lr: 0.000356, lamb: 1.8422, time: 9.2619\n","epoch 1109: train D loss: 0.6826, train F loss: -1.2297, train acc: 0.9934, domain acc: 0.5732, lr: 0.000355, lamb: 1.8424, time: 9.3040\n","epoch 1110: train D loss: 0.6801, train F loss: -1.2243, train acc: 0.9922, domain acc: 0.5715, lr: 0.000355, lamb: 1.8427, time: 9.2549\n","epoch 1111: train D loss: 0.6865, train F loss: -1.2362, train acc: 0.9926, domain acc: 0.5603, lr: 0.000355, lamb: 1.8429, time: 9.2961\n","epoch 1112: train D loss: 0.6827, train F loss: -1.2293, train acc: 0.9934, domain acc: 0.5712, lr: 0.000354, lamb: 1.8432, time: 9.2367\n","epoch 1113: train D loss: 0.6842, train F loss: -1.2262, train acc: 0.9910, domain acc: 0.5645, lr: 0.000354, lamb: 1.8434, time: 9.2066\n","epoch 1114: train D loss: 0.6856, train F loss: -1.2301, train acc: 0.9914, domain acc: 0.5618, lr: 0.000354, lamb: 1.8436, time: 9.2160\n","epoch 1115: train D loss: 0.6826, train F loss: -1.2253, train acc: 0.9934, domain acc: 0.5703, lr: 0.000353, lamb: 1.8439, time: 9.3273\n","epoch 1116: train D loss: 0.6833, train F loss: -1.2316, train acc: 0.9932, domain acc: 0.5660, lr: 0.000353, lamb: 1.8441, time: 9.9859\n","epoch 1117: train D loss: 0.6829, train F loss: -1.2283, train acc: 0.9940, domain acc: 0.5676, lr: 0.000353, lamb: 1.8443, time: 10.9507\n","epoch 1118: train D loss: 0.6857, train F loss: -1.2395, train acc: 0.9944, domain acc: 0.5579, lr: 0.000352, lamb: 1.8446, time: 10.0434\n","epoch 1119: train D loss: 0.6815, train F loss: -1.2329, train acc: 0.9946, domain acc: 0.5713, lr: 0.000352, lamb: 1.8448, time: 10.5585\n","epoch 1120: train D loss: 0.6857, train F loss: -1.2405, train acc: 0.9926, domain acc: 0.5583, lr: 0.000352, lamb: 1.8450, time: 11.7493\n","epoch 1121: train D loss: 0.6854, train F loss: -1.2357, train acc: 0.9930, domain acc: 0.5641, lr: 0.000351, lamb: 1.8453, time: 11.6238\n","epoch 1122: train D loss: 0.6862, train F loss: -1.2369, train acc: 0.9922, domain acc: 0.5535, lr: 0.000351, lamb: 1.8455, time: 10.0286\n","epoch 1123: train D loss: 0.6850, train F loss: -1.2417, train acc: 0.9948, domain acc: 0.5624, lr: 0.000350, lamb: 1.8457, time: 10.9125\n","epoch 1124: train D loss: 0.6840, train F loss: -1.2394, train acc: 0.9938, domain acc: 0.5638, lr: 0.000350, lamb: 1.8460, time: 11.2786\n","epoch 1125: train D loss: 0.6830, train F loss: -1.2304, train acc: 0.9928, domain acc: 0.5711, lr: 0.000350, lamb: 1.8462, time: 11.4671\n","epoch 1126: train D loss: 0.6833, train F loss: -1.2370, train acc: 0.9928, domain acc: 0.5683, lr: 0.000349, lamb: 1.8464, time: 10.7107\n","epoch 1127: train D loss: 0.6852, train F loss: -1.2359, train acc: 0.9932, domain acc: 0.5626, lr: 0.000349, lamb: 1.8467, time: 9.7928\n","epoch 1128: train D loss: 0.6808, train F loss: -1.2154, train acc: 0.9908, domain acc: 0.5736, lr: 0.000349, lamb: 1.8469, time: 11.3700\n","epoch 1129: train D loss: 0.6822, train F loss: -1.2303, train acc: 0.9920, domain acc: 0.5663, lr: 0.000348, lamb: 1.8471, time: 11.4203\n","epoch 1130: train D loss: 0.6854, train F loss: -1.2341, train acc: 0.9918, domain acc: 0.5625, lr: 0.000348, lamb: 1.8474, time: 11.3849\n","epoch 1131: train D loss: 0.6797, train F loss: -1.2289, train acc: 0.9938, domain acc: 0.5740, lr: 0.000348, lamb: 1.8476, time: 11.3638\n","epoch 1132: train D loss: 0.6820, train F loss: -1.2272, train acc: 0.9926, domain acc: 0.5725, lr: 0.000347, lamb: 1.8478, time: 10.7103\n","epoch 1133: train D loss: 0.6810, train F loss: -1.1954, train acc: 0.9884, domain acc: 0.5688, lr: 0.000347, lamb: 1.8481, time: 9.9216\n","epoch 1134: train D loss: 0.6815, train F loss: -1.2334, train acc: 0.9938, domain acc: 0.5741, lr: 0.000347, lamb: 1.8483, time: 10.4448\n","epoch 1135: train D loss: 0.6802, train F loss: -1.2249, train acc: 0.9934, domain acc: 0.5763, lr: 0.000346, lamb: 1.8485, time: 9.8205\n","epoch 1136: train D loss: 0.6813, train F loss: -1.2275, train acc: 0.9924, domain acc: 0.5713, lr: 0.000346, lamb: 1.8488, time: 9.7704\n","epoch 1137: train D loss: 0.6823, train F loss: -1.2255, train acc: 0.9916, domain acc: 0.5688, lr: 0.000346, lamb: 1.8490, time: 9.9778\n","epoch 1138: train D loss: 0.6810, train F loss: -1.2251, train acc: 0.9936, domain acc: 0.5795, lr: 0.000345, lamb: 1.8492, time: 9.6204\n","epoch 1139: train D loss: 0.6800, train F loss: -1.2285, train acc: 0.9934, domain acc: 0.5770, lr: 0.000345, lamb: 1.8495, time: 9.5658\n","epoch 1140: train D loss: 0.6841, train F loss: -1.2351, train acc: 0.9922, domain acc: 0.5653, lr: 0.000345, lamb: 1.8497, time: 10.8345\n","epoch 1141: train D loss: 0.6840, train F loss: -1.2298, train acc: 0.9908, domain acc: 0.5687, lr: 0.000344, lamb: 1.8499, time: 11.5095\n","epoch 1142: train D loss: 0.6822, train F loss: -1.2337, train acc: 0.9928, domain acc: 0.5786, lr: 0.000344, lamb: 1.8502, time: 11.5727\n","epoch 1143: train D loss: 0.6805, train F loss: -1.2357, train acc: 0.9942, domain acc: 0.5722, lr: 0.000344, lamb: 1.8504, time: 11.1677\n","epoch 1144: train D loss: 0.6810, train F loss: -1.2308, train acc: 0.9932, domain acc: 0.5734, lr: 0.000343, lamb: 1.8506, time: 12.0431\n","epoch 1145: train D loss: 0.6811, train F loss: -1.2334, train acc: 0.9936, domain acc: 0.5750, lr: 0.000343, lamb: 1.8508, time: 11.1003\n","epoch 1146: train D loss: 0.6831, train F loss: -1.2368, train acc: 0.9950, domain acc: 0.5650, lr: 0.000342, lamb: 1.8511, time: 11.3423\n","epoch 1147: train D loss: 0.6819, train F loss: -1.2278, train acc: 0.9914, domain acc: 0.5733, lr: 0.000342, lamb: 1.8513, time: 9.9884\n","epoch 1148: train D loss: 0.6810, train F loss: -1.2328, train acc: 0.9946, domain acc: 0.5804, lr: 0.000342, lamb: 1.8515, time: 9.4806\n","epoch 1149: train D loss: 0.6829, train F loss: -1.2376, train acc: 0.9932, domain acc: 0.5655, lr: 0.000341, lamb: 1.8518, time: 9.8064\n","epoch 1150: train D loss: 0.6868, train F loss: -1.2428, train acc: 0.9936, domain acc: 0.5538, lr: 0.000341, lamb: 1.8520, time: 10.3670\n","epoch 1151: train D loss: 0.6800, train F loss: -1.2318, train acc: 0.9928, domain acc: 0.5794, lr: 0.000341, lamb: 1.8522, time: 10.2428\n","epoch 1152: train D loss: 0.6823, train F loss: -1.2363, train acc: 0.9928, domain acc: 0.5671, lr: 0.000340, lamb: 1.8524, time: 10.1595\n","epoch 1153: train D loss: 0.6849, train F loss: -1.2404, train acc: 0.9930, domain acc: 0.5604, lr: 0.000340, lamb: 1.8527, time: 10.6155\n","epoch 1154: train D loss: 0.6822, train F loss: -1.2332, train acc: 0.9916, domain acc: 0.5728, lr: 0.000340, lamb: 1.8529, time: 9.8820\n","epoch 1155: train D loss: 0.6823, train F loss: -1.2368, train acc: 0.9924, domain acc: 0.5660, lr: 0.000339, lamb: 1.8531, time: 9.9371\n","epoch 1156: train D loss: 0.6848, train F loss: -1.2389, train acc: 0.9920, domain acc: 0.5627, lr: 0.000339, lamb: 1.8534, time: 9.9634\n","epoch 1157: train D loss: 0.6827, train F loss: -1.2344, train acc: 0.9924, domain acc: 0.5662, lr: 0.000339, lamb: 1.8536, time: 9.4748\n","epoch 1158: train D loss: 0.6874, train F loss: -1.2349, train acc: 0.9906, domain acc: 0.5575, lr: 0.000338, lamb: 1.8538, time: 9.3847\n","epoch 1159: train D loss: 0.6817, train F loss: -1.2349, train acc: 0.9918, domain acc: 0.5714, lr: 0.000338, lamb: 1.8540, time: 9.4707\n","epoch 1160: train D loss: 0.6811, train F loss: -1.2317, train acc: 0.9926, domain acc: 0.5703, lr: 0.000338, lamb: 1.8543, time: 9.8080\n","epoch 1161: train D loss: 0.6829, train F loss: -1.2401, train acc: 0.9916, domain acc: 0.5681, lr: 0.000337, lamb: 1.8545, time: 9.5687\n","epoch 1162: train D loss: 0.6843, train F loss: -1.2424, train acc: 0.9930, domain acc: 0.5681, lr: 0.000337, lamb: 1.8547, time: 9.4700\n","epoch 1163: train D loss: 0.6819, train F loss: -1.2399, train acc: 0.9942, domain acc: 0.5783, lr: 0.000337, lamb: 1.8549, time: 9.3336\n","epoch 1164: train D loss: 0.6823, train F loss: -1.2395, train acc: 0.9940, domain acc: 0.5706, lr: 0.000336, lamb: 1.8552, time: 9.3137\n","epoch 1165: train D loss: 0.6835, train F loss: -1.2415, train acc: 0.9940, domain acc: 0.5683, lr: 0.000336, lamb: 1.8554, time: 9.3716\n","epoch 1166: train D loss: 0.6845, train F loss: -1.2389, train acc: 0.9926, domain acc: 0.5633, lr: 0.000336, lamb: 1.8556, time: 9.4647\n","epoch 1167: train D loss: 0.6860, train F loss: -1.2478, train acc: 0.9932, domain acc: 0.5578, lr: 0.000335, lamb: 1.8558, time: 9.5332\n","epoch 1168: train D loss: 0.6854, train F loss: -1.2376, train acc: 0.9898, domain acc: 0.5625, lr: 0.000335, lamb: 1.8561, time: 9.2447\n","epoch 1169: train D loss: 0.6861, train F loss: -1.2463, train acc: 0.9938, domain acc: 0.5603, lr: 0.000335, lamb: 1.8563, time: 9.3336\n","epoch 1170: train D loss: 0.6854, train F loss: -1.2455, train acc: 0.9932, domain acc: 0.5636, lr: 0.000334, lamb: 1.8565, time: 9.3052\n","epoch 1171: train D loss: 0.6861, train F loss: -1.2456, train acc: 0.9928, domain acc: 0.5587, lr: 0.000334, lamb: 1.8567, time: 9.4398\n","epoch 1172: train D loss: 0.6824, train F loss: -1.2454, train acc: 0.9956, domain acc: 0.5667, lr: 0.000334, lamb: 1.8570, time: 9.4402\n","epoch 1173: train D loss: 0.6824, train F loss: -1.2442, train acc: 0.9950, domain acc: 0.5669, lr: 0.000333, lamb: 1.8572, time: 9.5513\n","epoch 1174: train D loss: 0.6829, train F loss: -1.2419, train acc: 0.9936, domain acc: 0.5651, lr: 0.000333, lamb: 1.8574, time: 9.2336\n","epoch 1175: train D loss: 0.6834, train F loss: -1.2402, train acc: 0.9934, domain acc: 0.5649, lr: 0.000333, lamb: 1.8576, time: 9.4190\n","epoch 1176: train D loss: 0.6856, train F loss: -1.2476, train acc: 0.9942, domain acc: 0.5606, lr: 0.000332, lamb: 1.8579, time: 9.6444\n","epoch 1177: train D loss: 0.6813, train F loss: -1.2439, train acc: 0.9950, domain acc: 0.5747, lr: 0.000332, lamb: 1.8581, time: 9.5928\n","epoch 1178: train D loss: 0.6835, train F loss: -1.2455, train acc: 0.9940, domain acc: 0.5625, lr: 0.000332, lamb: 1.8583, time: 9.5965\n","epoch 1179: train D loss: 0.6834, train F loss: -1.2442, train acc: 0.9934, domain acc: 0.5627, lr: 0.000331, lamb: 1.8585, time: 9.6281\n","epoch 1180: train D loss: 0.6837, train F loss: -1.2362, train acc: 0.9914, domain acc: 0.5658, lr: 0.000331, lamb: 1.8588, time: 9.5996\n","epoch 1181: train D loss: 0.6847, train F loss: -1.2513, train acc: 0.9946, domain acc: 0.5625, lr: 0.000331, lamb: 1.8590, time: 9.6347\n","epoch 1182: train D loss: 0.6836, train F loss: -1.2472, train acc: 0.9940, domain acc: 0.5675, lr: 0.000330, lamb: 1.8592, time: 9.6705\n","epoch 1183: train D loss: 0.6840, train F loss: -1.2422, train acc: 0.9936, domain acc: 0.5684, lr: 0.000330, lamb: 1.8594, time: 9.9953\n","epoch 1184: train D loss: 0.6808, train F loss: -1.2391, train acc: 0.9938, domain acc: 0.5718, lr: 0.000330, lamb: 1.8596, time: 10.3621\n","epoch 1185: train D loss: 0.6832, train F loss: -1.2453, train acc: 0.9932, domain acc: 0.5626, lr: 0.000329, lamb: 1.8599, time: 10.3472\n","epoch 1186: train D loss: 0.6832, train F loss: -1.2386, train acc: 0.9934, domain acc: 0.5656, lr: 0.000329, lamb: 1.8601, time: 9.8514\n","epoch 1187: train D loss: 0.6810, train F loss: -1.2359, train acc: 0.9932, domain acc: 0.5682, lr: 0.000329, lamb: 1.8603, time: 9.7425\n","epoch 1188: train D loss: 0.6839, train F loss: -1.2437, train acc: 0.9924, domain acc: 0.5597, lr: 0.000328, lamb: 1.8605, time: 9.6430\n","epoch 1189: train D loss: 0.6842, train F loss: -1.2462, train acc: 0.9932, domain acc: 0.5691, lr: 0.000328, lamb: 1.8608, time: 9.8402\n","epoch 1190: train D loss: 0.6854, train F loss: -1.2477, train acc: 0.9922, domain acc: 0.5592, lr: 0.000328, lamb: 1.8610, time: 10.0410\n","epoch 1191: train D loss: 0.6815, train F loss: -1.2386, train acc: 0.9920, domain acc: 0.5673, lr: 0.000327, lamb: 1.8612, time: 9.9323\n","epoch 1192: train D loss: 0.6860, train F loss: -1.2500, train acc: 0.9932, domain acc: 0.5618, lr: 0.000327, lamb: 1.8614, time: 9.6305\n","epoch 1193: train D loss: 0.6808, train F loss: -1.2480, train acc: 0.9956, domain acc: 0.5785, lr: 0.000327, lamb: 1.8616, time: 9.6343\n","epoch 1194: train D loss: 0.6859, train F loss: -1.2580, train acc: 0.9956, domain acc: 0.5589, lr: 0.000326, lamb: 1.8619, time: 9.5809\n","epoch 1195: train D loss: 0.6814, train F loss: -1.2369, train acc: 0.9912, domain acc: 0.5735, lr: 0.000326, lamb: 1.8621, time: 9.7616\n","epoch 1196: train D loss: 0.6833, train F loss: -1.2482, train acc: 0.9932, domain acc: 0.5638, lr: 0.000326, lamb: 1.8623, time: 9.5469\n","epoch 1197: train D loss: 0.6834, train F loss: -1.2463, train acc: 0.9936, domain acc: 0.5686, lr: 0.000325, lamb: 1.8625, time: 9.8297\n","epoch 1198: train D loss: 0.6851, train F loss: -1.2473, train acc: 0.9944, domain acc: 0.5613, lr: 0.000325, lamb: 1.8627, time: 9.7707\n","epoch 1199: train D loss: 0.6825, train F loss: -1.2443, train acc: 0.9926, domain acc: 0.5735, lr: 0.000325, lamb: 1.8630, time: 9.7412\n","epoch 1200: train D loss: 0.6846, train F loss: -1.2470, train acc: 0.9926, domain acc: 0.5663, lr: 0.000324, lamb: 1.8632, time: 9.8039\n","epoch 1201: train D loss: 0.6825, train F loss: -1.2454, train acc: 0.9936, domain acc: 0.5684, lr: 0.000324, lamb: 1.8634, time: 10.1855\n","epoch 1202: train D loss: 0.6836, train F loss: -1.2473, train acc: 0.9930, domain acc: 0.5707, lr: 0.000324, lamb: 1.8636, time: 9.7235\n","epoch 1203: train D loss: 0.6833, train F loss: -1.2480, train acc: 0.9934, domain acc: 0.5710, lr: 0.000323, lamb: 1.8638, time: 9.6339\n","epoch 1204: train D loss: 0.6859, train F loss: -1.2490, train acc: 0.9926, domain acc: 0.5589, lr: 0.000323, lamb: 1.8640, time: 9.7343\n","epoch 1205: train D loss: 0.6826, train F loss: -1.2537, train acc: 0.9956, domain acc: 0.5639, lr: 0.000323, lamb: 1.8643, time: 9.6971\n","epoch 1206: train D loss: 0.6839, train F loss: -1.2507, train acc: 0.9942, domain acc: 0.5622, lr: 0.000323, lamb: 1.8645, time: 9.6772\n","epoch 1207: train D loss: 0.6861, train F loss: -1.2537, train acc: 0.9938, domain acc: 0.5549, lr: 0.000322, lamb: 1.8647, time: 9.6847\n","epoch 1208: train D loss: 0.6873, train F loss: -1.2578, train acc: 0.9942, domain acc: 0.5598, lr: 0.000322, lamb: 1.8649, time: 9.7247\n","epoch 1209: train D loss: 0.6847, train F loss: -1.2445, train acc: 0.9918, domain acc: 0.5611, lr: 0.000322, lamb: 1.8651, time: 9.6896\n","epoch 1210: train D loss: 0.6832, train F loss: -1.2425, train acc: 0.9938, domain acc: 0.5675, lr: 0.000321, lamb: 1.8654, time: 9.6521\n","epoch 1211: train D loss: 0.6850, train F loss: -1.2491, train acc: 0.9924, domain acc: 0.5619, lr: 0.000321, lamb: 1.8656, time: 10.0723\n","epoch 1212: train D loss: 0.6811, train F loss: -1.2435, train acc: 0.9934, domain acc: 0.5714, lr: 0.000321, lamb: 1.8658, time: 9.7162\n","epoch 1213: train D loss: 0.6834, train F loss: -1.2511, train acc: 0.9944, domain acc: 0.5698, lr: 0.000320, lamb: 1.8660, time: 9.6193\n","epoch 1214: train D loss: 0.6812, train F loss: -1.2436, train acc: 0.9932, domain acc: 0.5705, lr: 0.000320, lamb: 1.8662, time: 9.6497\n","epoch 1215: train D loss: 0.6848, train F loss: -1.2562, train acc: 0.9946, domain acc: 0.5613, lr: 0.000320, lamb: 1.8664, time: 9.8293\n","epoch 1216: train D loss: 0.6832, train F loss: -1.2489, train acc: 0.9938, domain acc: 0.5699, lr: 0.000319, lamb: 1.8667, time: 9.8585\n","epoch 1217: train D loss: 0.6835, train F loss: -1.2553, train acc: 0.9950, domain acc: 0.5648, lr: 0.000319, lamb: 1.8669, time: 9.7288\n","epoch 1218: train D loss: 0.6830, train F loss: -1.2449, train acc: 0.9934, domain acc: 0.5673, lr: 0.000319, lamb: 1.8671, time: 9.7247\n","epoch 1219: train D loss: 0.6848, train F loss: -1.2551, train acc: 0.9942, domain acc: 0.5688, lr: 0.000318, lamb: 1.8673, time: 9.6997\n","epoch 1220: train D loss: 0.6847, train F loss: -1.2464, train acc: 0.9920, domain acc: 0.5655, lr: 0.000318, lamb: 1.8675, time: 9.3801\n","epoch 1221: train D loss: 0.6851, train F loss: -1.2491, train acc: 0.9934, domain acc: 0.5612, lr: 0.000318, lamb: 1.8677, time: 9.3381\n","epoch 1222: train D loss: 0.6847, train F loss: -1.2554, train acc: 0.9940, domain acc: 0.5619, lr: 0.000317, lamb: 1.8679, time: 9.3242\n","epoch 1223: train D loss: 0.6832, train F loss: -1.2542, train acc: 0.9944, domain acc: 0.5654, lr: 0.000317, lamb: 1.8682, time: 9.3084\n","epoch 1224: train D loss: 0.6823, train F loss: -1.2471, train acc: 0.9930, domain acc: 0.5711, lr: 0.000317, lamb: 1.8684, time: 9.2993\n","epoch 1225: train D loss: 0.6830, train F loss: -1.2525, train acc: 0.9946, domain acc: 0.5699, lr: 0.000316, lamb: 1.8686, time: 9.2603\n","epoch 1226: train D loss: 0.6857, train F loss: -1.2520, train acc: 0.9930, domain acc: 0.5582, lr: 0.000316, lamb: 1.8688, time: 9.3264\n","epoch 1227: train D loss: 0.6862, train F loss: -1.2466, train acc: 0.9912, domain acc: 0.5610, lr: 0.000316, lamb: 1.8690, time: 9.3587\n","epoch 1228: train D loss: 0.6850, train F loss: -1.2529, train acc: 0.9924, domain acc: 0.5563, lr: 0.000316, lamb: 1.8692, time: 9.2496\n","epoch 1229: train D loss: 0.6841, train F loss: -1.2506, train acc: 0.9934, domain acc: 0.5662, lr: 0.000315, lamb: 1.8694, time: 9.2526\n","epoch 1230: train D loss: 0.6832, train F loss: -1.2527, train acc: 0.9940, domain acc: 0.5656, lr: 0.000315, lamb: 1.8697, time: 9.4117\n","epoch 1231: train D loss: 0.6818, train F loss: -1.2514, train acc: 0.9948, domain acc: 0.5661, lr: 0.000315, lamb: 1.8699, time: 9.3315\n","epoch 1232: train D loss: 0.6830, train F loss: -1.2500, train acc: 0.9930, domain acc: 0.5664, lr: 0.000314, lamb: 1.8701, time: 9.3441\n","epoch 1233: train D loss: 0.6830, train F loss: -1.2538, train acc: 0.9956, domain acc: 0.5653, lr: 0.000314, lamb: 1.8703, time: 9.3697\n","epoch 1234: train D loss: 0.6852, train F loss: -1.2578, train acc: 0.9938, domain acc: 0.5602, lr: 0.000314, lamb: 1.8705, time: 9.3059\n","epoch 1235: train D loss: 0.6829, train F loss: -1.2526, train acc: 0.9936, domain acc: 0.5677, lr: 0.000313, lamb: 1.8707, time: 9.4543\n","epoch 1236: train D loss: 0.6829, train F loss: -1.2515, train acc: 0.9932, domain acc: 0.5662, lr: 0.000313, lamb: 1.8709, time: 9.3580\n","epoch 1237: train D loss: 0.6845, train F loss: -1.2512, train acc: 0.9908, domain acc: 0.5636, lr: 0.000313, lamb: 1.8712, time: 9.3579\n","epoch 1238: train D loss: 0.6808, train F loss: -1.2524, train acc: 0.9946, domain acc: 0.5718, lr: 0.000312, lamb: 1.8714, time: 9.4366\n","epoch 1239: train D loss: 0.6850, train F loss: -1.2538, train acc: 0.9918, domain acc: 0.5629, lr: 0.000312, lamb: 1.8716, time: 9.2930\n","epoch 1240: train D loss: 0.6809, train F loss: -1.2469, train acc: 0.9934, domain acc: 0.5757, lr: 0.000312, lamb: 1.8718, time: 9.3962\n","epoch 1241: train D loss: 0.6850, train F loss: -1.2558, train acc: 0.9936, domain acc: 0.5593, lr: 0.000311, lamb: 1.8720, time: 9.4209\n","epoch 1242: train D loss: 0.6846, train F loss: -1.2589, train acc: 0.9946, domain acc: 0.5616, lr: 0.000311, lamb: 1.8722, time: 9.2021\n","epoch 1243: train D loss: 0.6846, train F loss: -1.2582, train acc: 0.9946, domain acc: 0.5662, lr: 0.000311, lamb: 1.8724, time: 9.3486\n","epoch 1244: train D loss: 0.6828, train F loss: -1.2496, train acc: 0.9936, domain acc: 0.5686, lr: 0.000310, lamb: 1.8726, time: 9.4501\n","epoch 1245: train D loss: 0.6817, train F loss: -1.2527, train acc: 0.9944, domain acc: 0.5672, lr: 0.000310, lamb: 1.8728, time: 9.2977\n","epoch 1246: train D loss: 0.6845, train F loss: -1.2609, train acc: 0.9952, domain acc: 0.5674, lr: 0.000310, lamb: 1.8731, time: 9.2335\n","epoch 1247: train D loss: 0.6826, train F loss: -1.2565, train acc: 0.9950, domain acc: 0.5687, lr: 0.000310, lamb: 1.8733, time: 9.4735\n","epoch 1248: train D loss: 0.6850, train F loss: -1.2576, train acc: 0.9934, domain acc: 0.5639, lr: 0.000309, lamb: 1.8735, time: 9.5688\n","epoch 1249: train D loss: 0.6861, train F loss: -1.2519, train acc: 0.9932, domain acc: 0.5631, lr: 0.000309, lamb: 1.8737, time: 9.6029\n","epoch 1250: train D loss: 0.6840, train F loss: -1.2553, train acc: 0.9936, domain acc: 0.5585, lr: 0.000309, lamb: 1.8739, time: 9.6601\n","epoch 1251: train D loss: 0.6863, train F loss: -1.2623, train acc: 0.9940, domain acc: 0.5597, lr: 0.000308, lamb: 1.8741, time: 9.5232\n","epoch 1252: train D loss: 0.6869, train F loss: -1.2637, train acc: 0.9932, domain acc: 0.5580, lr: 0.000308, lamb: 1.8743, time: 9.4388\n","epoch 1253: train D loss: 0.6853, train F loss: -1.2605, train acc: 0.9940, domain acc: 0.5609, lr: 0.000308, lamb: 1.8745, time: 9.4282\n","epoch 1254: train D loss: 0.6860, train F loss: -1.2624, train acc: 0.9946, domain acc: 0.5576, lr: 0.000307, lamb: 1.8747, time: 9.4490\n","epoch 1255: train D loss: 0.6869, train F loss: -1.2627, train acc: 0.9944, domain acc: 0.5583, lr: 0.000307, lamb: 1.8749, time: 9.3129\n","epoch 1256: train D loss: 0.6844, train F loss: -1.2619, train acc: 0.9952, domain acc: 0.5624, lr: 0.000307, lamb: 1.8752, time: 9.3708\n","epoch 1257: train D loss: 0.6834, train F loss: -1.2514, train acc: 0.9920, domain acc: 0.5644, lr: 0.000306, lamb: 1.8754, time: 9.5928\n","epoch 1258: train D loss: 0.6828, train F loss: -1.2599, train acc: 0.9944, domain acc: 0.5610, lr: 0.000306, lamb: 1.8756, time: 9.5217\n","epoch 1259: train D loss: 0.6845, train F loss: -1.2569, train acc: 0.9938, domain acc: 0.5606, lr: 0.000306, lamb: 1.8758, time: 9.3658\n","epoch 1260: train D loss: 0.6846, train F loss: -1.2577, train acc: 0.9936, domain acc: 0.5628, lr: 0.000306, lamb: 1.8760, time: 9.3668\n","epoch 1261: train D loss: 0.6848, train F loss: -1.2623, train acc: 0.9936, domain acc: 0.5604, lr: 0.000305, lamb: 1.8762, time: 9.3457\n","epoch 1262: train D loss: 0.6850, train F loss: -1.2625, train acc: 0.9954, domain acc: 0.5580, lr: 0.000305, lamb: 1.8764, time: 9.3205\n","epoch 1263: train D loss: 0.6816, train F loss: -1.2554, train acc: 0.9950, domain acc: 0.5687, lr: 0.000305, lamb: 1.8766, time: 9.2101\n","epoch 1264: train D loss: 0.6873, train F loss: -1.2596, train acc: 0.9934, domain acc: 0.5524, lr: 0.000304, lamb: 1.8768, time: 9.3071\n","epoch 1265: train D loss: 0.6838, train F loss: -1.2539, train acc: 0.9922, domain acc: 0.5664, lr: 0.000304, lamb: 1.8770, time: 9.3191\n","epoch 1266: train D loss: 0.6844, train F loss: -1.2593, train acc: 0.9936, domain acc: 0.5665, lr: 0.000304, lamb: 1.8772, time: 9.3216\n","epoch 1267: train D loss: 0.6849, train F loss: -1.2600, train acc: 0.9938, domain acc: 0.5546, lr: 0.000303, lamb: 1.8775, time: 9.2790\n","epoch 1268: train D loss: 0.6821, train F loss: -1.2534, train acc: 0.9936, domain acc: 0.5676, lr: 0.000303, lamb: 1.8777, time: 9.6099\n","epoch 1269: train D loss: 0.6825, train F loss: -1.2566, train acc: 0.9942, domain acc: 0.5713, lr: 0.000303, lamb: 1.8779, time: 9.5927\n","epoch 1270: train D loss: 0.6820, train F loss: -1.2560, train acc: 0.9940, domain acc: 0.5678, lr: 0.000303, lamb: 1.8781, time: 9.6220\n","epoch 1271: train D loss: 0.6837, train F loss: -1.2607, train acc: 0.9946, domain acc: 0.5686, lr: 0.000302, lamb: 1.8783, time: 9.5956\n","epoch 1272: train D loss: 0.6831, train F loss: -1.2577, train acc: 0.9940, domain acc: 0.5649, lr: 0.000302, lamb: 1.8785, time: 9.6593\n","epoch 1273: train D loss: 0.6790, train F loss: -1.2507, train acc: 0.9942, domain acc: 0.5815, lr: 0.000302, lamb: 1.8787, time: 9.6243\n","epoch 1274: train D loss: 0.6858, train F loss: -1.2547, train acc: 0.9924, domain acc: 0.5633, lr: 0.000301, lamb: 1.8789, time: 9.6159\n","epoch 1275: train D loss: 0.6826, train F loss: -1.2567, train acc: 0.9944, domain acc: 0.5719, lr: 0.000301, lamb: 1.8791, time: 9.6396\n","epoch 1276: train D loss: 0.6823, train F loss: -1.2582, train acc: 0.9940, domain acc: 0.5707, lr: 0.000301, lamb: 1.8793, time: 9.6903\n","epoch 1277: train D loss: 0.6832, train F loss: -1.2552, train acc: 0.9928, domain acc: 0.5683, lr: 0.000300, lamb: 1.8795, time: 9.7826\n","epoch 1278: train D loss: 0.6869, train F loss: -1.2675, train acc: 0.9940, domain acc: 0.5602, lr: 0.000300, lamb: 1.8797, time: 9.4282\n","epoch 1279: train D loss: 0.6841, train F loss: -1.2581, train acc: 0.9938, domain acc: 0.5630, lr: 0.000300, lamb: 1.8799, time: 9.6654\n","epoch 1280: train D loss: 0.6843, train F loss: -1.2602, train acc: 0.9940, domain acc: 0.5650, lr: 0.000300, lamb: 1.8801, time: 9.4592\n","epoch 1281: train D loss: 0.6831, train F loss: -1.2602, train acc: 0.9952, domain acc: 0.5713, lr: 0.000299, lamb: 1.8803, time: 9.4839\n","epoch 1282: train D loss: 0.6829, train F loss: -1.2575, train acc: 0.9932, domain acc: 0.5729, lr: 0.000299, lamb: 1.8805, time: 9.4866\n","epoch 1283: train D loss: 0.6816, train F loss: -1.2561, train acc: 0.9938, domain acc: 0.5679, lr: 0.000299, lamb: 1.8807, time: 9.3599\n","epoch 1284: train D loss: 0.6844, train F loss: -1.2636, train acc: 0.9934, domain acc: 0.5656, lr: 0.000298, lamb: 1.8810, time: 9.3203\n","epoch 1285: train D loss: 0.6826, train F loss: -1.2460, train acc: 0.9910, domain acc: 0.5654, lr: 0.000298, lamb: 1.8812, time: 9.4115\n","epoch 1286: train D loss: 0.6822, train F loss: -1.2608, train acc: 0.9948, domain acc: 0.5622, lr: 0.000298, lamb: 1.8814, time: 9.3107\n","epoch 1287: train D loss: 0.6853, train F loss: -1.2655, train acc: 0.9946, domain acc: 0.5571, lr: 0.000297, lamb: 1.8816, time: 9.3510\n","epoch 1288: train D loss: 0.6834, train F loss: -1.2603, train acc: 0.9936, domain acc: 0.5608, lr: 0.000297, lamb: 1.8818, time: 9.3793\n","epoch 1289: train D loss: 0.6835, train F loss: -1.2668, train acc: 0.9956, domain acc: 0.5696, lr: 0.000297, lamb: 1.8820, time: 9.3283\n","epoch 1290: train D loss: 0.6838, train F loss: -1.2619, train acc: 0.9946, domain acc: 0.5646, lr: 0.000297, lamb: 1.8822, time: 9.3315\n","epoch 1291: train D loss: 0.6854, train F loss: -1.2648, train acc: 0.9940, domain acc: 0.5625, lr: 0.000296, lamb: 1.8824, time: 9.3617\n","epoch 1292: train D loss: 0.6847, train F loss: -1.2633, train acc: 0.9938, domain acc: 0.5606, lr: 0.000296, lamb: 1.8826, time: 9.3195\n","epoch 1293: train D loss: 0.6814, train F loss: -1.2550, train acc: 0.9940, domain acc: 0.5721, lr: 0.000296, lamb: 1.8828, time: 9.3535\n","epoch 1294: train D loss: 0.6840, train F loss: -1.2639, train acc: 0.9936, domain acc: 0.5675, lr: 0.000295, lamb: 1.8830, time: 9.3577\n","epoch 1295: train D loss: 0.6837, train F loss: -1.2597, train acc: 0.9924, domain acc: 0.5690, lr: 0.000295, lamb: 1.8832, time: 9.4629\n","epoch 1296: train D loss: 0.6872, train F loss: -1.2551, train acc: 0.9916, domain acc: 0.5495, lr: 0.000295, lamb: 1.8834, time: 9.5113\n","epoch 1297: train D loss: 0.6841, train F loss: -1.2649, train acc: 0.9940, domain acc: 0.5655, lr: 0.000294, lamb: 1.8836, time: 9.4694\n","epoch 1298: train D loss: 0.6827, train F loss: -1.2652, train acc: 0.9946, domain acc: 0.5637, lr: 0.000294, lamb: 1.8838, time: 9.5788\n","epoch 1299: train D loss: 0.6842, train F loss: -1.2621, train acc: 0.9926, domain acc: 0.5644, lr: 0.000294, lamb: 1.8840, time: 9.6767\n","epoch 1300: train D loss: 0.6834, train F loss: -1.2668, train acc: 0.9950, domain acc: 0.5594, lr: 0.000294, lamb: 1.8842, time: 9.5990\n","epoch 1301: train D loss: 0.6828, train F loss: -1.2619, train acc: 0.9938, domain acc: 0.5655, lr: 0.000293, lamb: 1.8844, time: 9.5547\n","epoch 1302: train D loss: 0.6813, train F loss: -1.2556, train acc: 0.9944, domain acc: 0.5723, lr: 0.000293, lamb: 1.8846, time: 9.5557\n","epoch 1303: train D loss: 0.6838, train F loss: -1.2683, train acc: 0.9958, domain acc: 0.5659, lr: 0.000293, lamb: 1.8848, time: 10.6106\n","epoch 1304: train D loss: 0.6829, train F loss: -1.2634, train acc: 0.9948, domain acc: 0.5673, lr: 0.000292, lamb: 1.8850, time: 10.9368\n","epoch 1305: train D loss: 0.6875, train F loss: -1.2653, train acc: 0.9926, domain acc: 0.5525, lr: 0.000292, lamb: 1.8852, time: 11.4848\n","epoch 1306: train D loss: 0.6860, train F loss: -1.2728, train acc: 0.9944, domain acc: 0.5642, lr: 0.000292, lamb: 1.8854, time: 11.6947\n","epoch 1307: train D loss: 0.6853, train F loss: -1.2715, train acc: 0.9944, domain acc: 0.5585, lr: 0.000292, lamb: 1.8856, time: 10.8095\n","epoch 1308: train D loss: 0.6862, train F loss: -1.2709, train acc: 0.9954, domain acc: 0.5556, lr: 0.000291, lamb: 1.8858, time: 11.1461\n","epoch 1309: train D loss: 0.6846, train F loss: -1.2710, train acc: 0.9956, domain acc: 0.5637, lr: 0.000291, lamb: 1.8860, time: 10.3336\n","epoch 1310: train D loss: 0.6844, train F loss: -1.2704, train acc: 0.9940, domain acc: 0.5676, lr: 0.000291, lamb: 1.8862, time: 10.6941\n","epoch 1311: train D loss: 0.6851, train F loss: -1.2599, train acc: 0.9934, domain acc: 0.5618, lr: 0.000290, lamb: 1.8864, time: 9.7939\n","epoch 1312: train D loss: 0.6810, train F loss: -1.2631, train acc: 0.9950, domain acc: 0.5696, lr: 0.000290, lamb: 1.8866, time: 9.4383\n","epoch 1313: train D loss: 0.6833, train F loss: -1.2657, train acc: 0.9932, domain acc: 0.5667, lr: 0.000290, lamb: 1.8868, time: 9.4646\n","epoch 1314: train D loss: 0.6850, train F loss: -1.2691, train acc: 0.9950, domain acc: 0.5613, lr: 0.000289, lamb: 1.8870, time: 9.4617\n","epoch 1315: train D loss: 0.6818, train F loss: -1.2573, train acc: 0.9922, domain acc: 0.5696, lr: 0.000289, lamb: 1.8872, time: 9.4471\n","epoch 1316: train D loss: 0.6855, train F loss: -1.2654, train acc: 0.9932, domain acc: 0.5569, lr: 0.000289, lamb: 1.8874, time: 9.3718\n","epoch 1317: train D loss: 0.6840, train F loss: -1.2536, train acc: 0.9912, domain acc: 0.5678, lr: 0.000289, lamb: 1.8876, time: 9.2352\n","epoch 1318: train D loss: 0.6813, train F loss: -1.2639, train acc: 0.9946, domain acc: 0.5697, lr: 0.000288, lamb: 1.8878, time: 9.3018\n","epoch 1319: train D loss: 0.6847, train F loss: -1.2668, train acc: 0.9930, domain acc: 0.5607, lr: 0.000288, lamb: 1.8880, time: 9.3661\n","epoch 1320: train D loss: 0.6844, train F loss: -1.2690, train acc: 0.9944, domain acc: 0.5568, lr: 0.000288, lamb: 1.8882, time: 9.4595\n","epoch 1321: train D loss: 0.6843, train F loss: -1.2679, train acc: 0.9934, domain acc: 0.5584, lr: 0.000287, lamb: 1.8884, time: 9.3475\n","epoch 1322: train D loss: 0.6849, train F loss: -1.2726, train acc: 0.9940, domain acc: 0.5600, lr: 0.000287, lamb: 1.8886, time: 9.2576\n","epoch 1323: train D loss: 0.6824, train F loss: -1.2604, train acc: 0.9924, domain acc: 0.5690, lr: 0.000287, lamb: 1.8888, time: 9.2553\n","epoch 1324: train D loss: 0.6838, train F loss: -1.2669, train acc: 0.9928, domain acc: 0.5663, lr: 0.000287, lamb: 1.8890, time: 9.2379\n","epoch 1325: train D loss: 0.6853, train F loss: -1.2677, train acc: 0.9920, domain acc: 0.5670, lr: 0.000286, lamb: 1.8892, time: 9.2498\n","epoch 1326: train D loss: 0.6841, train F loss: -1.2692, train acc: 0.9946, domain acc: 0.5606, lr: 0.000286, lamb: 1.8894, time: 9.3805\n","epoch 1327: train D loss: 0.6847, train F loss: -1.2698, train acc: 0.9938, domain acc: 0.5642, lr: 0.000286, lamb: 1.8896, time: 9.4619\n","epoch 1328: train D loss: 0.6844, train F loss: -1.2730, train acc: 0.9958, domain acc: 0.5617, lr: 0.000285, lamb: 1.8898, time: 9.3698\n","epoch 1329: train D loss: 0.6832, train F loss: -1.2646, train acc: 0.9942, domain acc: 0.5691, lr: 0.000285, lamb: 1.8900, time: 9.6605\n","epoch 1330: train D loss: 0.6863, train F loss: -1.2694, train acc: 0.9930, domain acc: 0.5602, lr: 0.000285, lamb: 1.8902, time: 9.7037\n","epoch 1331: train D loss: 0.6879, train F loss: -1.2741, train acc: 0.9928, domain acc: 0.5539, lr: 0.000285, lamb: 1.8904, time: 9.6724\n","epoch 1332: train D loss: 0.6846, train F loss: -1.2695, train acc: 0.9930, domain acc: 0.5625, lr: 0.000284, lamb: 1.8906, time: 9.4814\n","epoch 1333: train D loss: 0.6859, train F loss: -1.2714, train acc: 0.9942, domain acc: 0.5607, lr: 0.000284, lamb: 1.8908, time: 9.3512\n","epoch 1334: train D loss: 0.6854, train F loss: -1.2730, train acc: 0.9942, domain acc: 0.5571, lr: 0.000284, lamb: 1.8910, time: 9.3224\n","epoch 1335: train D loss: 0.6851, train F loss: -1.2763, train acc: 0.9966, domain acc: 0.5589, lr: 0.000283, lamb: 1.8912, time: 9.3449\n","epoch 1336: train D loss: 0.6854, train F loss: -1.2731, train acc: 0.9938, domain acc: 0.5619, lr: 0.000283, lamb: 1.8914, time: 9.2677\n","epoch 1337: train D loss: 0.6851, train F loss: -1.2685, train acc: 0.9934, domain acc: 0.5603, lr: 0.000283, lamb: 1.8916, time: 9.3779\n","epoch 1338: train D loss: 0.6850, train F loss: -1.2744, train acc: 0.9956, domain acc: 0.5577, lr: 0.000283, lamb: 1.8918, time: 9.3097\n","epoch 1339: train D loss: 0.6848, train F loss: -1.2716, train acc: 0.9938, domain acc: 0.5577, lr: 0.000282, lamb: 1.8920, time: 9.3538\n","epoch 1340: train D loss: 0.6842, train F loss: -1.2715, train acc: 0.9948, domain acc: 0.5644, lr: 0.000282, lamb: 1.8922, time: 9.4259\n","epoch 1341: train D loss: 0.6861, train F loss: -1.2744, train acc: 0.9928, domain acc: 0.5615, lr: 0.000282, lamb: 1.8924, time: 9.3554\n","epoch 1342: train D loss: 0.6831, train F loss: -1.2689, train acc: 0.9946, domain acc: 0.5633, lr: 0.000281, lamb: 1.8926, time: 9.4083\n","epoch 1343: train D loss: 0.6852, train F loss: -1.2727, train acc: 0.9940, domain acc: 0.5623, lr: 0.000281, lamb: 1.8928, time: 9.4690\n","epoch 1344: train D loss: 0.6850, train F loss: -1.2720, train acc: 0.9942, domain acc: 0.5603, lr: 0.000281, lamb: 1.8930, time: 9.6778\n","epoch 1345: train D loss: 0.6837, train F loss: -1.2742, train acc: 0.9952, domain acc: 0.5570, lr: 0.000281, lamb: 1.8931, time: 9.6458\n","epoch 1346: train D loss: 0.6847, train F loss: -1.2735, train acc: 0.9938, domain acc: 0.5628, lr: 0.000280, lamb: 1.8933, time: 9.9830\n","epoch 1347: train D loss: 0.6868, train F loss: -1.2758, train acc: 0.9936, domain acc: 0.5538, lr: 0.000280, lamb: 1.8935, time: 9.5571\n","epoch 1348: train D loss: 0.6861, train F loss: -1.2763, train acc: 0.9946, domain acc: 0.5596, lr: 0.000280, lamb: 1.8937, time: 9.5777\n","epoch 1349: train D loss: 0.6873, train F loss: -1.2761, train acc: 0.9940, domain acc: 0.5535, lr: 0.000280, lamb: 1.8939, time: 9.4802\n","epoch 1350: train D loss: 0.6871, train F loss: -1.2700, train acc: 0.9928, domain acc: 0.5566, lr: 0.000279, lamb: 1.8941, time: 9.5176\n","epoch 1351: train D loss: 0.6854, train F loss: -1.2683, train acc: 0.9926, domain acc: 0.5595, lr: 0.000279, lamb: 1.8943, time: 9.4500\n","epoch 1352: train D loss: 0.6849, train F loss: -1.2726, train acc: 0.9934, domain acc: 0.5596, lr: 0.000279, lamb: 1.8945, time: 9.5041\n","epoch 1353: train D loss: 0.6826, train F loss: -1.2709, train acc: 0.9952, domain acc: 0.5700, lr: 0.000278, lamb: 1.8947, time: 9.3840\n","epoch 1354: train D loss: 0.6875, train F loss: -1.2759, train acc: 0.9932, domain acc: 0.5565, lr: 0.000278, lamb: 1.8949, time: 9.3940\n","epoch 1355: train D loss: 0.6830, train F loss: -1.2713, train acc: 0.9938, domain acc: 0.5618, lr: 0.000278, lamb: 1.8951, time: 9.3869\n","epoch 1356: train D loss: 0.6841, train F loss: -1.2710, train acc: 0.9938, domain acc: 0.5628, lr: 0.000278, lamb: 1.8953, time: 9.4119\n","epoch 1357: train D loss: 0.6857, train F loss: -1.2773, train acc: 0.9940, domain acc: 0.5560, lr: 0.000277, lamb: 1.8955, time: 9.4998\n","epoch 1358: train D loss: 0.6839, train F loss: -1.2746, train acc: 0.9950, domain acc: 0.5642, lr: 0.000277, lamb: 1.8957, time: 9.5214\n","epoch 1359: train D loss: 0.6833, train F loss: -1.2664, train acc: 0.9932, domain acc: 0.5666, lr: 0.000277, lamb: 1.8959, time: 9.5357\n","epoch 1360: train D loss: 0.6860, train F loss: -1.2792, train acc: 0.9952, domain acc: 0.5549, lr: 0.000276, lamb: 1.8961, time: 9.3950\n","epoch 1361: train D loss: 0.6862, train F loss: -1.2819, train acc: 0.9954, domain acc: 0.5546, lr: 0.000276, lamb: 1.8963, time: 9.4382\n","epoch 1362: train D loss: 0.6862, train F loss: -1.2738, train acc: 0.9918, domain acc: 0.5583, lr: 0.000276, lamb: 1.8965, time: 9.4577\n","epoch 1363: train D loss: 0.6830, train F loss: -1.2726, train acc: 0.9936, domain acc: 0.5637, lr: 0.000276, lamb: 1.8966, time: 9.3224\n","epoch 1364: train D loss: 0.6841, train F loss: -1.2740, train acc: 0.9940, domain acc: 0.5619, lr: 0.000275, lamb: 1.8968, time: 9.4071\n","epoch 1365: train D loss: 0.6822, train F loss: -1.2716, train acc: 0.9944, domain acc: 0.5668, lr: 0.000275, lamb: 1.8970, time: 9.4282\n","epoch 1366: train D loss: 0.6850, train F loss: -1.2746, train acc: 0.9930, domain acc: 0.5646, lr: 0.000275, lamb: 1.8972, time: 9.3143\n","epoch 1367: train D loss: 0.6801, train F loss: -1.2644, train acc: 0.9932, domain acc: 0.5721, lr: 0.000275, lamb: 1.8974, time: 9.2898\n","epoch 1368: train D loss: 0.6841, train F loss: -1.2748, train acc: 0.9946, domain acc: 0.5614, lr: 0.000274, lamb: 1.8976, time: 9.2200\n","epoch 1369: train D loss: 0.6805, train F loss: -1.2676, train acc: 0.9936, domain acc: 0.5703, lr: 0.000274, lamb: 1.8978, time: 9.3083\n","epoch 1370: train D loss: 0.6824, train F loss: -1.2650, train acc: 0.9930, domain acc: 0.5645, lr: 0.000274, lamb: 1.8980, time: 9.2328\n","epoch 1371: train D loss: 0.6843, train F loss: -1.2786, train acc: 0.9952, domain acc: 0.5645, lr: 0.000273, lamb: 1.8982, time: 9.4213\n","epoch 1372: train D loss: 0.6847, train F loss: -1.2755, train acc: 0.9946, domain acc: 0.5604, lr: 0.000273, lamb: 1.8984, time: 9.3005\n","epoch 1373: train D loss: 0.6842, train F loss: -1.2734, train acc: 0.9932, domain acc: 0.5656, lr: 0.000273, lamb: 1.8986, time: 9.5057\n","epoch 1374: train D loss: 0.6827, train F loss: -1.2733, train acc: 0.9946, domain acc: 0.5611, lr: 0.000273, lamb: 1.8988, time: 9.7660\n","epoch 1375: train D loss: 0.6818, train F loss: -1.2732, train acc: 0.9946, domain acc: 0.5676, lr: 0.000272, lamb: 1.8989, time: 9.6002\n","epoch 1376: train D loss: 0.6835, train F loss: -1.2793, train acc: 0.9956, domain acc: 0.5704, lr: 0.000272, lamb: 1.8991, time: 9.6157\n","epoch 1377: train D loss: 0.6849, train F loss: -1.2778, train acc: 0.9944, domain acc: 0.5581, lr: 0.000272, lamb: 1.8993, time: 9.8612\n","epoch 1378: train D loss: 0.6862, train F loss: -1.2774, train acc: 0.9924, domain acc: 0.5556, lr: 0.000272, lamb: 1.8995, time: 9.6294\n","epoch 1379: train D loss: 0.6827, train F loss: -1.2758, train acc: 0.9944, domain acc: 0.5658, lr: 0.000271, lamb: 1.8997, time: 9.6755\n","epoch 1380: train D loss: 0.6843, train F loss: -1.2805, train acc: 0.9952, domain acc: 0.5590, lr: 0.000271, lamb: 1.8999, time: 9.3330\n","epoch 1381: train D loss: 0.6867, train F loss: -1.2801, train acc: 0.9942, domain acc: 0.5491, lr: 0.000271, lamb: 1.9001, time: 9.2696\n","epoch 1382: train D loss: 0.6853, train F loss: -1.2786, train acc: 0.9936, domain acc: 0.5618, lr: 0.000270, lamb: 1.9003, time: 9.3188\n","epoch 1383: train D loss: 0.6821, train F loss: -1.2727, train acc: 0.9938, domain acc: 0.5699, lr: 0.000270, lamb: 1.9005, time: 9.2581\n","epoch 1384: train D loss: 0.6855, train F loss: -1.2746, train acc: 0.9938, domain acc: 0.5611, lr: 0.000270, lamb: 1.9007, time: 9.2781\n","epoch 1385: train D loss: 0.6846, train F loss: -1.2674, train acc: 0.9922, domain acc: 0.5587, lr: 0.000270, lamb: 1.9009, time: 9.5504\n","epoch 1386: train D loss: 0.6817, train F loss: -1.2590, train acc: 0.9936, domain acc: 0.5739, lr: 0.000269, lamb: 1.9010, time: 9.4733\n","epoch 1387: train D loss: 0.6819, train F loss: -1.2705, train acc: 0.9934, domain acc: 0.5673, lr: 0.000269, lamb: 1.9012, time: 9.4079\n","epoch 1388: train D loss: 0.6867, train F loss: -1.2825, train acc: 0.9942, domain acc: 0.5527, lr: 0.000269, lamb: 1.9014, time: 9.4002\n","epoch 1389: train D loss: 0.6851, train F loss: -1.2826, train acc: 0.9960, domain acc: 0.5554, lr: 0.000269, lamb: 1.9016, time: 9.5266\n","epoch 1390: train D loss: 0.6852, train F loss: -1.2836, train acc: 0.9950, domain acc: 0.5606, lr: 0.000268, lamb: 1.9018, time: 9.6641\n","epoch 1391: train D loss: 0.6863, train F loss: -1.2845, train acc: 0.9950, domain acc: 0.5625, lr: 0.000268, lamb: 1.9020, time: 9.4161\n","epoch 1392: train D loss: 0.6828, train F loss: -1.2774, train acc: 0.9954, domain acc: 0.5703, lr: 0.000268, lamb: 1.9022, time: 9.3950\n","epoch 1393: train D loss: 0.6836, train F loss: -1.2804, train acc: 0.9946, domain acc: 0.5638, lr: 0.000267, lamb: 1.9024, time: 9.4648\n","epoch 1394: train D loss: 0.6860, train F loss: -1.2841, train acc: 0.9948, domain acc: 0.5615, lr: 0.000267, lamb: 1.9026, time: 9.4523\n","epoch 1395: train D loss: 0.6832, train F loss: -1.2759, train acc: 0.9940, domain acc: 0.5682, lr: 0.000267, lamb: 1.9027, time: 9.4203\n","epoch 1396: train D loss: 0.6862, train F loss: -1.2840, train acc: 0.9944, domain acc: 0.5571, lr: 0.000267, lamb: 1.9029, time: 9.4562\n","epoch 1397: train D loss: 0.6845, train F loss: -1.2750, train acc: 0.9924, domain acc: 0.5577, lr: 0.000266, lamb: 1.9031, time: 9.3836\n","epoch 1398: train D loss: 0.6830, train F loss: -1.2739, train acc: 0.9924, domain acc: 0.5694, lr: 0.000266, lamb: 1.9033, time: 9.9567\n","epoch 1399: train D loss: 0.6856, train F loss: -1.2808, train acc: 0.9942, domain acc: 0.5531, lr: 0.000266, lamb: 1.9035, time: 9.5742\n","epoch 1400: train D loss: 0.6850, train F loss: -1.2802, train acc: 0.9944, domain acc: 0.5651, lr: 0.000266, lamb: 1.9037, time: 9.5981\n","epoch 1401: train D loss: 0.6848, train F loss: -1.2795, train acc: 0.9936, domain acc: 0.5597, lr: 0.000265, lamb: 1.9039, time: 9.5040\n","epoch 1402: train D loss: 0.6840, train F loss: -1.2799, train acc: 0.9940, domain acc: 0.5601, lr: 0.000265, lamb: 1.9041, time: 9.4643\n","epoch 1403: train D loss: 0.6869, train F loss: -1.2800, train acc: 0.9930, domain acc: 0.5576, lr: 0.000265, lamb: 1.9042, time: 9.5300\n","epoch 1404: train D loss: 0.6859, train F loss: -1.2859, train acc: 0.9946, domain acc: 0.5592, lr: 0.000265, lamb: 1.9044, time: 9.5601\n","epoch 1405: train D loss: 0.6840, train F loss: -1.2737, train acc: 0.9956, domain acc: 0.5632, lr: 0.000264, lamb: 1.9046, time: 9.6608\n","epoch 1406: train D loss: 0.6865, train F loss: -1.2654, train acc: 0.9932, domain acc: 0.5559, lr: 0.000264, lamb: 1.9048, time: 9.4371\n","epoch 1407: train D loss: 0.6842, train F loss: -1.2799, train acc: 0.9944, domain acc: 0.5605, lr: 0.000264, lamb: 1.9050, time: 9.4128\n","epoch 1408: train D loss: 0.6852, train F loss: -1.2762, train acc: 0.9916, domain acc: 0.5577, lr: 0.000264, lamb: 1.9052, time: 9.6795\n","epoch 1409: train D loss: 0.6863, train F loss: -1.2839, train acc: 0.9938, domain acc: 0.5609, lr: 0.000263, lamb: 1.9054, time: 9.5988\n","epoch 1410: train D loss: 0.6828, train F loss: -1.2747, train acc: 0.9934, domain acc: 0.5665, lr: 0.000263, lamb: 1.9056, time: 9.4422\n","epoch 1411: train D loss: 0.6816, train F loss: -1.2739, train acc: 0.9934, domain acc: 0.5691, lr: 0.000263, lamb: 1.9057, time: 9.6682\n","epoch 1412: train D loss: 0.6832, train F loss: -1.2803, train acc: 0.9954, domain acc: 0.5618, lr: 0.000262, lamb: 1.9059, time: 9.8066\n","epoch 1413: train D loss: 0.6832, train F loss: -1.2848, train acc: 0.9960, domain acc: 0.5651, lr: 0.000262, lamb: 1.9061, time: 9.4739\n","epoch 1414: train D loss: 0.6848, train F loss: -1.2817, train acc: 0.9938, domain acc: 0.5545, lr: 0.000262, lamb: 1.9063, time: 9.6213\n","epoch 1415: train D loss: 0.6842, train F loss: -1.2830, train acc: 0.9952, domain acc: 0.5578, lr: 0.000262, lamb: 1.9065, time: 9.7685\n","epoch 1416: train D loss: 0.6838, train F loss: -1.2819, train acc: 0.9952, domain acc: 0.5648, lr: 0.000261, lamb: 1.9067, time: 9.7350\n","epoch 1417: train D loss: 0.6841, train F loss: -1.2799, train acc: 0.9942, domain acc: 0.5574, lr: 0.000261, lamb: 1.9069, time: 9.4889\n","epoch 1418: train D loss: 0.6844, train F loss: -1.2804, train acc: 0.9932, domain acc: 0.5599, lr: 0.000261, lamb: 1.9070, time: 9.9034\n","epoch 1419: train D loss: 0.6864, train F loss: -1.2824, train acc: 0.9928, domain acc: 0.5567, lr: 0.000261, lamb: 1.9072, time: 11.3510\n","epoch 1420: train D loss: 0.6861, train F loss: -1.2880, train acc: 0.9948, domain acc: 0.5523, lr: 0.000260, lamb: 1.9074, time: 9.7833\n","epoch 1421: train D loss: 0.6853, train F loss: -1.2851, train acc: 0.9938, domain acc: 0.5558, lr: 0.000260, lamb: 1.9076, time: 9.7598\n","epoch 1422: train D loss: 0.6844, train F loss: -1.2834, train acc: 0.9938, domain acc: 0.5627, lr: 0.000260, lamb: 1.9078, time: 9.1407\n","epoch 1423: train D loss: 0.6866, train F loss: -1.2897, train acc: 0.9952, domain acc: 0.5541, lr: 0.000260, lamb: 1.9080, time: 9.7145\n","epoch 1424: train D loss: 0.6833, train F loss: -1.2825, train acc: 0.9942, domain acc: 0.5613, lr: 0.000259, lamb: 1.9081, time: 9.3422\n","epoch 1425: train D loss: 0.6860, train F loss: -1.2849, train acc: 0.9942, domain acc: 0.5581, lr: 0.000259, lamb: 1.9083, time: 10.9830\n","epoch 1426: train D loss: 0.6835, train F loss: -1.2834, train acc: 0.9952, domain acc: 0.5620, lr: 0.000259, lamb: 1.9085, time: 10.4174\n","epoch 1427: train D loss: 0.6848, train F loss: -1.2858, train acc: 0.9962, domain acc: 0.5647, lr: 0.000259, lamb: 1.9087, time: 9.4304\n","epoch 1428: train D loss: 0.6859, train F loss: -1.2918, train acc: 0.9950, domain acc: 0.5606, lr: 0.000258, lamb: 1.9089, time: 10.8454\n","epoch 1429: train D loss: 0.6856, train F loss: -1.2828, train acc: 0.9936, domain acc: 0.5538, lr: 0.000258, lamb: 1.9091, time: 10.7250\n","epoch 1430: train D loss: 0.6852, train F loss: -1.2902, train acc: 0.9950, domain acc: 0.5594, lr: 0.000258, lamb: 1.9093, time: 10.6494\n","epoch 1431: train D loss: 0.6836, train F loss: -1.2826, train acc: 0.9948, domain acc: 0.5619, lr: 0.000258, lamb: 1.9094, time: 10.9547\n","epoch 1432: train D loss: 0.6863, train F loss: -1.2894, train acc: 0.9948, domain acc: 0.5611, lr: 0.000257, lamb: 1.9096, time: 10.3336\n","epoch 1433: train D loss: 0.6845, train F loss: -1.2860, train acc: 0.9940, domain acc: 0.5639, lr: 0.000257, lamb: 1.9098, time: 10.0771\n","epoch 1434: train D loss: 0.6838, train F loss: -1.2843, train acc: 0.9944, domain acc: 0.5631, lr: 0.000257, lamb: 1.9100, time: 10.2677\n","epoch 1435: train D loss: 0.6876, train F loss: -1.2895, train acc: 0.9938, domain acc: 0.5529, lr: 0.000256, lamb: 1.9102, time: 10.0724\n","epoch 1436: train D loss: 0.6845, train F loss: -1.2851, train acc: 0.9936, domain acc: 0.5525, lr: 0.000256, lamb: 1.9104, time: 11.2306\n","epoch 1437: train D loss: 0.6830, train F loss: -1.2774, train acc: 0.9940, domain acc: 0.5575, lr: 0.000256, lamb: 1.9105, time: 10.6568\n","epoch 1438: train D loss: 0.6825, train F loss: -1.2791, train acc: 0.9940, domain acc: 0.5625, lr: 0.000256, lamb: 1.9107, time: 10.5148\n","epoch 1439: train D loss: 0.6839, train F loss: -1.2807, train acc: 0.9930, domain acc: 0.5667, lr: 0.000255, lamb: 1.9109, time: 11.2988\n","epoch 1440: train D loss: 0.6864, train F loss: -1.2892, train acc: 0.9948, domain acc: 0.5562, lr: 0.000255, lamb: 1.9111, time: 10.5718\n","epoch 1441: train D loss: 0.6853, train F loss: -1.2907, train acc: 0.9956, domain acc: 0.5566, lr: 0.000255, lamb: 1.9113, time: 11.0777\n","epoch 1442: train D loss: 0.6844, train F loss: -1.2854, train acc: 0.9944, domain acc: 0.5623, lr: 0.000255, lamb: 1.9114, time: 11.0337\n","epoch 1443: train D loss: 0.6863, train F loss: -1.2892, train acc: 0.9944, domain acc: 0.5575, lr: 0.000254, lamb: 1.9116, time: 10.5449\n","epoch 1444: train D loss: 0.6829, train F loss: -1.2850, train acc: 0.9944, domain acc: 0.5673, lr: 0.000254, lamb: 1.9118, time: 10.6549\n","epoch 1445: train D loss: 0.6865, train F loss: -1.2929, train acc: 0.9952, domain acc: 0.5623, lr: 0.000254, lamb: 1.9120, time: 11.1154\n","epoch 1446: train D loss: 0.6826, train F loss: -1.2863, train acc: 0.9954, domain acc: 0.5628, lr: 0.000254, lamb: 1.9122, time: 10.1942\n","epoch 1447: train D loss: 0.6815, train F loss: -1.2828, train acc: 0.9950, domain acc: 0.5648, lr: 0.000253, lamb: 1.9124, time: 10.6455\n","epoch 1448: train D loss: 0.6843, train F loss: -1.2910, train acc: 0.9966, domain acc: 0.5608, lr: 0.000253, lamb: 1.9125, time: 11.0607\n","epoch 1449: train D loss: 0.6826, train F loss: -1.2843, train acc: 0.9942, domain acc: 0.5692, lr: 0.000253, lamb: 1.9127, time: 9.8190\n","epoch 1450: train D loss: 0.6830, train F loss: -1.2841, train acc: 0.9948, domain acc: 0.5658, lr: 0.000253, lamb: 1.9129, time: 10.6491\n","epoch 1451: train D loss: 0.6836, train F loss: -1.2810, train acc: 0.9924, domain acc: 0.5629, lr: 0.000252, lamb: 1.9131, time: 10.0277\n","epoch 1452: train D loss: 0.6859, train F loss: -1.2855, train acc: 0.9928, domain acc: 0.5606, lr: 0.000252, lamb: 1.9133, time: 9.7244\n","epoch 1453: train D loss: 0.6867, train F loss: -1.2922, train acc: 0.9942, domain acc: 0.5553, lr: 0.000252, lamb: 1.9134, time: 10.2310\n","epoch 1454: train D loss: 0.6867, train F loss: -1.2944, train acc: 0.9958, domain acc: 0.5545, lr: 0.000252, lamb: 1.9136, time: 11.1600\n","epoch 1455: train D loss: 0.6872, train F loss: -1.2945, train acc: 0.9942, domain acc: 0.5543, lr: 0.000251, lamb: 1.9138, time: 10.1543\n","epoch 1456: train D loss: 0.6850, train F loss: -1.2897, train acc: 0.9954, domain acc: 0.5607, lr: 0.000251, lamb: 1.9140, time: 10.6851\n","epoch 1457: train D loss: 0.6833, train F loss: -1.2791, train acc: 0.9942, domain acc: 0.5644, lr: 0.000251, lamb: 1.9142, time: 9.7546\n","epoch 1458: train D loss: 0.6870, train F loss: -1.2871, train acc: 0.9928, domain acc: 0.5574, lr: 0.000251, lamb: 1.9143, time: 10.2354\n","epoch 1459: train D loss: 0.6873, train F loss: -1.2970, train acc: 0.9954, domain acc: 0.5534, lr: 0.000250, lamb: 1.9145, time: 10.3151\n","epoch 1460: train D loss: 0.6848, train F loss: -1.2881, train acc: 0.9936, domain acc: 0.5598, lr: 0.000250, lamb: 1.9147, time: 9.6845\n","epoch 1461: train D loss: 0.6835, train F loss: -1.2899, train acc: 0.9948, domain acc: 0.5664, lr: 0.000250, lamb: 1.9149, time: 9.8995\n","epoch 1462: train D loss: 0.6845, train F loss: -1.2895, train acc: 0.9938, domain acc: 0.5623, lr: 0.000250, lamb: 1.9151, time: 9.4212\n","epoch 1463: train D loss: 0.6837, train F loss: -1.2839, train acc: 0.9936, domain acc: 0.5646, lr: 0.000249, lamb: 1.9152, time: 9.7305\n","epoch 1464: train D loss: 0.6847, train F loss: -1.2883, train acc: 0.9936, domain acc: 0.5649, lr: 0.000249, lamb: 1.9154, time: 9.9483\n","epoch 1465: train D loss: 0.6817, train F loss: -1.2790, train acc: 0.9944, domain acc: 0.5715, lr: 0.000249, lamb: 1.9156, time: 11.1894\n","epoch 1466: train D loss: 0.6846, train F loss: -1.2889, train acc: 0.9946, domain acc: 0.5596, lr: 0.000249, lamb: 1.9158, time: 11.4004\n","epoch 1467: train D loss: 0.6829, train F loss: -1.2889, train acc: 0.9948, domain acc: 0.5614, lr: 0.000248, lamb: 1.9160, time: 10.5164\n","epoch 1468: train D loss: 0.6869, train F loss: -1.2903, train acc: 0.9934, domain acc: 0.5476, lr: 0.000248, lamb: 1.9161, time: 9.9324\n","epoch 1469: train D loss: 0.6855, train F loss: -1.2920, train acc: 0.9954, domain acc: 0.5587, lr: 0.000248, lamb: 1.9163, time: 9.9783\n","epoch 1470: train D loss: 0.6832, train F loss: -1.2866, train acc: 0.9946, domain acc: 0.5651, lr: 0.000248, lamb: 1.9165, time: 10.0436\n","epoch 1471: train D loss: 0.6856, train F loss: -1.2924, train acc: 0.9938, domain acc: 0.5515, lr: 0.000247, lamb: 1.9167, time: 9.9653\n","epoch 1472: train D loss: 0.6862, train F loss: -1.2938, train acc: 0.9948, domain acc: 0.5548, lr: 0.000247, lamb: 1.9169, time: 9.9587\n","epoch 1473: train D loss: 0.6880, train F loss: -1.3004, train acc: 0.9956, domain acc: 0.5474, lr: 0.000247, lamb: 1.9170, time: 9.9523\n","epoch 1474: train D loss: 0.6869, train F loss: -1.2952, train acc: 0.9946, domain acc: 0.5535, lr: 0.000247, lamb: 1.9172, time: 9.8652\n","epoch 1475: train D loss: 0.6863, train F loss: -1.2955, train acc: 0.9954, domain acc: 0.5603, lr: 0.000246, lamb: 1.9174, time: 9.6000\n","epoch 1476: train D loss: 0.6865, train F loss: -1.2930, train acc: 0.9944, domain acc: 0.5529, lr: 0.000246, lamb: 1.9176, time: 9.8544\n","epoch 1477: train D loss: 0.6852, train F loss: -1.2917, train acc: 0.9948, domain acc: 0.5588, lr: 0.000246, lamb: 1.9178, time: 9.7856\n","epoch 1478: train D loss: 0.6852, train F loss: -1.2907, train acc: 0.9946, domain acc: 0.5604, lr: 0.000246, lamb: 1.9179, time: 9.4278\n","epoch 1479: train D loss: 0.6856, train F loss: -1.2921, train acc: 0.9948, domain acc: 0.5635, lr: 0.000245, lamb: 1.9181, time: 9.6918\n","epoch 1480: train D loss: 0.6849, train F loss: -1.2928, train acc: 0.9934, domain acc: 0.5620, lr: 0.000245, lamb: 1.9183, time: 9.7811\n","epoch 1481: train D loss: 0.6829, train F loss: -1.2881, train acc: 0.9942, domain acc: 0.5593, lr: 0.000245, lamb: 1.9185, time: 10.1236\n","epoch 1482: train D loss: 0.6869, train F loss: -1.2982, train acc: 0.9956, domain acc: 0.5552, lr: 0.000245, lamb: 1.9186, time: 10.1060\n","epoch 1483: train D loss: 0.6867, train F loss: -1.2872, train acc: 0.9918, domain acc: 0.5523, lr: 0.000244, lamb: 1.9188, time: 10.6495\n","epoch 1484: train D loss: 0.6848, train F loss: -1.2923, train acc: 0.9944, domain acc: 0.5665, lr: 0.000244, lamb: 1.9190, time: 10.9654\n","epoch 1485: train D loss: 0.6878, train F loss: -1.3022, train acc: 0.9954, domain acc: 0.5500, lr: 0.000244, lamb: 1.9192, time: 11.0964\n","epoch 1486: train D loss: 0.6853, train F loss: -1.2946, train acc: 0.9946, domain acc: 0.5599, lr: 0.000244, lamb: 1.9193, time: 11.0414\n","epoch 1487: train D loss: 0.6807, train F loss: -1.2841, train acc: 0.9944, domain acc: 0.5736, lr: 0.000243, lamb: 1.9195, time: 10.8470\n","epoch 1488: train D loss: 0.6852, train F loss: -1.2915, train acc: 0.9942, domain acc: 0.5565, lr: 0.000243, lamb: 1.9197, time: 11.2187\n","epoch 1489: train D loss: 0.6869, train F loss: -1.3004, train acc: 0.9958, domain acc: 0.5509, lr: 0.000243, lamb: 1.9199, time: 10.4579\n","epoch 1490: train D loss: 0.6858, train F loss: -1.2948, train acc: 0.9944, domain acc: 0.5571, lr: 0.000243, lamb: 1.9201, time: 10.0510\n","epoch 1491: train D loss: 0.6876, train F loss: -1.2986, train acc: 0.9940, domain acc: 0.5457, lr: 0.000243, lamb: 1.9202, time: 9.7880\n","epoch 1492: train D loss: 0.6860, train F loss: -1.3019, train acc: 0.9962, domain acc: 0.5561, lr: 0.000242, lamb: 1.9204, time: 10.5467\n","epoch 1493: train D loss: 0.6853, train F loss: -1.2932, train acc: 0.9946, domain acc: 0.5532, lr: 0.000242, lamb: 1.9206, time: 10.0424\n","epoch 1494: train D loss: 0.6849, train F loss: -1.2949, train acc: 0.9950, domain acc: 0.5617, lr: 0.000242, lamb: 1.9208, time: 10.3519\n","epoch 1495: train D loss: 0.6861, train F loss: -1.2949, train acc: 0.9942, domain acc: 0.5541, lr: 0.000242, lamb: 1.9209, time: 10.0269\n","epoch 1496: train D loss: 0.6867, train F loss: -1.2967, train acc: 0.9940, domain acc: 0.5587, lr: 0.000241, lamb: 1.9211, time: 10.0361\n","epoch 1497: train D loss: 0.6876, train F loss: -1.2972, train acc: 0.9946, domain acc: 0.5480, lr: 0.000241, lamb: 1.9213, time: 10.0597\n","epoch 1498: train D loss: 0.6866, train F loss: -1.2990, train acc: 0.9946, domain acc: 0.5588, lr: 0.000241, lamb: 1.9215, time: 10.0478\n","epoch 1499: train D loss: 0.6873, train F loss: -1.2963, train acc: 0.9948, domain acc: 0.5558, lr: 0.000241, lamb: 1.9216, time: 9.9461\n","epoch 1500: train D loss: 0.6845, train F loss: -1.2921, train acc: 0.9942, domain acc: 0.5582, lr: 0.000240, lamb: 1.9218, time: 10.5165\n","epoch 1501: train D loss: 0.6864, train F loss: -1.2987, train acc: 0.9950, domain acc: 0.5539, lr: 0.000240, lamb: 1.9220, time: 9.5907\n","epoch 1502: train D loss: 0.6870, train F loss: -1.2933, train acc: 0.9944, domain acc: 0.5488, lr: 0.000240, lamb: 1.9222, time: 10.9129\n","epoch 1503: train D loss: 0.6851, train F loss: -1.2958, train acc: 0.9944, domain acc: 0.5604, lr: 0.000240, lamb: 1.9223, time: 10.2194\n","epoch 1504: train D loss: 0.6874, train F loss: -1.2968, train acc: 0.9950, domain acc: 0.5508, lr: 0.000239, lamb: 1.9225, time: 10.2943\n","epoch 1505: train D loss: 0.6845, train F loss: -1.2982, train acc: 0.9954, domain acc: 0.5675, lr: 0.000239, lamb: 1.9227, time: 10.3592\n","epoch 1506: train D loss: 0.6845, train F loss: -1.2990, train acc: 0.9962, domain acc: 0.5638, lr: 0.000239, lamb: 1.9229, time: 10.2488\n","epoch 1507: train D loss: 0.6861, train F loss: -1.2953, train acc: 0.9944, domain acc: 0.5549, lr: 0.000239, lamb: 1.9230, time: 10.5762\n","epoch 1508: train D loss: 0.6851, train F loss: -1.2985, train acc: 0.9948, domain acc: 0.5595, lr: 0.000238, lamb: 1.9232, time: 10.3208\n","epoch 1509: train D loss: 0.6859, train F loss: -1.2961, train acc: 0.9936, domain acc: 0.5609, lr: 0.000238, lamb: 1.9234, time: 10.3042\n","epoch 1510: train D loss: 0.6841, train F loss: -1.2923, train acc: 0.9936, domain acc: 0.5644, lr: 0.000238, lamb: 1.9236, time: 11.5798\n","epoch 1511: train D loss: 0.6863, train F loss: -1.3028, train acc: 0.9954, domain acc: 0.5599, lr: 0.000238, lamb: 1.9237, time: 10.6700\n","epoch 1512: train D loss: 0.6842, train F loss: -1.2985, train acc: 0.9954, domain acc: 0.5586, lr: 0.000237, lamb: 1.9239, time: 10.2256\n","epoch 1513: train D loss: 0.6868, train F loss: -1.2969, train acc: 0.9946, domain acc: 0.5503, lr: 0.000237, lamb: 1.9241, time: 10.3845\n","epoch 1514: train D loss: 0.6858, train F loss: -1.3014, train acc: 0.9948, domain acc: 0.5560, lr: 0.000237, lamb: 1.9243, time: 11.5553\n","epoch 1515: train D loss: 0.6870, train F loss: -1.3019, train acc: 0.9948, domain acc: 0.5500, lr: 0.000237, lamb: 1.9244, time: 10.9166\n","epoch 1516: train D loss: 0.6874, train F loss: -1.3029, train acc: 0.9946, domain acc: 0.5541, lr: 0.000237, lamb: 1.9246, time: 10.5744\n","epoch 1517: train D loss: 0.6867, train F loss: -1.3021, train acc: 0.9942, domain acc: 0.5552, lr: 0.000236, lamb: 1.9248, time: 10.3881\n","epoch 1518: train D loss: 0.6867, train F loss: -1.3052, train acc: 0.9960, domain acc: 0.5512, lr: 0.000236, lamb: 1.9249, time: 10.9129\n","epoch 1519: train D loss: 0.6857, train F loss: -1.2971, train acc: 0.9936, domain acc: 0.5595, lr: 0.000236, lamb: 1.9251, time: 10.4154\n","epoch 1520: train D loss: 0.6858, train F loss: -1.2976, train acc: 0.9948, domain acc: 0.5519, lr: 0.000236, lamb: 1.9253, time: 10.1974\n","epoch 1521: train D loss: 0.6838, train F loss: -1.2989, train acc: 0.9946, domain acc: 0.5643, lr: 0.000235, lamb: 1.9255, time: 10.1434\n","epoch 1522: train D loss: 0.6872, train F loss: -1.2975, train acc: 0.9942, domain acc: 0.5539, lr: 0.000235, lamb: 1.9256, time: 10.1221\n","epoch 1523: train D loss: 0.6887, train F loss: -1.3027, train acc: 0.9936, domain acc: 0.5456, lr: 0.000235, lamb: 1.9258, time: 10.0707\n","epoch 1524: train D loss: 0.6862, train F loss: -1.3035, train acc: 0.9952, domain acc: 0.5508, lr: 0.000235, lamb: 1.9260, time: 10.1190\n","epoch 1525: train D loss: 0.6870, train F loss: -1.3057, train acc: 0.9952, domain acc: 0.5547, lr: 0.000234, lamb: 1.9262, time: 9.6375\n","epoch 1526: train D loss: 0.6865, train F loss: -1.2988, train acc: 0.9940, domain acc: 0.5555, lr: 0.000234, lamb: 1.9263, time: 9.5793\n","epoch 1527: train D loss: 0.6844, train F loss: -1.2954, train acc: 0.9940, domain acc: 0.5599, lr: 0.000234, lamb: 1.9265, time: 9.5516\n","epoch 1528: train D loss: 0.6858, train F loss: -1.3047, train acc: 0.9962, domain acc: 0.5522, lr: 0.000234, lamb: 1.9267, time: 9.2730\n","epoch 1529: train D loss: 0.6847, train F loss: -1.2968, train acc: 0.9942, domain acc: 0.5567, lr: 0.000233, lamb: 1.9268, time: 9.6207\n","epoch 1530: train D loss: 0.6851, train F loss: -1.2978, train acc: 0.9944, domain acc: 0.5608, lr: 0.000233, lamb: 1.9270, time: 9.1254\n","epoch 1531: train D loss: 0.6852, train F loss: -1.3011, train acc: 0.9954, domain acc: 0.5562, lr: 0.000233, lamb: 1.9272, time: 9.5454\n","epoch 1532: train D loss: 0.6858, train F loss: -1.2967, train acc: 0.9936, domain acc: 0.5606, lr: 0.000233, lamb: 1.9274, time: 9.2457\n","epoch 1533: train D loss: 0.6849, train F loss: -1.3009, train acc: 0.9952, domain acc: 0.5561, lr: 0.000233, lamb: 1.9275, time: 9.4760\n","epoch 1534: train D loss: 0.6873, train F loss: -1.3023, train acc: 0.9942, domain acc: 0.5542, lr: 0.000232, lamb: 1.9277, time: 9.4355\n","epoch 1535: train D loss: 0.6875, train F loss: -1.3105, train acc: 0.9966, domain acc: 0.5481, lr: 0.000232, lamb: 1.9279, time: 9.2204\n","epoch 1536: train D loss: 0.6861, train F loss: -1.3039, train acc: 0.9952, domain acc: 0.5570, lr: 0.000232, lamb: 1.9280, time: 9.5034\n","epoch 1537: train D loss: 0.6847, train F loss: -1.2978, train acc: 0.9934, domain acc: 0.5578, lr: 0.000232, lamb: 1.9282, time: 9.0715\n","epoch 1538: train D loss: 0.6851, train F loss: -1.2969, train acc: 0.9930, domain acc: 0.5563, lr: 0.000231, lamb: 1.9284, time: 9.4688\n","epoch 1539: train D loss: 0.6828, train F loss: -1.2956, train acc: 0.9938, domain acc: 0.5644, lr: 0.000231, lamb: 1.9286, time: 9.2024\n","epoch 1540: train D loss: 0.6845, train F loss: -1.2974, train acc: 0.9944, domain acc: 0.5629, lr: 0.000231, lamb: 1.9287, time: 9.3622\n","epoch 1541: train D loss: 0.6866, train F loss: -1.3036, train acc: 0.9938, domain acc: 0.5521, lr: 0.000231, lamb: 1.9289, time: 9.3653\n","epoch 1542: train D loss: 0.6832, train F loss: -1.2996, train acc: 0.9944, domain acc: 0.5639, lr: 0.000230, lamb: 1.9291, time: 9.1895\n","epoch 1543: train D loss: 0.6842, train F loss: -1.2982, train acc: 0.9956, domain acc: 0.5610, lr: 0.000230, lamb: 1.9292, time: 9.4502\n","epoch 1544: train D loss: 0.6859, train F loss: -1.3023, train acc: 0.9946, domain acc: 0.5488, lr: 0.000230, lamb: 1.9294, time: 9.1461\n","epoch 1545: train D loss: 0.6852, train F loss: -1.2991, train acc: 0.9936, domain acc: 0.5540, lr: 0.000230, lamb: 1.9296, time: 9.4846\n","epoch 1546: train D loss: 0.6858, train F loss: -1.3033, train acc: 0.9944, domain acc: 0.5571, lr: 0.000230, lamb: 1.9297, time: 9.1648\n","epoch 1547: train D loss: 0.6841, train F loss: -1.2945, train acc: 0.9926, domain acc: 0.5644, lr: 0.000229, lamb: 1.9299, time: 9.4032\n","epoch 1548: train D loss: 0.6858, train F loss: -1.3042, train acc: 0.9948, domain acc: 0.5573, lr: 0.000229, lamb: 1.9301, time: 9.3212\n","epoch 1549: train D loss: 0.6872, train F loss: -1.3096, train acc: 0.9958, domain acc: 0.5529, lr: 0.000229, lamb: 1.9303, time: 9.3262\n","epoch 1550: train D loss: 0.6862, train F loss: -1.3010, train acc: 0.9938, domain acc: 0.5589, lr: 0.000229, lamb: 1.9304, time: 9.5446\n","epoch 1551: train D loss: 0.6837, train F loss: -1.2975, train acc: 0.9936, domain acc: 0.5628, lr: 0.000228, lamb: 1.9306, time: 9.1451\n","epoch 1552: train D loss: 0.6837, train F loss: -1.2995, train acc: 0.9944, domain acc: 0.5608, lr: 0.000228, lamb: 1.9308, time: 9.4775\n","epoch 1553: train D loss: 0.6827, train F loss: -1.2977, train acc: 0.9952, domain acc: 0.5643, lr: 0.000228, lamb: 1.9309, time: 9.1605\n","epoch 1554: train D loss: 0.6856, train F loss: -1.3010, train acc: 0.9930, domain acc: 0.5625, lr: 0.000228, lamb: 1.9311, time: 9.4933\n","epoch 1555: train D loss: 0.6852, train F loss: -1.3058, train acc: 0.9956, domain acc: 0.5630, lr: 0.000227, lamb: 1.9313, time: 9.2223\n","epoch 1556: train D loss: 0.6864, train F loss: -1.3067, train acc: 0.9950, domain acc: 0.5528, lr: 0.000227, lamb: 1.9314, time: 9.2768\n","epoch 1557: train D loss: 0.6855, train F loss: -1.3002, train acc: 0.9944, domain acc: 0.5559, lr: 0.000227, lamb: 1.9316, time: 9.5368\n","epoch 1558: train D loss: 0.6868, train F loss: -1.3033, train acc: 0.9944, domain acc: 0.5551, lr: 0.000227, lamb: 1.9318, time: 9.1114\n","epoch 1559: train D loss: 0.6868, train F loss: -1.3056, train acc: 0.9944, domain acc: 0.5562, lr: 0.000227, lamb: 1.9319, time: 9.5504\n","epoch 1560: train D loss: 0.6856, train F loss: -1.3043, train acc: 0.9952, domain acc: 0.5619, lr: 0.000226, lamb: 1.9321, time: 9.1285\n","epoch 1561: train D loss: 0.6812, train F loss: -1.2932, train acc: 0.9938, domain acc: 0.5648, lr: 0.000226, lamb: 1.9323, time: 9.4449\n","epoch 1562: train D loss: 0.6826, train F loss: -1.2996, train acc: 0.9958, domain acc: 0.5680, lr: 0.000226, lamb: 1.9325, time: 9.2136\n","epoch 1563: train D loss: 0.6846, train F loss: -1.3050, train acc: 0.9960, domain acc: 0.5628, lr: 0.000226, lamb: 1.9326, time: 9.3197\n","epoch 1564: train D loss: 0.6856, train F loss: -1.3060, train acc: 0.9952, domain acc: 0.5592, lr: 0.000225, lamb: 1.9328, time: 9.3489\n","epoch 1565: train D loss: 0.6856, train F loss: -1.3034, train acc: 0.9952, domain acc: 0.5566, lr: 0.000225, lamb: 1.9330, time: 9.1762\n","epoch 1566: train D loss: 0.6834, train F loss: -1.2927, train acc: 0.9938, domain acc: 0.5687, lr: 0.000225, lamb: 1.9331, time: 9.4871\n","epoch 1567: train D loss: 0.6832, train F loss: -1.2979, train acc: 0.9948, domain acc: 0.5668, lr: 0.000225, lamb: 1.9333, time: 9.0166\n","epoch 1568: train D loss: 0.6842, train F loss: -1.3027, train acc: 0.9962, domain acc: 0.5593, lr: 0.000225, lamb: 1.9335, time: 9.4237\n","epoch 1569: train D loss: 0.6829, train F loss: -1.2990, train acc: 0.9936, domain acc: 0.5673, lr: 0.000224, lamb: 1.9336, time: 9.1598\n","epoch 1570: train D loss: 0.6843, train F loss: -1.3002, train acc: 0.9946, domain acc: 0.5580, lr: 0.000224, lamb: 1.9338, time: 9.2782\n","epoch 1571: train D loss: 0.6843, train F loss: -1.3038, train acc: 0.9948, domain acc: 0.5600, lr: 0.000224, lamb: 1.9340, time: 9.2679\n","epoch 1572: train D loss: 0.6847, train F loss: -1.3027, train acc: 0.9948, domain acc: 0.5577, lr: 0.000224, lamb: 1.9341, time: 9.1647\n","epoch 1573: train D loss: 0.6859, train F loss: -1.3085, train acc: 0.9954, domain acc: 0.5532, lr: 0.000223, lamb: 1.9343, time: 9.4264\n","epoch 1574: train D loss: 0.6824, train F loss: -1.2988, train acc: 0.9958, domain acc: 0.5671, lr: 0.000223, lamb: 1.9345, time: 8.9991\n","epoch 1575: train D loss: 0.6864, train F loss: -1.3047, train acc: 0.9944, domain acc: 0.5566, lr: 0.000223, lamb: 1.9346, time: 9.4322\n","epoch 1576: train D loss: 0.6872, train F loss: -1.3064, train acc: 0.9950, domain acc: 0.5454, lr: 0.000223, lamb: 1.9348, time: 9.0358\n","epoch 1577: train D loss: 0.6841, train F loss: -1.2996, train acc: 0.9936, domain acc: 0.5604, lr: 0.000223, lamb: 1.9350, time: 9.5515\n","epoch 1578: train D loss: 0.6862, train F loss: -1.3056, train acc: 0.9944, domain acc: 0.5605, lr: 0.000222, lamb: 1.9351, time: 9.1637\n","epoch 1579: train D loss: 0.6870, train F loss: -1.3075, train acc: 0.9944, domain acc: 0.5509, lr: 0.000222, lamb: 1.9353, time: 9.2565\n","epoch 1580: train D loss: 0.6847, train F loss: -1.3033, train acc: 0.9940, domain acc: 0.5571, lr: 0.000222, lamb: 1.9355, time: 9.4128\n","epoch 1581: train D loss: 0.6829, train F loss: -1.2945, train acc: 0.9944, domain acc: 0.5641, lr: 0.000222, lamb: 1.9356, time: 9.0600\n","epoch 1582: train D loss: 0.6844, train F loss: -1.3049, train acc: 0.9952, domain acc: 0.5599, lr: 0.000221, lamb: 1.9358, time: 9.3939\n","epoch 1583: train D loss: 0.6874, train F loss: -1.3086, train acc: 0.9948, domain acc: 0.5497, lr: 0.000221, lamb: 1.9360, time: 8.9778\n","epoch 1584: train D loss: 0.6855, train F loss: -1.3072, train acc: 0.9950, domain acc: 0.5590, lr: 0.000221, lamb: 1.9361, time: 9.4344\n","epoch 1585: train D loss: 0.6859, train F loss: -1.3081, train acc: 0.9952, domain acc: 0.5513, lr: 0.000221, lamb: 1.9363, time: 9.1762\n","epoch 1586: train D loss: 0.6843, train F loss: -1.3036, train acc: 0.9934, domain acc: 0.5606, lr: 0.000221, lamb: 1.9365, time: 9.2836\n","epoch 1587: train D loss: 0.6835, train F loss: -1.3036, train acc: 0.9952, domain acc: 0.5625, lr: 0.000220, lamb: 1.9366, time: 9.2568\n","epoch 1588: train D loss: 0.6859, train F loss: -1.3110, train acc: 0.9966, domain acc: 0.5552, lr: 0.000220, lamb: 1.9368, time: 9.1853\n","epoch 1589: train D loss: 0.6870, train F loss: -1.3125, train acc: 0.9954, domain acc: 0.5513, lr: 0.000220, lamb: 1.9370, time: 9.4829\n","epoch 1590: train D loss: 0.6853, train F loss: -1.3052, train acc: 0.9942, domain acc: 0.5610, lr: 0.000220, lamb: 1.9371, time: 9.0587\n","epoch 1591: train D loss: 0.6853, train F loss: -1.3086, train acc: 0.9952, domain acc: 0.5595, lr: 0.000219, lamb: 1.9373, time: 9.4522\n","epoch 1592: train D loss: 0.6853, train F loss: -1.3064, train acc: 0.9938, domain acc: 0.5612, lr: 0.000219, lamb: 1.9375, time: 9.0324\n","epoch 1593: train D loss: 0.6872, train F loss: -1.3129, train acc: 0.9944, domain acc: 0.5514, lr: 0.000219, lamb: 1.9376, time: 9.3156\n","epoch 1594: train D loss: 0.6844, train F loss: -1.3063, train acc: 0.9950, domain acc: 0.5594, lr: 0.000219, lamb: 1.9378, time: 9.1452\n","epoch 1595: train D loss: 0.6839, train F loss: -1.3071, train acc: 0.9962, domain acc: 0.5625, lr: 0.000219, lamb: 1.9379, time: 9.1810\n","epoch 1596: train D loss: 0.6845, train F loss: -1.3086, train acc: 0.9962, domain acc: 0.5602, lr: 0.000218, lamb: 1.9381, time: 9.3693\n","epoch 1597: train D loss: 0.6874, train F loss: -1.3086, train acc: 0.9942, domain acc: 0.5495, lr: 0.000218, lamb: 1.9383, time: 9.1295\n","epoch 1598: train D loss: 0.6863, train F loss: -1.3075, train acc: 0.9938, domain acc: 0.5560, lr: 0.000218, lamb: 1.9384, time: 9.4731\n","epoch 1599: train D loss: 0.6857, train F loss: -1.3083, train acc: 0.9952, domain acc: 0.5589, lr: 0.000218, lamb: 1.9386, time: 9.0602\n","epoch 1600: train D loss: 0.6846, train F loss: -1.3089, train acc: 0.9956, domain acc: 0.5556, lr: 0.000217, lamb: 1.9388, time: 9.4522\n","epoch 1601: train D loss: 0.6876, train F loss: -1.3073, train acc: 0.9944, domain acc: 0.5541, lr: 0.000217, lamb: 1.9389, time: 9.1614\n","epoch 1602: train D loss: 0.6866, train F loss: -1.3047, train acc: 0.9938, domain acc: 0.5528, lr: 0.000217, lamb: 1.9391, time: 9.2478\n","epoch 1603: train D loss: 0.6855, train F loss: -1.3080, train acc: 0.9934, domain acc: 0.5515, lr: 0.000217, lamb: 1.9393, time: 9.3329\n","epoch 1604: train D loss: 0.6868, train F loss: -1.3105, train acc: 0.9942, domain acc: 0.5567, lr: 0.000217, lamb: 1.9394, time: 9.1762\n","epoch 1605: train D loss: 0.6843, train F loss: -1.3085, train acc: 0.9948, domain acc: 0.5604, lr: 0.000216, lamb: 1.9396, time: 9.3803\n","epoch 1606: train D loss: 0.6863, train F loss: -1.3079, train acc: 0.9954, domain acc: 0.5604, lr: 0.000216, lamb: 1.9398, time: 9.0757\n","epoch 1607: train D loss: 0.6848, train F loss: -1.3122, train acc: 0.9960, domain acc: 0.5600, lr: 0.000216, lamb: 1.9399, time: 9.5012\n","epoch 1608: train D loss: 0.6871, train F loss: -1.3131, train acc: 0.9940, domain acc: 0.5533, lr: 0.000216, lamb: 1.9401, time: 9.1087\n","epoch 1609: train D loss: 0.6847, train F loss: -1.3049, train acc: 0.9952, domain acc: 0.5576, lr: 0.000216, lamb: 1.9402, time: 9.3678\n","epoch 1610: train D loss: 0.6849, train F loss: -1.3123, train acc: 0.9958, domain acc: 0.5547, lr: 0.000215, lamb: 1.9404, time: 9.2322\n","epoch 1611: train D loss: 0.6856, train F loss: -1.3112, train acc: 0.9950, domain acc: 0.5599, lr: 0.000215, lamb: 1.9406, time: 9.1499\n","epoch 1612: train D loss: 0.6858, train F loss: -1.3110, train acc: 0.9958, domain acc: 0.5506, lr: 0.000215, lamb: 1.9407, time: 9.2670\n","epoch 1613: train D loss: 0.6867, train F loss: -1.3045, train acc: 0.9938, domain acc: 0.5553, lr: 0.000215, lamb: 1.9409, time: 9.1042\n","epoch 1614: train D loss: 0.6851, train F loss: -1.3138, train acc: 0.9962, domain acc: 0.5628, lr: 0.000214, lamb: 1.9411, time: 9.4788\n","epoch 1615: train D loss: 0.6836, train F loss: -1.3089, train acc: 0.9958, domain acc: 0.5609, lr: 0.000214, lamb: 1.9412, time: 9.1191\n","epoch 1616: train D loss: 0.6864, train F loss: -1.3107, train acc: 0.9942, domain acc: 0.5552, lr: 0.000214, lamb: 1.9414, time: 9.4563\n","epoch 1617: train D loss: 0.6875, train F loss: -1.3141, train acc: 0.9954, domain acc: 0.5558, lr: 0.000214, lamb: 1.9415, time: 9.2224\n","epoch 1618: train D loss: 0.6870, train F loss: -1.3110, train acc: 0.9944, domain acc: 0.5457, lr: 0.000214, lamb: 1.9417, time: 9.3187\n","epoch 1619: train D loss: 0.6859, train F loss: -1.3128, train acc: 0.9954, domain acc: 0.5547, lr: 0.000213, lamb: 1.9419, time: 9.4389\n","epoch 1620: train D loss: 0.6856, train F loss: -1.3129, train acc: 0.9950, domain acc: 0.5536, lr: 0.000213, lamb: 1.9420, time: 9.1391\n","epoch 1621: train D loss: 0.6863, train F loss: -1.3060, train acc: 0.9934, domain acc: 0.5518, lr: 0.000213, lamb: 1.9422, time: 9.5208\n","epoch 1622: train D loss: 0.6880, train F loss: -1.3182, train acc: 0.9950, domain acc: 0.5511, lr: 0.000213, lamb: 1.9424, time: 9.2499\n","epoch 1623: train D loss: 0.6862, train F loss: -1.3147, train acc: 0.9952, domain acc: 0.5574, lr: 0.000213, lamb: 1.9425, time: 10.4529\n","epoch 1624: train D loss: 0.6857, train F loss: -1.3036, train acc: 0.9932, domain acc: 0.5574, lr: 0.000212, lamb: 1.9427, time: 9.7603\n","epoch 1625: train D loss: 0.6847, train F loss: -1.3094, train acc: 0.9952, domain acc: 0.5568, lr: 0.000212, lamb: 1.9428, time: 9.5880\n","epoch 1626: train D loss: 0.6848, train F loss: -1.3101, train acc: 0.9946, domain acc: 0.5585, lr: 0.000212, lamb: 1.9430, time: 9.9624\n","epoch 1627: train D loss: 0.6838, train F loss: -1.3051, train acc: 0.9934, domain acc: 0.5668, lr: 0.000212, lamb: 1.9432, time: 9.9809\n","epoch 1628: train D loss: 0.6847, train F loss: -1.3052, train acc: 0.9942, domain acc: 0.5570, lr: 0.000211, lamb: 1.9433, time: 10.4115\n","epoch 1629: train D loss: 0.6859, train F loss: -1.3121, train acc: 0.9952, domain acc: 0.5540, lr: 0.000211, lamb: 1.9435, time: 10.1093\n","epoch 1630: train D loss: 0.6845, train F loss: -1.3107, train acc: 0.9956, domain acc: 0.5590, lr: 0.000211, lamb: 1.9437, time: 10.4125\n","epoch 1631: train D loss: 0.6845, train F loss: -1.3086, train acc: 0.9954, domain acc: 0.5621, lr: 0.000211, lamb: 1.9438, time: 9.4356\n","epoch 1632: train D loss: 0.6876, train F loss: -1.3172, train acc: 0.9952, domain acc: 0.5506, lr: 0.000211, lamb: 1.9440, time: 10.6288\n","epoch 1633: train D loss: 0.6860, train F loss: -1.3135, train acc: 0.9934, domain acc: 0.5508, lr: 0.000210, lamb: 1.9441, time: 10.2917\n","epoch 1634: train D loss: 0.6852, train F loss: -1.3137, train acc: 0.9952, domain acc: 0.5577, lr: 0.000210, lamb: 1.9443, time: 9.7090\n","epoch 1635: train D loss: 0.6860, train F loss: -1.3152, train acc: 0.9954, domain acc: 0.5578, lr: 0.000210, lamb: 1.9445, time: 10.2419\n","epoch 1636: train D loss: 0.6860, train F loss: -1.3079, train acc: 0.9946, domain acc: 0.5561, lr: 0.000210, lamb: 1.9446, time: 10.7572\n","epoch 1637: train D loss: 0.6861, train F loss: -1.3172, train acc: 0.9954, domain acc: 0.5506, lr: 0.000210, lamb: 1.9448, time: 10.8949\n","epoch 1638: train D loss: 0.6868, train F loss: -1.3139, train acc: 0.9946, domain acc: 0.5563, lr: 0.000209, lamb: 1.9449, time: 10.1620\n","epoch 1639: train D loss: 0.6860, train F loss: -1.3160, train acc: 0.9946, domain acc: 0.5592, lr: 0.000209, lamb: 1.9451, time: 9.8707\n","epoch 1640: train D loss: 0.6874, train F loss: -1.3072, train acc: 0.9952, domain acc: 0.5548, lr: 0.000209, lamb: 1.9453, time: 10.6773\n","epoch 1641: train D loss: 0.6850, train F loss: -1.3160, train acc: 0.9958, domain acc: 0.5590, lr: 0.000209, lamb: 1.9454, time: 10.0399\n","epoch 1642: train D loss: 0.6859, train F loss: -1.3144, train acc: 0.9942, domain acc: 0.5581, lr: 0.000209, lamb: 1.9456, time: 10.1363\n","epoch 1643: train D loss: 0.6835, train F loss: -1.3133, train acc: 0.9950, domain acc: 0.5633, lr: 0.000208, lamb: 1.9457, time: 10.0427\n","epoch 1644: train D loss: 0.6832, train F loss: -1.3100, train acc: 0.9956, domain acc: 0.5631, lr: 0.000208, lamb: 1.9459, time: 10.0338\n","epoch 1645: train D loss: 0.6864, train F loss: -1.3137, train acc: 0.9940, domain acc: 0.5573, lr: 0.000208, lamb: 1.9461, time: 10.0720\n","epoch 1646: train D loss: 0.6856, train F loss: -1.3097, train acc: 0.9950, domain acc: 0.5616, lr: 0.000208, lamb: 1.9462, time: 9.9828\n","epoch 1647: train D loss: 0.6875, train F loss: -1.3169, train acc: 0.9944, domain acc: 0.5526, lr: 0.000207, lamb: 1.9464, time: 9.8954\n","epoch 1648: train D loss: 0.6855, train F loss: -1.3155, train acc: 0.9952, domain acc: 0.5502, lr: 0.000207, lamb: 1.9465, time: 10.9042\n","epoch 1649: train D loss: 0.6843, train F loss: -1.3105, train acc: 0.9938, domain acc: 0.5568, lr: 0.000207, lamb: 1.9467, time: 10.2451\n","epoch 1650: train D loss: 0.6850, train F loss: -1.3129, train acc: 0.9946, domain acc: 0.5562, lr: 0.000207, lamb: 1.9469, time: 10.6119\n","epoch 1651: train D loss: 0.6853, train F loss: -1.3089, train acc: 0.9938, domain acc: 0.5546, lr: 0.000207, lamb: 1.9470, time: 11.1763\n","epoch 1652: train D loss: 0.6846, train F loss: -1.3136, train acc: 0.9952, domain acc: 0.5627, lr: 0.000206, lamb: 1.9472, time: 10.2527\n","epoch 1653: train D loss: 0.6824, train F loss: -1.3092, train acc: 0.9942, domain acc: 0.5669, lr: 0.000206, lamb: 1.9473, time: 10.2047\n","epoch 1654: train D loss: 0.6846, train F loss: -1.3132, train acc: 0.9950, domain acc: 0.5578, lr: 0.000206, lamb: 1.9475, time: 10.0346\n","epoch 1655: train D loss: 0.6868, train F loss: -1.3200, train acc: 0.9948, domain acc: 0.5628, lr: 0.000206, lamb: 1.9477, time: 10.2175\n","epoch 1656: train D loss: 0.6836, train F loss: -1.3032, train acc: 0.9930, domain acc: 0.5636, lr: 0.000206, lamb: 1.9478, time: 10.3290\n","epoch 1657: train D loss: 0.6851, train F loss: -1.3173, train acc: 0.9954, domain acc: 0.5600, lr: 0.000205, lamb: 1.9480, time: 10.7500\n","epoch 1658: train D loss: 0.6878, train F loss: -1.3249, train acc: 0.9966, domain acc: 0.5524, lr: 0.000205, lamb: 1.9481, time: 11.3452\n","epoch 1659: train D loss: 0.6877, train F loss: -1.3197, train acc: 0.9950, domain acc: 0.5489, lr: 0.000205, lamb: 1.9483, time: 9.8155\n","epoch 1660: train D loss: 0.6860, train F loss: -1.3139, train acc: 0.9934, domain acc: 0.5499, lr: 0.000205, lamb: 1.9484, time: 9.7376\n","epoch 1661: train D loss: 0.6870, train F loss: -1.3157, train acc: 0.9936, domain acc: 0.5547, lr: 0.000205, lamb: 1.9486, time: 9.8326\n","epoch 1662: train D loss: 0.6869, train F loss: -1.3221, train acc: 0.9962, domain acc: 0.5543, lr: 0.000204, lamb: 1.9488, time: 9.5532\n","epoch 1663: train D loss: 0.6856, train F loss: -1.3209, train acc: 0.9964, domain acc: 0.5590, lr: 0.000204, lamb: 1.9489, time: 9.3412\n","epoch 1664: train D loss: 0.6870, train F loss: -1.3212, train acc: 0.9950, domain acc: 0.5535, lr: 0.000204, lamb: 1.9491, time: 9.5018\n","epoch 1665: train D loss: 0.6854, train F loss: -1.3163, train acc: 0.9948, domain acc: 0.5572, lr: 0.000204, lamb: 1.9492, time: 9.5244\n","epoch 1666: train D loss: 0.6862, train F loss: -1.3139, train acc: 0.9924, domain acc: 0.5540, lr: 0.000204, lamb: 1.9494, time: 9.5177\n","epoch 1667: train D loss: 0.6871, train F loss: -1.3201, train acc: 0.9956, domain acc: 0.5471, lr: 0.000203, lamb: 1.9496, time: 10.7479\n","epoch 1668: train D loss: 0.6880, train F loss: -1.3186, train acc: 0.9946, domain acc: 0.5445, lr: 0.000203, lamb: 1.9497, time: 10.0017\n","epoch 1669: train D loss: 0.6858, train F loss: -1.3208, train acc: 0.9960, domain acc: 0.5549, lr: 0.000203, lamb: 1.9499, time: 10.1969\n","epoch 1670: train D loss: 0.6851, train F loss: -1.3128, train acc: 0.9946, domain acc: 0.5519, lr: 0.000203, lamb: 1.9500, time: 9.9623\n","epoch 1671: train D loss: 0.6846, train F loss: -1.3168, train acc: 0.9950, domain acc: 0.5535, lr: 0.000203, lamb: 1.9502, time: 10.2801\n","epoch 1672: train D loss: 0.6822, train F loss: -1.2986, train acc: 0.9938, domain acc: 0.5682, lr: 0.000202, lamb: 1.9503, time: 10.6777\n","epoch 1673: train D loss: 0.6857, train F loss: -1.3140, train acc: 0.9936, domain acc: 0.5619, lr: 0.000202, lamb: 1.9505, time: 10.5606\n","epoch 1674: train D loss: 0.6849, train F loss: -1.3135, train acc: 0.9950, domain acc: 0.5601, lr: 0.000202, lamb: 1.9507, time: 9.8976\n","epoch 1675: train D loss: 0.6851, train F loss: -1.3162, train acc: 0.9944, domain acc: 0.5574, lr: 0.000202, lamb: 1.9508, time: 10.8802\n","epoch 1676: train D loss: 0.6849, train F loss: -1.3180, train acc: 0.9964, domain acc: 0.5614, lr: 0.000202, lamb: 1.9510, time: 10.1185\n","epoch 1677: train D loss: 0.6858, train F loss: -1.3155, train acc: 0.9938, domain acc: 0.5574, lr: 0.000201, lamb: 1.9511, time: 10.2990\n","epoch 1678: train D loss: 0.6831, train F loss: -1.3091, train acc: 0.9930, domain acc: 0.5597, lr: 0.000201, lamb: 1.9513, time: 10.7334\n","epoch 1679: train D loss: 0.6842, train F loss: -1.3162, train acc: 0.9950, domain acc: 0.5574, lr: 0.000201, lamb: 1.9514, time: 10.9260\n","epoch 1680: train D loss: 0.6827, train F loss: -1.3138, train acc: 0.9958, domain acc: 0.5628, lr: 0.000201, lamb: 1.9516, time: 10.2136\n","epoch 1681: train D loss: 0.6860, train F loss: -1.3217, train acc: 0.9962, domain acc: 0.5553, lr: 0.000201, lamb: 1.9517, time: 10.0256\n","epoch 1682: train D loss: 0.6850, train F loss: -1.3136, train acc: 0.9946, domain acc: 0.5636, lr: 0.000200, lamb: 1.9519, time: 10.1296\n","epoch 1683: train D loss: 0.6871, train F loss: -1.3209, train acc: 0.9946, domain acc: 0.5501, lr: 0.000200, lamb: 1.9521, time: 10.7312\n","epoch 1684: train D loss: 0.6858, train F loss: -1.3121, train acc: 0.9926, domain acc: 0.5564, lr: 0.000200, lamb: 1.9522, time: 10.1607\n","epoch 1685: train D loss: 0.6863, train F loss: -1.3200, train acc: 0.9940, domain acc: 0.5516, lr: 0.000200, lamb: 1.9524, time: 10.3471\n","epoch 1686: train D loss: 0.6841, train F loss: -1.3186, train acc: 0.9946, domain acc: 0.5565, lr: 0.000200, lamb: 1.9525, time: 10.1218\n","epoch 1687: train D loss: 0.6850, train F loss: -1.3167, train acc: 0.9954, domain acc: 0.5542, lr: 0.000199, lamb: 1.9527, time: 10.1508\n","epoch 1688: train D loss: 0.6851, train F loss: -1.3120, train acc: 0.9936, domain acc: 0.5570, lr: 0.000199, lamb: 1.9528, time: 10.0565\n","epoch 1689: train D loss: 0.6879, train F loss: -1.3260, train acc: 0.9962, domain acc: 0.5476, lr: 0.000199, lamb: 1.9530, time: 9.9749\n","epoch 1690: train D loss: 0.6852, train F loss: -1.3149, train acc: 0.9952, domain acc: 0.5573, lr: 0.000199, lamb: 1.9532, time: 9.2988\n","epoch 1691: train D loss: 0.6864, train F loss: -1.3188, train acc: 0.9936, domain acc: 0.5590, lr: 0.000199, lamb: 1.9533, time: 9.6944\n","epoch 1692: train D loss: 0.6857, train F loss: -1.3178, train acc: 0.9948, domain acc: 0.5485, lr: 0.000198, lamb: 1.9535, time: 9.0671\n","epoch 1693: train D loss: 0.6848, train F loss: -1.3204, train acc: 0.9956, domain acc: 0.5616, lr: 0.000198, lamb: 1.9536, time: 9.8443\n","epoch 1694: train D loss: 0.6824, train F loss: -1.3153, train acc: 0.9956, domain acc: 0.5651, lr: 0.000198, lamb: 1.9538, time: 9.7232\n","epoch 1695: train D loss: 0.6834, train F loss: -1.3173, train acc: 0.9952, domain acc: 0.5620, lr: 0.000198, lamb: 1.9539, time: 10.8080\n","epoch 1696: train D loss: 0.6844, train F loss: -1.3177, train acc: 0.9944, domain acc: 0.5603, lr: 0.000198, lamb: 1.9541, time: 10.0307\n","epoch 1697: train D loss: 0.6836, train F loss: -1.3168, train acc: 0.9952, domain acc: 0.5644, lr: 0.000197, lamb: 1.9542, time: 10.3134\n","epoch 1698: train D loss: 0.6849, train F loss: -1.3205, train acc: 0.9954, domain acc: 0.5577, lr: 0.000197, lamb: 1.9544, time: 9.8891\n","epoch 1699: train D loss: 0.6835, train F loss: -1.3170, train acc: 0.9956, domain acc: 0.5651, lr: 0.000197, lamb: 1.9545, time: 10.1828\n","epoch 1700: train D loss: 0.6855, train F loss: -1.3213, train acc: 0.9960, domain acc: 0.5561, lr: 0.000197, lamb: 1.9547, time: 10.8955\n","epoch 1701: train D loss: 0.6871, train F loss: -1.3234, train acc: 0.9940, domain acc: 0.5479, lr: 0.000197, lamb: 1.9549, time: 10.7373\n","epoch 1702: train D loss: 0.6874, train F loss: -1.3247, train acc: 0.9954, domain acc: 0.5475, lr: 0.000196, lamb: 1.9550, time: 9.9266\n","epoch 1703: train D loss: 0.6841, train F loss: -1.3177, train acc: 0.9940, domain acc: 0.5625, lr: 0.000196, lamb: 1.9552, time: 10.4164\n","epoch 1704: train D loss: 0.6865, train F loss: -1.3202, train acc: 0.9954, domain acc: 0.5512, lr: 0.000196, lamb: 1.9553, time: 10.3437\n","epoch 1705: train D loss: 0.6863, train F loss: -1.3224, train acc: 0.9950, domain acc: 0.5601, lr: 0.000196, lamb: 1.9555, time: 10.3130\n","epoch 1706: train D loss: 0.6848, train F loss: -1.3187, train acc: 0.9946, domain acc: 0.5623, lr: 0.000196, lamb: 1.9556, time: 9.8904\n","epoch 1707: train D loss: 0.6868, train F loss: -1.3259, train acc: 0.9958, domain acc: 0.5555, lr: 0.000195, lamb: 1.9558, time: 9.9419\n","epoch 1708: train D loss: 0.6863, train F loss: -1.3247, train acc: 0.9956, domain acc: 0.5505, lr: 0.000195, lamb: 1.9559, time: 9.9903\n","epoch 1709: train D loss: 0.6862, train F loss: -1.3186, train acc: 0.9946, domain acc: 0.5575, lr: 0.000195, lamb: 1.9561, time: 10.6791\n","epoch 1710: train D loss: 0.6842, train F loss: -1.3206, train acc: 0.9956, domain acc: 0.5574, lr: 0.000195, lamb: 1.9562, time: 10.8573\n","epoch 1711: train D loss: 0.6874, train F loss: -1.3175, train acc: 0.9954, domain acc: 0.5495, lr: 0.000195, lamb: 1.9564, time: 11.0249\n","epoch 1712: train D loss: 0.6850, train F loss: -1.3041, train acc: 0.9936, domain acc: 0.5586, lr: 0.000194, lamb: 1.9566, time: 10.2690\n","epoch 1713: train D loss: 0.6834, train F loss: -1.3170, train acc: 0.9938, domain acc: 0.5651, lr: 0.000194, lamb: 1.9567, time: 10.4728\n","epoch 1714: train D loss: 0.6820, train F loss: -1.3131, train acc: 0.9946, domain acc: 0.5694, lr: 0.000194, lamb: 1.9569, time: 10.6736\n","epoch 1715: train D loss: 0.6830, train F loss: -1.3166, train acc: 0.9960, domain acc: 0.5614, lr: 0.000194, lamb: 1.9570, time: 10.2500\n","epoch 1716: train D loss: 0.6840, train F loss: -1.3192, train acc: 0.9950, domain acc: 0.5576, lr: 0.000194, lamb: 1.9572, time: 10.0501\n","epoch 1717: train D loss: 0.6847, train F loss: -1.3246, train acc: 0.9964, domain acc: 0.5558, lr: 0.000193, lamb: 1.9573, time: 10.0312\n","epoch 1718: train D loss: 0.6856, train F loss: -1.3033, train acc: 0.9950, domain acc: 0.5539, lr: 0.000193, lamb: 1.9575, time: 9.6272\n","epoch 1719: train D loss: 0.6851, train F loss: -1.3214, train acc: 0.9946, domain acc: 0.5612, lr: 0.000193, lamb: 1.9576, time: 9.7165\n","epoch 1720: train D loss: 0.6838, train F loss: -1.3184, train acc: 0.9948, domain acc: 0.5643, lr: 0.000193, lamb: 1.9578, time: 9.2810\n","epoch 1721: train D loss: 0.6830, train F loss: -1.3160, train acc: 0.9952, domain acc: 0.5639, lr: 0.000193, lamb: 1.9579, time: 9.4427\n","epoch 1722: train D loss: 0.6817, train F loss: -1.3164, train acc: 0.9950, domain acc: 0.5660, lr: 0.000192, lamb: 1.9581, time: 9.4122\n","epoch 1723: train D loss: 0.6854, train F loss: -1.3214, train acc: 0.9950, domain acc: 0.5575, lr: 0.000192, lamb: 1.9582, time: 9.2118\n","epoch 1724: train D loss: 0.6840, train F loss: -1.3191, train acc: 0.9950, domain acc: 0.5571, lr: 0.000192, lamb: 1.9584, time: 9.6268\n","epoch 1725: train D loss: 0.6837, train F loss: -1.3209, train acc: 0.9950, domain acc: 0.5629, lr: 0.000192, lamb: 1.9585, time: 9.1267\n","epoch 1726: train D loss: 0.6839, train F loss: -1.3193, train acc: 0.9946, domain acc: 0.5608, lr: 0.000192, lamb: 1.9587, time: 9.5786\n","epoch 1727: train D loss: 0.6848, train F loss: -1.3239, train acc: 0.9952, domain acc: 0.5613, lr: 0.000192, lamb: 1.9588, time: 9.5228\n","epoch 1728: train D loss: 0.6840, train F loss: -1.3241, train acc: 0.9966, domain acc: 0.5584, lr: 0.000191, lamb: 1.9590, time: 9.6057\n","epoch 1729: train D loss: 0.6856, train F loss: -1.3238, train acc: 0.9952, domain acc: 0.5654, lr: 0.000191, lamb: 1.9591, time: 9.9053\n","epoch 1730: train D loss: 0.6852, train F loss: -1.3161, train acc: 0.9944, domain acc: 0.5542, lr: 0.000191, lamb: 1.9593, time: 10.3965\n","epoch 1731: train D loss: 0.6829, train F loss: -1.3200, train acc: 0.9954, domain acc: 0.5645, lr: 0.000191, lamb: 1.9595, time: 10.7586\n","epoch 1732: train D loss: 0.6844, train F loss: -1.3257, train acc: 0.9966, domain acc: 0.5623, lr: 0.000191, lamb: 1.9596, time: 10.1495\n","epoch 1733: train D loss: 0.6864, train F loss: -1.3263, train acc: 0.9950, domain acc: 0.5550, lr: 0.000190, lamb: 1.9598, time: 9.9155\n","epoch 1734: train D loss: 0.6867, train F loss: -1.3252, train acc: 0.9950, domain acc: 0.5430, lr: 0.000190, lamb: 1.9599, time: 9.8060\n","epoch 1735: train D loss: 0.6840, train F loss: -1.3210, train acc: 0.9950, domain acc: 0.5579, lr: 0.000190, lamb: 1.9601, time: 9.8917\n","epoch 1736: train D loss: 0.6848, train F loss: -1.3266, train acc: 0.9958, domain acc: 0.5567, lr: 0.000190, lamb: 1.9602, time: 9.9102\n","epoch 1737: train D loss: 0.6848, train F loss: -1.3235, train acc: 0.9948, domain acc: 0.5564, lr: 0.000190, lamb: 1.9604, time: 9.3875\n","epoch 1738: train D loss: 0.6848, train F loss: -1.3194, train acc: 0.9950, domain acc: 0.5615, lr: 0.000189, lamb: 1.9605, time: 10.8048\n","epoch 1739: train D loss: 0.6832, train F loss: -1.3121, train acc: 0.9940, domain acc: 0.5642, lr: 0.000189, lamb: 1.9607, time: 10.6751\n","epoch 1740: train D loss: 0.6887, train F loss: -1.3301, train acc: 0.9938, domain acc: 0.5459, lr: 0.000189, lamb: 1.9608, time: 10.1032\n","epoch 1741: train D loss: 0.6867, train F loss: -1.3270, train acc: 0.9948, domain acc: 0.5559, lr: 0.000189, lamb: 1.9610, time: 9.6815\n","epoch 1742: train D loss: 0.6849, train F loss: -1.3229, train acc: 0.9942, domain acc: 0.5601, lr: 0.000189, lamb: 1.9611, time: 9.9631\n","epoch 1743: train D loss: 0.6823, train F loss: -1.3207, train acc: 0.9956, domain acc: 0.5675, lr: 0.000188, lamb: 1.9613, time: 9.6051\n","epoch 1744: train D loss: 0.6847, train F loss: -1.3252, train acc: 0.9958, domain acc: 0.5605, lr: 0.000188, lamb: 1.9614, time: 9.6984\n","epoch 1745: train D loss: 0.6848, train F loss: -1.3251, train acc: 0.9956, domain acc: 0.5599, lr: 0.000188, lamb: 1.9616, time: 9.3651\n","epoch 1746: train D loss: 0.6861, train F loss: -1.3234, train acc: 0.9952, domain acc: 0.5559, lr: 0.000188, lamb: 1.9617, time: 9.2867\n","epoch 1747: train D loss: 0.6842, train F loss: -1.3277, train acc: 0.9966, domain acc: 0.5613, lr: 0.000188, lamb: 1.9619, time: 9.2364\n","epoch 1748: train D loss: 0.6863, train F loss: -1.3147, train acc: 0.9940, domain acc: 0.5556, lr: 0.000188, lamb: 1.9620, time: 9.2122\n","epoch 1749: train D loss: 0.6856, train F loss: -1.3265, train acc: 0.9950, domain acc: 0.5579, lr: 0.000187, lamb: 1.9622, time: 9.1713\n","epoch 1750: train D loss: 0.6830, train F loss: -1.3246, train acc: 0.9956, domain acc: 0.5672, lr: 0.000187, lamb: 1.9623, time: 9.2112\n","epoch 1751: train D loss: 0.6847, train F loss: -1.3234, train acc: 0.9948, domain acc: 0.5569, lr: 0.000187, lamb: 1.9625, time: 9.1313\n","epoch 1752: train D loss: 0.6869, train F loss: -1.3306, train acc: 0.9958, domain acc: 0.5506, lr: 0.000187, lamb: 1.9626, time: 9.3824\n","epoch 1753: train D loss: 0.6852, train F loss: -1.3278, train acc: 0.9958, domain acc: 0.5587, lr: 0.000187, lamb: 1.9628, time: 9.6395\n","epoch 1754: train D loss: 0.6852, train F loss: -1.3278, train acc: 0.9950, domain acc: 0.5556, lr: 0.000186, lamb: 1.9629, time: 9.8867\n","epoch 1755: train D loss: 0.6833, train F loss: -1.3228, train acc: 0.9946, domain acc: 0.5609, lr: 0.000186, lamb: 1.9631, time: 9.8835\n","epoch 1756: train D loss: 0.6846, train F loss: -1.3232, train acc: 0.9936, domain acc: 0.5604, lr: 0.000186, lamb: 1.9632, time: 9.7418\n","epoch 1757: train D loss: 0.6889, train F loss: -1.3345, train acc: 0.9944, domain acc: 0.5437, lr: 0.000186, lamb: 1.9634, time: 9.8375\n","epoch 1758: train D loss: 0.6877, train F loss: -1.3306, train acc: 0.9938, domain acc: 0.5473, lr: 0.000186, lamb: 1.9635, time: 9.8837\n","epoch 1759: train D loss: 0.6831, train F loss: -1.3253, train acc: 0.9958, domain acc: 0.5648, lr: 0.000185, lamb: 1.9637, time: 9.8161\n","epoch 1760: train D loss: 0.6864, train F loss: -1.3313, train acc: 0.9960, domain acc: 0.5536, lr: 0.000185, lamb: 1.9638, time: 9.9142\n","epoch 1761: train D loss: 0.6868, train F loss: -1.3322, train acc: 0.9960, domain acc: 0.5569, lr: 0.000185, lamb: 1.9640, time: 9.9169\n","epoch 1762: train D loss: 0.6827, train F loss: -1.3273, train acc: 0.9972, domain acc: 0.5638, lr: 0.000185, lamb: 1.9641, time: 9.1083\n","epoch 1763: train D loss: 0.6856, train F loss: -1.3258, train acc: 0.9950, domain acc: 0.5550, lr: 0.000185, lamb: 1.9643, time: 9.6063\n","epoch 1764: train D loss: 0.6842, train F loss: -1.3240, train acc: 0.9946, domain acc: 0.5597, lr: 0.000185, lamb: 1.9644, time: 9.6290\n","epoch 1765: train D loss: 0.6867, train F loss: -1.3295, train acc: 0.9948, domain acc: 0.5548, lr: 0.000184, lamb: 1.9646, time: 10.5815\n","epoch 1766: train D loss: 0.6867, train F loss: -1.3322, train acc: 0.9958, domain acc: 0.5536, lr: 0.000184, lamb: 1.9647, time: 10.0015\n","epoch 1767: train D loss: 0.6876, train F loss: -1.3347, train acc: 0.9962, domain acc: 0.5483, lr: 0.000184, lamb: 1.9649, time: 10.4241\n","epoch 1768: train D loss: 0.6852, train F loss: -1.3262, train acc: 0.9952, domain acc: 0.5557, lr: 0.000184, lamb: 1.9650, time: 10.3042\n","epoch 1769: train D loss: 0.6869, train F loss: -1.3282, train acc: 0.9938, domain acc: 0.5531, lr: 0.000184, lamb: 1.9652, time: 9.8904\n","epoch 1770: train D loss: 0.6860, train F loss: -1.3254, train acc: 0.9954, domain acc: 0.5590, lr: 0.000183, lamb: 1.9653, time: 9.8848\n","epoch 1771: train D loss: 0.6859, train F loss: -1.3288, train acc: 0.9938, domain acc: 0.5538, lr: 0.000183, lamb: 1.9655, time: 9.8192\n","epoch 1772: train D loss: 0.6860, train F loss: -1.3312, train acc: 0.9946, domain acc: 0.5604, lr: 0.000183, lamb: 1.9656, time: 9.3173\n","epoch 1773: train D loss: 0.6840, train F loss: -1.3290, train acc: 0.9960, domain acc: 0.5667, lr: 0.000183, lamb: 1.9658, time: 9.7653\n","epoch 1774: train D loss: 0.6875, train F loss: -1.3364, train acc: 0.9958, domain acc: 0.5453, lr: 0.000183, lamb: 1.9659, time: 9.6371\n","epoch 1775: train D loss: 0.6887, train F loss: -1.3369, train acc: 0.9962, domain acc: 0.5453, lr: 0.000183, lamb: 1.9660, time: 10.2026\n","epoch 1776: train D loss: 0.6873, train F loss: -1.3336, train acc: 0.9948, domain acc: 0.5430, lr: 0.000182, lamb: 1.9662, time: 10.4663\n","epoch 1777: train D loss: 0.6883, train F loss: -1.3339, train acc: 0.9958, domain acc: 0.5459, lr: 0.000182, lamb: 1.9663, time: 10.2761\n","epoch 1778: train D loss: 0.6857, train F loss: -1.3317, train acc: 0.9956, domain acc: 0.5552, lr: 0.000182, lamb: 1.9665, time: 9.7411\n","epoch 1779: train D loss: 0.6862, train F loss: -1.3304, train acc: 0.9956, domain acc: 0.5526, lr: 0.000182, lamb: 1.9666, time: 9.9081\n","epoch 1780: train D loss: 0.6877, train F loss: -1.3361, train acc: 0.9960, domain acc: 0.5489, lr: 0.000182, lamb: 1.9668, time: 9.8825\n","epoch 1781: train D loss: 0.6877, train F loss: -1.3328, train acc: 0.9950, domain acc: 0.5475, lr: 0.000181, lamb: 1.9669, time: 9.7221\n","epoch 1782: train D loss: 0.6851, train F loss: -1.3286, train acc: 0.9946, domain acc: 0.5544, lr: 0.000181, lamb: 1.9671, time: 9.1341\n","epoch 1783: train D loss: 0.6868, train F loss: -1.3312, train acc: 0.9956, domain acc: 0.5497, lr: 0.000181, lamb: 1.9672, time: 9.5959\n","epoch 1784: train D loss: 0.6840, train F loss: -1.3278, train acc: 0.9950, domain acc: 0.5634, lr: 0.000181, lamb: 1.9674, time: 9.1024\n","epoch 1785: train D loss: 0.6868, train F loss: -1.3285, train acc: 0.9936, domain acc: 0.5522, lr: 0.000181, lamb: 1.9675, time: 10.4908\n","epoch 1786: train D loss: 0.6881, train F loss: -1.3313, train acc: 0.9942, domain acc: 0.5501, lr: 0.000181, lamb: 1.9677, time: 10.1839\n","epoch 1787: train D loss: 0.6872, train F loss: -1.3289, train acc: 0.9938, domain acc: 0.5444, lr: 0.000180, lamb: 1.9678, time: 10.3965\n","epoch 1788: train D loss: 0.6870, train F loss: -1.3331, train acc: 0.9952, domain acc: 0.5525, lr: 0.000180, lamb: 1.9680, time: 9.5011\n","epoch 1789: train D loss: 0.6860, train F loss: -1.3311, train acc: 0.9956, domain acc: 0.5546, lr: 0.000180, lamb: 1.9681, time: 10.1650\n","epoch 1790: train D loss: 0.6841, train F loss: -1.3292, train acc: 0.9958, domain acc: 0.5581, lr: 0.000180, lamb: 1.9683, time: 10.6853\n","epoch 1791: train D loss: 0.6856, train F loss: -1.3312, train acc: 0.9948, domain acc: 0.5571, lr: 0.000180, lamb: 1.9684, time: 10.6763\n","epoch 1792: train D loss: 0.6866, train F loss: -1.3315, train acc: 0.9940, domain acc: 0.5598, lr: 0.000179, lamb: 1.9686, time: 9.9183\n","epoch 1793: train D loss: 0.6866, train F loss: -1.3311, train acc: 0.9944, domain acc: 0.5555, lr: 0.000179, lamb: 1.9687, time: 10.8034\n","epoch 1794: train D loss: 0.6869, train F loss: -1.3350, train acc: 0.9952, domain acc: 0.5521, lr: 0.000179, lamb: 1.9688, time: 10.6017\n","epoch 1795: train D loss: 0.6890, train F loss: -1.3348, train acc: 0.9938, domain acc: 0.5477, lr: 0.000179, lamb: 1.9690, time: 10.5730\n","epoch 1796: train D loss: 0.6864, train F loss: -1.3349, train acc: 0.9958, domain acc: 0.5564, lr: 0.000179, lamb: 1.9691, time: 10.7507\n","epoch 1797: train D loss: 0.6875, train F loss: -1.3355, train acc: 0.9952, domain acc: 0.5520, lr: 0.000179, lamb: 1.9693, time: 10.2922\n","epoch 1798: train D loss: 0.6887, train F loss: -1.3358, train acc: 0.9944, domain acc: 0.5495, lr: 0.000178, lamb: 1.9694, time: 10.5099\n","epoch 1799: train D loss: 0.6875, train F loss: -1.3367, train acc: 0.9944, domain acc: 0.5520, lr: 0.000178, lamb: 1.9696, time: 10.7912\n","epoch 1800: train D loss: 0.6868, train F loss: -1.3360, train acc: 0.9952, domain acc: 0.5496, lr: 0.000178, lamb: 1.9697, time: 9.9183\n","epoch 1801: train D loss: 0.6874, train F loss: -1.3355, train acc: 0.9954, domain acc: 0.5519, lr: 0.000178, lamb: 1.9699, time: 9.7666\n","epoch 1802: train D loss: 0.6849, train F loss: -1.3267, train acc: 0.9948, domain acc: 0.5535, lr: 0.000178, lamb: 1.9700, time: 10.7716\n","epoch 1803: train D loss: 0.6857, train F loss: -1.3358, train acc: 0.9964, domain acc: 0.5551, lr: 0.000177, lamb: 1.9702, time: 10.1735\n","epoch 1804: train D loss: 0.6862, train F loss: -1.3349, train acc: 0.9956, domain acc: 0.5525, lr: 0.000177, lamb: 1.9703, time: 9.9693\n","epoch 1805: train D loss: 0.6877, train F loss: -1.3392, train acc: 0.9960, domain acc: 0.5521, lr: 0.000177, lamb: 1.9705, time: 9.9619\n","epoch 1806: train D loss: 0.6859, train F loss: -1.3321, train acc: 0.9950, domain acc: 0.5571, lr: 0.000177, lamb: 1.9706, time: 10.0413\n","epoch 1807: train D loss: 0.6856, train F loss: -1.3324, train acc: 0.9952, domain acc: 0.5641, lr: 0.000177, lamb: 1.9707, time: 9.8997\n","epoch 1808: train D loss: 0.6824, train F loss: -1.3315, train acc: 0.9966, domain acc: 0.5665, lr: 0.000177, lamb: 1.9709, time: 9.9943\n","epoch 1809: train D loss: 0.6886, train F loss: -1.3392, train acc: 0.9956, domain acc: 0.5461, lr: 0.000176, lamb: 1.9710, time: 10.4451\n","epoch 1810: train D loss: 0.6862, train F loss: -1.3349, train acc: 0.9956, domain acc: 0.5517, lr: 0.000176, lamb: 1.9712, time: 10.3555\n","epoch 1811: train D loss: 0.6879, train F loss: -1.3398, train acc: 0.9952, domain acc: 0.5469, lr: 0.000176, lamb: 1.9713, time: 10.4078\n","epoch 1812: train D loss: 0.6859, train F loss: -1.3316, train acc: 0.9944, domain acc: 0.5559, lr: 0.000176, lamb: 1.9715, time: 10.8326\n","epoch 1813: train D loss: 0.6877, train F loss: -1.3359, train acc: 0.9958, domain acc: 0.5495, lr: 0.000176, lamb: 1.9716, time: 10.2908\n","epoch 1814: train D loss: 0.6867, train F loss: -1.3342, train acc: 0.9956, domain acc: 0.5485, lr: 0.000176, lamb: 1.9718, time: 9.6862\n","epoch 1815: train D loss: 0.6889, train F loss: -1.3403, train acc: 0.9952, domain acc: 0.5523, lr: 0.000175, lamb: 1.9719, time: 10.4577\n","epoch 1816: train D loss: 0.6861, train F loss: -1.3361, train acc: 0.9960, domain acc: 0.5546, lr: 0.000175, lamb: 1.9720, time: 10.2824\n","epoch 1817: train D loss: 0.6860, train F loss: -1.3377, train acc: 0.9964, domain acc: 0.5541, lr: 0.000175, lamb: 1.9722, time: 10.8213\n","epoch 1818: train D loss: 0.6846, train F loss: -1.3359, train acc: 0.9964, domain acc: 0.5628, lr: 0.000175, lamb: 1.9723, time: 10.2301\n","epoch 1819: train D loss: 0.6852, train F loss: -1.3376, train acc: 0.9958, domain acc: 0.5584, lr: 0.000175, lamb: 1.9725, time: 10.6978\n","epoch 1820: train D loss: 0.6845, train F loss: -1.3335, train acc: 0.9954, domain acc: 0.5570, lr: 0.000174, lamb: 1.9726, time: 10.2992\n","epoch 1821: train D loss: 0.6862, train F loss: -1.3345, train acc: 0.9954, domain acc: 0.5531, lr: 0.000174, lamb: 1.9728, time: 10.5830\n","epoch 1822: train D loss: 0.6890, train F loss: -1.3416, train acc: 0.9952, domain acc: 0.5423, lr: 0.000174, lamb: 1.9729, time: 11.0076\n","epoch 1823: train D loss: 0.6872, train F loss: -1.3397, train acc: 0.9954, domain acc: 0.5509, lr: 0.000174, lamb: 1.9731, time: 10.9493\n","epoch 1824: train D loss: 0.6873, train F loss: -1.3356, train acc: 0.9942, domain acc: 0.5493, lr: 0.000174, lamb: 1.9732, time: 10.4396\n","epoch 1825: train D loss: 0.6859, train F loss: -1.3355, train acc: 0.9956, domain acc: 0.5571, lr: 0.000174, lamb: 1.9733, time: 10.1428\n","epoch 1826: train D loss: 0.6850, train F loss: -1.3382, train acc: 0.9966, domain acc: 0.5565, lr: 0.000173, lamb: 1.9735, time: 9.5114\n","epoch 1827: train D loss: 0.6834, train F loss: -1.3317, train acc: 0.9948, domain acc: 0.5662, lr: 0.000173, lamb: 1.9736, time: 9.6388\n","epoch 1828: train D loss: 0.6852, train F loss: -1.3363, train acc: 0.9954, domain acc: 0.5560, lr: 0.000173, lamb: 1.9738, time: 10.6129\n","epoch 1829: train D loss: 0.6838, train F loss: -1.3315, train acc: 0.9952, domain acc: 0.5668, lr: 0.000173, lamb: 1.9739, time: 10.1445\n","epoch 1830: train D loss: 0.6856, train F loss: -1.3329, train acc: 0.9956, domain acc: 0.5554, lr: 0.000173, lamb: 1.9741, time: 10.7240\n","epoch 1831: train D loss: 0.6867, train F loss: -1.3355, train acc: 0.9948, domain acc: 0.5567, lr: 0.000173, lamb: 1.9742, time: 10.4345\n","epoch 1832: train D loss: 0.6860, train F loss: -1.3362, train acc: 0.9952, domain acc: 0.5536, lr: 0.000172, lamb: 1.9744, time: 10.4843\n","epoch 1833: train D loss: 0.6869, train F loss: -1.3391, train acc: 0.9958, domain acc: 0.5536, lr: 0.000172, lamb: 1.9745, time: 9.7224\n","epoch 1834: train D loss: 0.6876, train F loss: -1.3417, train acc: 0.9960, domain acc: 0.5568, lr: 0.000172, lamb: 1.9746, time: 10.1081\n","epoch 1835: train D loss: 0.6876, train F loss: -1.3378, train acc: 0.9954, domain acc: 0.5513, lr: 0.000172, lamb: 1.9748, time: 9.5496\n","epoch 1836: train D loss: 0.6882, train F loss: -1.3386, train acc: 0.9948, domain acc: 0.5444, lr: 0.000172, lamb: 1.9749, time: 10.1842\n","epoch 1837: train D loss: 0.6850, train F loss: -1.3375, train acc: 0.9964, domain acc: 0.5553, lr: 0.000172, lamb: 1.9751, time: 9.7597\n","epoch 1838: train D loss: 0.6867, train F loss: -1.3382, train acc: 0.9956, domain acc: 0.5514, lr: 0.000171, lamb: 1.9752, time: 10.2578\n","epoch 1839: train D loss: 0.6844, train F loss: -1.3304, train acc: 0.9956, domain acc: 0.5584, lr: 0.000171, lamb: 1.9754, time: 9.7094\n","epoch 1840: train D loss: 0.6867, train F loss: -1.3343, train acc: 0.9944, domain acc: 0.5506, lr: 0.000171, lamb: 1.9755, time: 9.9692\n","epoch 1841: train D loss: 0.6870, train F loss: -1.3384, train acc: 0.9948, domain acc: 0.5480, lr: 0.000171, lamb: 1.9756, time: 9.6043\n","epoch 1842: train D loss: 0.6860, train F loss: -1.3371, train acc: 0.9956, domain acc: 0.5572, lr: 0.000171, lamb: 1.9758, time: 9.6365\n","epoch 1843: train D loss: 0.6846, train F loss: -1.3323, train acc: 0.9944, domain acc: 0.5621, lr: 0.000171, lamb: 1.9759, time: 10.4021\n","epoch 1844: train D loss: 0.6842, train F loss: -1.3346, train acc: 0.9952, domain acc: 0.5608, lr: 0.000170, lamb: 1.9761, time: 10.6065\n","epoch 1845: train D loss: 0.6845, train F loss: -1.3330, train acc: 0.9944, domain acc: 0.5620, lr: 0.000170, lamb: 1.9762, time: 10.7847\n","epoch 1846: train D loss: 0.6875, train F loss: -1.3417, train acc: 0.9966, domain acc: 0.5515, lr: 0.000170, lamb: 1.9764, time: 11.0344\n","epoch 1847: train D loss: 0.6900, train F loss: -1.3412, train acc: 0.9936, domain acc: 0.5412, lr: 0.000170, lamb: 1.9765, time: 10.8807\n","epoch 1848: train D loss: 0.6873, train F loss: -1.3433, train acc: 0.9958, domain acc: 0.5503, lr: 0.000170, lamb: 1.9766, time: 10.7676\n","epoch 1849: train D loss: 0.6858, train F loss: -1.3401, train acc: 0.9958, domain acc: 0.5566, lr: 0.000170, lamb: 1.9768, time: 10.1957\n","epoch 1850: train D loss: 0.6890, train F loss: -1.3406, train acc: 0.9946, domain acc: 0.5427, lr: 0.000169, lamb: 1.9769, time: 9.4739\n","epoch 1851: train D loss: 0.6866, train F loss: -1.3385, train acc: 0.9948, domain acc: 0.5499, lr: 0.000169, lamb: 1.9771, time: 9.7354\n","epoch 1852: train D loss: 0.6858, train F loss: -1.3358, train acc: 0.9958, domain acc: 0.5557, lr: 0.000169, lamb: 1.9772, time: 9.4595\n","epoch 1853: train D loss: 0.6876, train F loss: -1.3323, train acc: 0.9940, domain acc: 0.5485, lr: 0.000169, lamb: 1.9773, time: 9.5559\n","epoch 1854: train D loss: 0.6854, train F loss: -1.3400, train acc: 0.9956, domain acc: 0.5580, lr: 0.000169, lamb: 1.9775, time: 9.6627\n","epoch 1855: train D loss: 0.6861, train F loss: -1.3392, train acc: 0.9952, domain acc: 0.5522, lr: 0.000168, lamb: 1.9776, time: 9.7938\n","epoch 1856: train D loss: 0.6875, train F loss: -1.3471, train acc: 0.9970, domain acc: 0.5506, lr: 0.000168, lamb: 1.9778, time: 10.6327\n","epoch 1857: train D loss: 0.6855, train F loss: -1.3371, train acc: 0.9946, domain acc: 0.5537, lr: 0.000168, lamb: 1.9779, time: 9.8209\n","epoch 1858: train D loss: 0.6858, train F loss: -1.3419, train acc: 0.9966, domain acc: 0.5569, lr: 0.000168, lamb: 1.9781, time: 9.8247\n","epoch 1859: train D loss: 0.6839, train F loss: -1.3332, train acc: 0.9960, domain acc: 0.5650, lr: 0.000168, lamb: 1.9782, time: 9.9721\n","epoch 1860: train D loss: 0.6861, train F loss: -1.3401, train acc: 0.9952, domain acc: 0.5563, lr: 0.000168, lamb: 1.9783, time: 10.1134\n","epoch 1861: train D loss: 0.6877, train F loss: -1.3458, train acc: 0.9964, domain acc: 0.5505, lr: 0.000167, lamb: 1.9785, time: 9.9843\n","epoch 1862: train D loss: 0.6871, train F loss: -1.3436, train acc: 0.9960, domain acc: 0.5495, lr: 0.000167, lamb: 1.9786, time: 10.0153\n","epoch 1863: train D loss: 0.6855, train F loss: -1.3407, train acc: 0.9964, domain acc: 0.5505, lr: 0.000167, lamb: 1.9788, time: 9.7034\n","epoch 1864: train D loss: 0.6843, train F loss: -1.3374, train acc: 0.9958, domain acc: 0.5555, lr: 0.000167, lamb: 1.9789, time: 10.0052\n","epoch 1865: train D loss: 0.6869, train F loss: -1.3438, train acc: 0.9956, domain acc: 0.5498, lr: 0.000167, lamb: 1.9790, time: 9.7934\n","epoch 1866: train D loss: 0.6870, train F loss: -1.3432, train acc: 0.9956, domain acc: 0.5500, lr: 0.000167, lamb: 1.9792, time: 9.6227\n","epoch 1867: train D loss: 0.6879, train F loss: -1.3444, train acc: 0.9954, domain acc: 0.5484, lr: 0.000166, lamb: 1.9793, time: 9.7058\n","epoch 1868: train D loss: 0.6848, train F loss: -1.3393, train acc: 0.9960, domain acc: 0.5544, lr: 0.000166, lamb: 1.9795, time: 9.7452\n","epoch 1869: train D loss: 0.6869, train F loss: -1.3409, train acc: 0.9958, domain acc: 0.5486, lr: 0.000166, lamb: 1.9796, time: 9.3867\n","epoch 1870: train D loss: 0.6882, train F loss: -1.3447, train acc: 0.9958, domain acc: 0.5507, lr: 0.000166, lamb: 1.9797, time: 9.5537\n","epoch 1871: train D loss: 0.6882, train F loss: -1.3446, train acc: 0.9962, domain acc: 0.5508, lr: 0.000166, lamb: 1.9799, time: 9.4190\n","epoch 1872: train D loss: 0.6861, train F loss: -1.3444, train acc: 0.9964, domain acc: 0.5512, lr: 0.000166, lamb: 1.9800, time: 9.1989\n","epoch 1873: train D loss: 0.6898, train F loss: -1.3492, train acc: 0.9956, domain acc: 0.5409, lr: 0.000165, lamb: 1.9802, time: 9.1497\n","epoch 1874: train D loss: 0.6882, train F loss: -1.3393, train acc: 0.9944, domain acc: 0.5487, lr: 0.000165, lamb: 1.9803, time: 9.4660\n","epoch 1875: train D loss: 0.6877, train F loss: -1.3439, train acc: 0.9948, domain acc: 0.5499, lr: 0.000165, lamb: 1.9805, time: 9.2179\n","epoch 1876: train D loss: 0.6869, train F loss: -1.3383, train acc: 0.9946, domain acc: 0.5459, lr: 0.000165, lamb: 1.9806, time: 9.9796\n","epoch 1877: train D loss: 0.6863, train F loss: -1.3377, train acc: 0.9940, domain acc: 0.5511, lr: 0.000165, lamb: 1.9807, time: 11.6970\n","epoch 1878: train D loss: 0.6866, train F loss: -1.3423, train acc: 0.9954, domain acc: 0.5502, lr: 0.000165, lamb: 1.9809, time: 10.6379\n","epoch 1879: train D loss: 0.6875, train F loss: -1.3473, train acc: 0.9972, domain acc: 0.5490, lr: 0.000164, lamb: 1.9810, time: 10.9547\n","epoch 1880: train D loss: 0.6867, train F loss: -1.3470, train acc: 0.9964, domain acc: 0.5525, lr: 0.000164, lamb: 1.9812, time: 10.9737\n","epoch 1881: train D loss: 0.6858, train F loss: -1.3392, train acc: 0.9944, domain acc: 0.5556, lr: 0.000164, lamb: 1.9813, time: 9.9369\n","epoch 1882: train D loss: 0.6875, train F loss: -1.3463, train acc: 0.9958, domain acc: 0.5505, lr: 0.000164, lamb: 1.9814, time: 10.3408\n","epoch 1883: train D loss: 0.6871, train F loss: -1.3423, train acc: 0.9944, domain acc: 0.5512, lr: 0.000164, lamb: 1.9816, time: 10.2531\n","epoch 1884: train D loss: 0.6874, train F loss: -1.3481, train acc: 0.9962, domain acc: 0.5490, lr: 0.000164, lamb: 1.9817, time: 10.8527\n","epoch 1885: train D loss: 0.6876, train F loss: -1.3474, train acc: 0.9960, domain acc: 0.5485, lr: 0.000164, lamb: 1.9818, time: 9.8347\n","epoch 1886: train D loss: 0.6847, train F loss: -1.3409, train acc: 0.9962, domain acc: 0.5605, lr: 0.000163, lamb: 1.9820, time: 9.8446\n","epoch 1887: train D loss: 0.6858, train F loss: -1.3424, train acc: 0.9950, domain acc: 0.5532, lr: 0.000163, lamb: 1.9821, time: 10.2575\n","epoch 1888: train D loss: 0.6860, train F loss: -1.3381, train acc: 0.9938, domain acc: 0.5560, lr: 0.000163, lamb: 1.9823, time: 9.9539\n","epoch 1889: train D loss: 0.6853, train F loss: -1.3407, train acc: 0.9952, domain acc: 0.5517, lr: 0.000163, lamb: 1.9824, time: 10.1400\n","epoch 1890: train D loss: 0.6867, train F loss: -1.3459, train acc: 0.9956, domain acc: 0.5500, lr: 0.000163, lamb: 1.9825, time: 9.8139\n","epoch 1891: train D loss: 0.6878, train F loss: -1.3507, train acc: 0.9964, domain acc: 0.5439, lr: 0.000163, lamb: 1.9827, time: 9.9955\n","epoch 1892: train D loss: 0.6859, train F loss: -1.3437, train acc: 0.9956, domain acc: 0.5537, lr: 0.000162, lamb: 1.9828, time: 9.8323\n","epoch 1893: train D loss: 0.6874, train F loss: -1.3467, train acc: 0.9956, domain acc: 0.5483, lr: 0.000162, lamb: 1.9830, time: 9.6748\n","epoch 1894: train D loss: 0.6860, train F loss: -1.3442, train acc: 0.9948, domain acc: 0.5560, lr: 0.000162, lamb: 1.9831, time: 9.6952\n","epoch 1895: train D loss: 0.6868, train F loss: -1.3432, train acc: 0.9942, domain acc: 0.5488, lr: 0.000162, lamb: 1.9832, time: 9.4880\n","epoch 1896: train D loss: 0.6882, train F loss: -1.3415, train acc: 0.9940, domain acc: 0.5498, lr: 0.000162, lamb: 1.9834, time: 9.3515\n","epoch 1897: train D loss: 0.6855, train F loss: -1.3454, train acc: 0.9964, domain acc: 0.5578, lr: 0.000162, lamb: 1.9835, time: 9.3416\n","epoch 1898: train D loss: 0.6865, train F loss: -1.3469, train acc: 0.9964, domain acc: 0.5616, lr: 0.000161, lamb: 1.9837, time: 9.4353\n","epoch 1899: train D loss: 0.6863, train F loss: -1.3461, train acc: 0.9958, domain acc: 0.5544, lr: 0.000161, lamb: 1.9838, time: 9.3845\n","epoch 1900: train D loss: 0.6855, train F loss: -1.3420, train acc: 0.9960, domain acc: 0.5530, lr: 0.000161, lamb: 1.9839, time: 9.2863\n","epoch 1901: train D loss: 0.6874, train F loss: -1.3440, train acc: 0.9946, domain acc: 0.5467, lr: 0.000161, lamb: 1.9841, time: 10.0142\n","epoch 1902: train D loss: 0.6886, train F loss: -1.3493, train acc: 0.9950, domain acc: 0.5510, lr: 0.000161, lamb: 1.9842, time: 9.9737\n","epoch 1903: train D loss: 0.6856, train F loss: -1.3432, train acc: 0.9956, domain acc: 0.5494, lr: 0.000161, lamb: 1.9843, time: 9.7720\n","epoch 1904: train D loss: 0.6840, train F loss: -1.3426, train acc: 0.9962, domain acc: 0.5628, lr: 0.000160, lamb: 1.9845, time: 9.7986\n","epoch 1905: train D loss: 0.6853, train F loss: -1.3461, train acc: 0.9968, domain acc: 0.5600, lr: 0.000160, lamb: 1.9846, time: 9.8063\n","epoch 1906: train D loss: 0.6868, train F loss: -1.3485, train acc: 0.9956, domain acc: 0.5488, lr: 0.000160, lamb: 1.9848, time: 9.8963\n","epoch 1907: train D loss: 0.6876, train F loss: -1.3426, train acc: 0.9944, domain acc: 0.5501, lr: 0.000160, lamb: 1.9849, time: 9.8378\n","epoch 1908: train D loss: 0.6845, train F loss: -1.3447, train acc: 0.9964, domain acc: 0.5610, lr: 0.000160, lamb: 1.9850, time: 10.1505\n","epoch 1909: train D loss: 0.6861, train F loss: -1.3453, train acc: 0.9952, domain acc: 0.5540, lr: 0.000160, lamb: 1.9852, time: 9.7399\n","epoch 1910: train D loss: 0.6861, train F loss: -1.3426, train acc: 0.9954, domain acc: 0.5543, lr: 0.000159, lamb: 1.9853, time: 9.5742\n","epoch 1911: train D loss: 0.6855, train F loss: -1.3422, train acc: 0.9952, domain acc: 0.5552, lr: 0.000159, lamb: 1.9854, time: 10.0539\n","epoch 1912: train D loss: 0.6873, train F loss: -1.3468, train acc: 0.9954, domain acc: 0.5556, lr: 0.000159, lamb: 1.9856, time: 9.5335\n","epoch 1913: train D loss: 0.6851, train F loss: -1.3394, train acc: 0.9942, domain acc: 0.5617, lr: 0.000159, lamb: 1.9857, time: 9.3142\n","epoch 1914: train D loss: 0.6868, train F loss: -1.3479, train acc: 0.9960, domain acc: 0.5468, lr: 0.000159, lamb: 1.9859, time: 9.6474\n","epoch 1915: train D loss: 0.6863, train F loss: -1.3477, train acc: 0.9958, domain acc: 0.5500, lr: 0.000159, lamb: 1.9860, time: 9.3989\n","epoch 1916: train D loss: 0.6857, train F loss: -1.3414, train acc: 0.9944, domain acc: 0.5558, lr: 0.000159, lamb: 1.9861, time: 10.1256\n","epoch 1917: train D loss: 0.6860, train F loss: -1.3438, train acc: 0.9946, domain acc: 0.5537, lr: 0.000158, lamb: 1.9863, time: 9.2935\n","epoch 1918: train D loss: 0.6852, train F loss: -1.3296, train acc: 0.9948, domain acc: 0.5586, lr: 0.000158, lamb: 1.9864, time: 9.4117\n","epoch 1919: train D loss: 0.6857, train F loss: -1.3432, train acc: 0.9946, domain acc: 0.5508, lr: 0.000158, lamb: 1.9865, time: 9.3031\n","epoch 1920: train D loss: 0.6852, train F loss: -1.3442, train acc: 0.9944, domain acc: 0.5562, lr: 0.000158, lamb: 1.9867, time: 9.2039\n","epoch 1921: train D loss: 0.6843, train F loss: -1.3430, train acc: 0.9958, domain acc: 0.5567, lr: 0.000158, lamb: 1.9868, time: 9.7817\n","epoch 1922: train D loss: 0.6850, train F loss: -1.3454, train acc: 0.9958, domain acc: 0.5553, lr: 0.000158, lamb: 1.9870, time: 9.6577\n","epoch 1923: train D loss: 0.6850, train F loss: -1.3479, train acc: 0.9964, domain acc: 0.5583, lr: 0.000157, lamb: 1.9871, time: 9.5805\n","epoch 1924: train D loss: 0.6858, train F loss: -1.3476, train acc: 0.9960, domain acc: 0.5547, lr: 0.000157, lamb: 1.9872, time: 9.6336\n","epoch 1925: train D loss: 0.6877, train F loss: -1.3502, train acc: 0.9950, domain acc: 0.5413, lr: 0.000157, lamb: 1.9874, time: 9.6867\n","epoch 1926: train D loss: 0.6847, train F loss: -1.3473, train acc: 0.9966, domain acc: 0.5517, lr: 0.000157, lamb: 1.9875, time: 9.6681\n","epoch 1927: train D loss: 0.6867, train F loss: -1.3492, train acc: 0.9948, domain acc: 0.5477, lr: 0.000157, lamb: 1.9876, time: 9.6009\n","epoch 1928: train D loss: 0.6857, train F loss: -1.3494, train acc: 0.9958, domain acc: 0.5575, lr: 0.000157, lamb: 1.9878, time: 9.5732\n","epoch 1929: train D loss: 0.6869, train F loss: -1.3462, train acc: 0.9950, domain acc: 0.5511, lr: 0.000156, lamb: 1.9879, time: 9.4929\n","epoch 1930: train D loss: 0.6883, train F loss: -1.3517, train acc: 0.9964, domain acc: 0.5444, lr: 0.000156, lamb: 1.9880, time: 9.6090\n","epoch 1931: train D loss: 0.6866, train F loss: -1.3499, train acc: 0.9962, domain acc: 0.5528, lr: 0.000156, lamb: 1.9882, time: 9.5045\n","epoch 1932: train D loss: 0.6893, train F loss: -1.3545, train acc: 0.9952, domain acc: 0.5465, lr: 0.000156, lamb: 1.9883, time: 9.2375\n","epoch 1933: train D loss: 0.6863, train F loss: -1.3464, train acc: 0.9950, domain acc: 0.5521, lr: 0.000156, lamb: 1.9885, time: 9.6475\n","epoch 1934: train D loss: 0.6878, train F loss: -1.3507, train acc: 0.9948, domain acc: 0.5456, lr: 0.000156, lamb: 1.9886, time: 9.1856\n","epoch 1935: train D loss: 0.6877, train F loss: -1.3498, train acc: 0.9960, domain acc: 0.5446, lr: 0.000156, lamb: 1.9887, time: 9.5908\n","epoch 1936: train D loss: 0.6856, train F loss: -1.3449, train acc: 0.9952, domain acc: 0.5548, lr: 0.000155, lamb: 1.9889, time: 9.2666\n","epoch 1937: train D loss: 0.6874, train F loss: -1.3513, train acc: 0.9962, domain acc: 0.5480, lr: 0.000155, lamb: 1.9890, time: 9.3356\n","epoch 1938: train D loss: 0.6869, train F loss: -1.3515, train acc: 0.9962, domain acc: 0.5551, lr: 0.000155, lamb: 1.9891, time: 9.4591\n","epoch 1939: train D loss: 0.6876, train F loss: -1.3541, train acc: 0.9970, domain acc: 0.5503, lr: 0.000155, lamb: 1.9893, time: 9.1613\n","epoch 1940: train D loss: 0.6883, train F loss: -1.3537, train acc: 0.9958, domain acc: 0.5443, lr: 0.000155, lamb: 1.9894, time: 9.5849\n","epoch 1941: train D loss: 0.6872, train F loss: -1.3500, train acc: 0.9956, domain acc: 0.5475, lr: 0.000155, lamb: 1.9895, time: 9.1280\n","epoch 1942: train D loss: 0.6882, train F loss: -1.3529, train acc: 0.9958, domain acc: 0.5481, lr: 0.000154, lamb: 1.9897, time: 9.5295\n","epoch 1943: train D loss: 0.6859, train F loss: -1.3511, train acc: 0.9964, domain acc: 0.5554, lr: 0.000154, lamb: 1.9898, time: 9.4577\n","epoch 1944: train D loss: 0.6872, train F loss: -1.3508, train acc: 0.9954, domain acc: 0.5528, lr: 0.000154, lamb: 1.9899, time: 9.3688\n","epoch 1945: train D loss: 0.6872, train F loss: -1.3517, train acc: 0.9958, domain acc: 0.5520, lr: 0.000154, lamb: 1.9901, time: 9.5152\n","epoch 1946: train D loss: 0.6890, train F loss: -1.3571, train acc: 0.9960, domain acc: 0.5374, lr: 0.000154, lamb: 1.9902, time: 9.2033\n","epoch 1947: train D loss: 0.6864, train F loss: -1.3511, train acc: 0.9962, domain acc: 0.5492, lr: 0.000154, lamb: 1.9904, time: 9.5552\n","epoch 1948: train D loss: 0.6851, train F loss: -1.3451, train acc: 0.9954, domain acc: 0.5582, lr: 0.000154, lamb: 1.9905, time: 9.0993\n","epoch 1949: train D loss: 0.6875, train F loss: -1.3542, train acc: 0.9960, domain acc: 0.5454, lr: 0.000153, lamb: 1.9906, time: 9.8738\n","epoch 1950: train D loss: 0.6874, train F loss: -1.3515, train acc: 0.9948, domain acc: 0.5507, lr: 0.000153, lamb: 1.9908, time: 9.3140\n","epoch 1951: train D loss: 0.6866, train F loss: -1.3512, train acc: 0.9954, domain acc: 0.5528, lr: 0.000153, lamb: 1.9909, time: 9.1988\n","epoch 1952: train D loss: 0.6865, train F loss: -1.3512, train acc: 0.9960, domain acc: 0.5553, lr: 0.000153, lamb: 1.9910, time: 9.1673\n","epoch 1953: train D loss: 0.6893, train F loss: -1.3514, train acc: 0.9940, domain acc: 0.5409, lr: 0.000153, lamb: 1.9912, time: 9.3978\n","epoch 1954: train D loss: 0.6880, train F loss: -1.3523, train acc: 0.9954, domain acc: 0.5411, lr: 0.000153, lamb: 1.9913, time: 9.2383\n","epoch 1955: train D loss: 0.6860, train F loss: -1.3519, train acc: 0.9966, domain acc: 0.5549, lr: 0.000152, lamb: 1.9914, time: 9.2061\n","epoch 1956: train D loss: 0.6864, train F loss: -1.3550, train acc: 0.9966, domain acc: 0.5559, lr: 0.000152, lamb: 1.9916, time: 9.2094\n","epoch 1957: train D loss: 0.6885, train F loss: -1.3500, train acc: 0.9948, domain acc: 0.5436, lr: 0.000152, lamb: 1.9917, time: 9.1522\n","epoch 1958: train D loss: 0.6871, train F loss: -1.3524, train acc: 0.9946, domain acc: 0.5490, lr: 0.000152, lamb: 1.9918, time: 9.2871\n","epoch 1959: train D loss: 0.6878, train F loss: -1.3549, train acc: 0.9956, domain acc: 0.5473, lr: 0.000152, lamb: 1.9920, time: 9.1977\n","epoch 1960: train D loss: 0.6865, train F loss: -1.3519, train acc: 0.9956, domain acc: 0.5529, lr: 0.000152, lamb: 1.9921, time: 9.3056\n","epoch 1961: train D loss: 0.6869, train F loss: -1.3504, train acc: 0.9954, domain acc: 0.5517, lr: 0.000152, lamb: 1.9922, time: 9.4401\n","epoch 1962: train D loss: 0.6859, train F loss: -1.3503, train acc: 0.9958, domain acc: 0.5535, lr: 0.000151, lamb: 1.9924, time: 9.2901\n","epoch 1963: train D loss: 0.6853, train F loss: -1.3446, train acc: 0.9948, domain acc: 0.5555, lr: 0.000151, lamb: 1.9925, time: 9.1511\n","epoch 1964: train D loss: 0.6873, train F loss: -1.3503, train acc: 0.9954, domain acc: 0.5562, lr: 0.000151, lamb: 1.9926, time: 9.2489\n","epoch 1965: train D loss: 0.6836, train F loss: -1.3459, train acc: 0.9956, domain acc: 0.5594, lr: 0.000151, lamb: 1.9928, time: 9.2069\n","epoch 1966: train D loss: 0.6859, train F loss: -1.3501, train acc: 0.9960, domain acc: 0.5515, lr: 0.000151, lamb: 1.9929, time: 9.3604\n","epoch 1967: train D loss: 0.6860, train F loss: -1.3521, train acc: 0.9956, domain acc: 0.5569, lr: 0.000151, lamb: 1.9930, time: 9.2539\n","epoch 1968: train D loss: 0.6874, train F loss: -1.3556, train acc: 0.9958, domain acc: 0.5450, lr: 0.000150, lamb: 1.9932, time: 9.1884\n","epoch 1969: train D loss: 0.6868, train F loss: -1.3488, train acc: 0.9950, domain acc: 0.5503, lr: 0.000150, lamb: 1.9933, time: 9.2703\n","epoch 1970: train D loss: 0.6877, train F loss: -1.3532, train acc: 0.9956, domain acc: 0.5415, lr: 0.000150, lamb: 1.9934, time: 9.3713\n","epoch 1971: train D loss: 0.6873, train F loss: -1.3542, train acc: 0.9958, domain acc: 0.5451, lr: 0.000150, lamb: 1.9936, time: 9.3775\n","epoch 1972: train D loss: 0.6861, train F loss: -1.3519, train acc: 0.9958, domain acc: 0.5517, lr: 0.000150, lamb: 1.9937, time: 9.3656\n","epoch 1973: train D loss: 0.6860, train F loss: -1.3520, train acc: 0.9962, domain acc: 0.5560, lr: 0.000150, lamb: 1.9938, time: 9.5006\n","epoch 1974: train D loss: 0.6849, train F loss: -1.3457, train acc: 0.9946, domain acc: 0.5568, lr: 0.000150, lamb: 1.9940, time: 9.3212\n","epoch 1975: train D loss: 0.6862, train F loss: -1.3506, train acc: 0.9956, domain acc: 0.5602, lr: 0.000149, lamb: 1.9941, time: 9.4210\n","epoch 1976: train D loss: 0.6859, train F loss: -1.3521, train acc: 0.9964, domain acc: 0.5474, lr: 0.000149, lamb: 1.9942, time: 9.4358\n","epoch 1977: train D loss: 0.6872, train F loss: -1.3497, train acc: 0.9954, domain acc: 0.5484, lr: 0.000149, lamb: 1.9944, time: 9.5268\n","epoch 1978: train D loss: 0.6869, train F loss: -1.3524, train acc: 0.9946, domain acc: 0.5508, lr: 0.000149, lamb: 1.9945, time: 9.3016\n","epoch 1979: train D loss: 0.6855, train F loss: -1.3463, train acc: 0.9942, domain acc: 0.5473, lr: 0.000149, lamb: 1.9946, time: 9.6139\n","epoch 1980: train D loss: 0.6867, train F loss: -1.3534, train acc: 0.9948, domain acc: 0.5473, lr: 0.000149, lamb: 1.9948, time: 9.7418\n","epoch 1981: train D loss: 0.6863, train F loss: -1.3564, train acc: 0.9968, domain acc: 0.5520, lr: 0.000149, lamb: 1.9949, time: 9.4746\n","epoch 1982: train D loss: 0.6862, train F loss: -1.3502, train acc: 0.9956, domain acc: 0.5497, lr: 0.000148, lamb: 1.9950, time: 9.7925\n","epoch 1983: train D loss: 0.6864, train F loss: -1.3546, train acc: 0.9960, domain acc: 0.5525, lr: 0.000148, lamb: 1.9952, time: 9.5674\n","epoch 1984: train D loss: 0.6862, train F loss: -1.3555, train acc: 0.9956, domain acc: 0.5562, lr: 0.000148, lamb: 1.9953, time: 9.6330\n","epoch 1985: train D loss: 0.6863, train F loss: -1.3541, train acc: 0.9960, domain acc: 0.5483, lr: 0.000148, lamb: 1.9954, time: 9.3943\n","epoch 1986: train D loss: 0.6868, train F loss: -1.3547, train acc: 0.9964, domain acc: 0.5525, lr: 0.000148, lamb: 1.9956, time: 9.4937\n","epoch 1987: train D loss: 0.6855, train F loss: -1.3528, train acc: 0.9952, domain acc: 0.5583, lr: 0.000148, lamb: 1.9957, time: 9.5238\n","epoch 1988: train D loss: 0.6872, train F loss: -1.3513, train acc: 0.9954, domain acc: 0.5505, lr: 0.000147, lamb: 1.9958, time: 9.7613\n","epoch 1989: train D loss: 0.6864, train F loss: -1.3525, train acc: 0.9958, domain acc: 0.5526, lr: 0.000147, lamb: 1.9960, time: 9.9640\n","epoch 1990: train D loss: 0.6842, train F loss: -1.3525, train acc: 0.9968, domain acc: 0.5509, lr: 0.000147, lamb: 1.9961, time: 9.7946\n","epoch 1991: train D loss: 0.6875, train F loss: -1.3588, train acc: 0.9972, domain acc: 0.5424, lr: 0.000147, lamb: 1.9962, time: 9.9219\n","epoch 1992: train D loss: 0.6850, train F loss: -1.3504, train acc: 0.9948, domain acc: 0.5548, lr: 0.000147, lamb: 1.9964, time: 9.8061\n","epoch 1993: train D loss: 0.6867, train F loss: -1.3560, train acc: 0.9962, domain acc: 0.5509, lr: 0.000147, lamb: 1.9965, time: 9.5255\n","epoch 1994: train D loss: 0.6853, train F loss: -1.3511, train acc: 0.9950, domain acc: 0.5570, lr: 0.000147, lamb: 1.9966, time: 9.6154\n","epoch 1995: train D loss: 0.6843, train F loss: -1.3438, train acc: 0.9950, domain acc: 0.5568, lr: 0.000146, lamb: 1.9968, time: 9.5113\n","epoch 1996: train D loss: 0.6869, train F loss: -1.3538, train acc: 0.9946, domain acc: 0.5497, lr: 0.000146, lamb: 1.9969, time: 9.5999\n","epoch 1997: train D loss: 0.6880, train F loss: -1.3587, train acc: 0.9968, domain acc: 0.5417, lr: 0.000146, lamb: 1.9970, time: 9.5158\n","epoch 1998: train D loss: 0.6882, train F loss: -1.3522, train acc: 0.9938, domain acc: 0.5425, lr: 0.000146, lamb: 1.9971, time: 9.4661\n","epoch 1999: train D loss: 0.6870, train F loss: -1.3565, train acc: 0.9962, domain acc: 0.5476, lr: 0.000146, lamb: 1.9973, time: 9.5808\n"]}],"source":["def train_epoch(source_dataloader, target_dataloader, lamb ,lr):\n","    '''\n","      Args:\n","        source_dataloader: source data的dataloader\n","        target_dataloader: target data的dataloader\n","        lamb: control the balance of domain adaptatoin and classification.\n","    '''\n","\n","    # D loss: Domain Classifier的loss\n","    # F loss: Feature Extrator & Label Predictor的loss\n","    running_D_loss, running_F_loss = 0.0, 0.0\n","    total_hit, total_num = 0.0, 0.0\n","    domain_hit, target_num = 0.0, 0.0    \n","\n","    for i, ((source_data, source_label), (target_data, _)) in enumerate(zip(source_dataloader, target_dataloader)):\n","\n","        source_data = source_data.cuda()\n","        source_label = source_label.cuda()\n","        target_data = target_data.cuda()\n","        \n","        # Mixed the source data and target data, or it'll mislead the running params\n","        #   of batch_norm. (runnning mean/var of soucre and target data are different.)\n","        mixed_data = torch.cat([source_data, target_data], dim=0)\n","        domain_label = torch.zeros([source_data.shape[0] + target_data.shape[0], 1]).cuda()\n","        # set domain label of source data to be 1.\n","        domain_label[:source_data.shape[0]] = 1\n","\n","        # Step 1 : train domain classifier\n","        feature = feature_extractor(mixed_data)\n","        # We don't need to train feature extractor in step 1.\n","        # Thus we detach the feature neuron to avoid backpropgation.\n","        domain_logits = domain_classifier(feature.detach())\n","        loss = domain_criterion(domain_logits, domain_label)\n","        running_D_loss+= loss.item()\n","        loss.backward()\n","        optimizer_D.step()\n","\n","        # Step 2 : train feature extractor and label classifier\n","        class_logits = label_predictor(feature[:source_data.shape[0]])\n","        domain_logits = domain_classifier(feature)\n","        # loss = cross entropy of classification - lamb * domain binary cross entropy.\n","        #  The reason why using subtraction is similar to generator loss in disciminator of GAN\n","        loss = class_criterion(class_logits, source_label) - lamb * domain_criterion(domain_logits, domain_label)\n","        running_F_loss+= loss.item()\n","        loss.backward()\n","        optimizer_F.step()\n","        optimizer_C.step()\n","\n","        optimizer_D.zero_grad()\n","        optimizer_F.zero_grad()\n","        optimizer_C.zero_grad()\n","        \n","        optimizer_D.param_groups[0]['lr'] = lr\n","        optimizer_F.param_groups[0]['lr'] = lr\n","        optimizer_C.param_groups[0]['lr'] = lr\n","        \n","        total_hit += torch.sum(torch.argmax(class_logits, dim=1) == source_label).item()\n","        domain_outs = (torch.sigmoid(domain_logits) > 0.5).float()\n","        domain_hit += torch.sum(domain_outs == domain_label).item()\n","\n","        total_num += source_data.shape[0]\n","        target_num += mixed_data.shape[0]\n","        #print(class_logits.shape,source_label.shape,source_data.shape)        \n","        #print(domain_outs.shape,domain_label.shape,mixed_data.shape)\n","        #print(domain_outs)\n","        #print(domain_label)\n","\n","        #print(i, end='\\r')\n","\n","    return running_D_loss / (i+1), running_F_loss / (i+1), total_hit / total_num ,domain_hit/target_num\n","\n","# train 1000 epochs\n","import time\n","import math\n","lr = learning_rate\n","beta =3*1e-3\n","lamb = 1e-2\n","record = []\n","# adaptive lamb\n","for epoch in range(2000):\n","    # You should chooose lamnda cleverly.\n","    start = time.time()\n","    train_D_loss, train_F_loss, train_acc ,domain_acc= train_epoch(source_dataloader, target_dataloader,\n","                                                        lamb=lamb,lr=lr)\n","    last_time = time.time() - start\n","    if train_acc > 0.95:\n","        lr = lr * 0.999\n","    else:\n","        lr = lr * 1\n","    #lamb = 2 - (1/((epoch+1)/100 - (1/(lamb_init-2))))\n","    if epoch>1:\n","        # lamb = math.log(pow(epoch,10),2)/49.8\n","        lamb = math.log(pow(epoch,10),2)/54.9\n","        # lamb = math.log(pow(epoch,10),2)/61.5\n","    if ((epoch+1) % 10) == 0 :\n","        torch.save(feature_extractor.state_dict(), f'extractor_model.bin')\n","        torch.save(label_predictor.state_dict(), f'predictor_model.bin')\n","    if ((epoch+1) % 50) == 0 :\n","        torch.save(feature_extractor.state_dict(), f'extractor_model'+str(epoch+1)+'.bin')\n","        torch.save(label_predictor.state_dict(), f'predictor_model'+str(epoch+1)+'.bin')\n","    record.append([train_D_loss,train_F_loss,train_acc,domain_acc,lamb,lr])    \n","    print('epoch {:>3d}: train D loss: {:6.4f}, train F loss: {:6.4f}, train acc: {:6.4f}, domain acc: {:6.4f}, lr: {:.6f}, lamb: {:6.4f}, time: {:6.4f}'\n","          .format(epoch,train_D_loss,train_F_loss,train_acc,domain_acc,lr,lamb,last_time))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"o8_-0iSSje4w"},"source":["# Inference\n","\n","就跟前幾次作業一樣。這裡我使用pd來生產csv，因為看起來比較潮(?)\n","\n","此外，200 epochs的Accuracy可能會不太穩定，可以多丟幾次或train久一點。"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-06-03T06:48:34.219993Z","iopub.status.idle":"2023-06-03T06:48:34.220485Z","shell.execute_reply":"2023-06-03T06:48:34.220252Z","shell.execute_reply.started":"2023-06-03T06:48:34.220231Z"},"id":"Wly5AgH2jePv","trusted":true},"outputs":[],"source":["result = []\n","label_predictor.load_state_dict(torch.load(\"C:\\code\\pytorch\\predictor_model.bin\"))\n","feature_extractor.load_state_dict(torch.load(\"C:\\code\\pytorch\\extractor_model.bin\"))\n","\n","label_predictor.eval()\n","feature_extractor.eval()\n","for i, (test_data, _) in enumerate(test_dataloader):\n","    test_data = test_data.cuda()\n","\n","    class_logits = label_predictor(feature_extractor(test_data))\n","\n","    x = torch.argmax(class_logits, dim=1).cpu().detach().numpy()\n","    result.append(x)\n","\n","import pandas as pd\n","result = np.concatenate(result)\n","\n","# Generate your submission\n","df = pd.DataFrame({'id': np.arange(0,len(result)), 'label': result})\n","df.to_csv('DaNN_submission3.csv',index=False)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ZtXnEMUNCE78"},"source":["# Training Statistics\n","\n","- Number of parameters:\n","  - Feature Extractor: 2, 142, 336\n","  - Label Predictor: 530, 442\n","  - Domain Classifier: 1, 055, 233\n","\n","- Simple\n"," - Training time on colab: ~ 1 hr\n","- Medium\n"," - Training time on colab: 2 ~ 4 hr\n","- Strong\n"," - Training time on colab: 5 ~ 6 hrs\n","- Boss\n"," - **Unmeasurable**"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"duk7k43Am9xH"},"source":["# Learning Curve (Strong Baseline)\n","* This method is slightly different from colab.\n","\n","![Loss Curve](https://i.imgur.com/vIujQyo.png)\n","\n","# Accuracy Curve (Strong Baseline)\n","* Note that you cannot access testing accuracy. But this plot tells you that even though the model overfits the training data, the testing accuracy is still improving, and that's why you need to train more epochs.\n","\n","![Acc Curve](https://i.imgur.com/4W1otXG.png)\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"s6UfXzef-wNl"},"source":["# Special Thanks\n","下面是原本台大助教提供的參考作業。\n","\n","[NTU_r08942071_太神啦 / 組長: 劉正仁同學](https://drive.google.com/open?id=11uNDcz7_eMS8dMQxvnWsbrdguu9k4c-c)\n","\n","[NTU_r08921a08_CAT / 組長: 廖子毅同學](https://drive.google.com/open?id=1xIkSs8HAShdcfV1E0NEnf4JDbL7POZTf)\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.0"}},"nbformat":4,"nbformat_minor":4}
