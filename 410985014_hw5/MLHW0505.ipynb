{"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"hw7_macbert_post-proces","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"widgets":{"application/vnd.jupyter.widget-state+json":{"10d0f6c2ce434ca39368ee84a74cb0c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"156089d6b4154cf5b7e3e5ab57c6ca71":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"1c1fde95b0514c2386d23562c6e74111":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"1de3e29e090a472f99045df4db561910":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fd84584280eb48edb6c95d7af687778a","IPY_MODEL_2dfa673b5c30405b89ff2939b7de2c52"],"layout":"IPY_MODEL_f6eb141d6ffe4b8aaac8e9f76759ef26"}},"1df37e6af4184e478d28bc1e426997c3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"100%","description_tooltip":null,"layout":"IPY_MODEL_7dea96acd33c4f1f96fdc33a35610903","max":3367,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e3ac97577b054f9cb424cea593ce1d06","value":3367}},"22912102b8d64c58a811698e46466034":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5f56fdb8cbf649a8bfc5488a52e1875b","IPY_MODEL_cf4f7857c8e547ff92922962b5d8c72d"],"layout":"IPY_MODEL_6e506deb5f594ae59c017e6af2174653"}},"26af0fe4453c40988cb1223ade37b667":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"298e48e35f2549a78a9c913cef621f71":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"29ad889789254729adaf624fe1436e83":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_46e9e1b2e88c467f8cec4d943ed59f63","placeholder":"​","style":"IPY_MODEL_3cfbd85119d24c2980598a388a48ffe1","value":" 3524/3524 [01:59&lt;00:00, 29.59it/s]"}},"2dfa673b5c30405b89ff2939b7de2c52":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_32634ac4777f44c989a6f89277b2ac2d","placeholder":"​","style":"IPY_MODEL_90beb800a68f43b8acb5c8e778159da4","value":" 3524/3524 [01:58&lt;00:00, 29.64it/s]"}},"32634ac4777f44c989a6f89277b2ac2d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"37e63fdf92f84758bedb1c0553048576":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3cfbd85119d24c2980598a388a48ffe1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"43e30460face4b9585e3f4d5e0a83d12":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"46e9e1b2e88c467f8cec4d943ed59f63":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"49395392dd304204b9fd9d390edc0831":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"561daaeffedc4279aa82de07c32d57dc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_49395392dd304204b9fd9d390edc0831","placeholder":"​","style":"IPY_MODEL_37e63fdf92f84758bedb1c0553048576","value":" 3524/3524 [01:59&lt;00:00, 29.40it/s]"}},"5f56fdb8cbf649a8bfc5488a52e1875b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"100%","description_tooltip":null,"layout":"IPY_MODEL_97d4d92a577f460e81090b812679e877","max":3367,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b711afd927b54033bb026b6526325162","value":3367}},"61a8295903b3444098f007b97a29eead":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"69cea7d8b8f0469aa2d25564a1f8e2c9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"6d2d8b2f9926414ca34a48b2fd2964a6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"100%","description_tooltip":null,"layout":"IPY_MODEL_26af0fe4453c40988cb1223ade37b667","max":3367,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9f094c22470d48eb892ee90ca667e4bf","value":3367}},"6e506deb5f594ae59c017e6af2174653":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e871b8843934b13adeea4bb39385e24":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"100%","description_tooltip":null,"layout":"IPY_MODEL_957ad0fef1584ff4829095987f93911e","max":3493,"min":0,"orientation":"horizontal","style":"IPY_MODEL_61a8295903b3444098f007b97a29eead","value":3493}},"720949528cb9469d95994085b6aca532":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7361adfa6d1447b0ac0f5235289da59b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"746ec3f80a7d48cbba6a96381e4d142b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7d9721459b884f03a12212dd8a49c89c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_298e48e35f2549a78a9c913cef621f71","placeholder":"​","style":"IPY_MODEL_e28a991943ba490ab34d2b4bf6530d41","value":" 3367/3367 [16:25&lt;00:00,  3.42it/s]"}},"7dea96acd33c4f1f96fdc33a35610903":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f48e3798adf463894c797836393fa2a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"877c29b1c39f4573bd98e567633c1781":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"891e658421564a15acec55785f2348c7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"894a1cd4b531407d8eb8b791d954e041":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"100%","description_tooltip":null,"layout":"IPY_MODEL_e66e977492df469a988c79682038d043","max":3524,"min":0,"orientation":"horizontal","style":"IPY_MODEL_156089d6b4154cf5b7e3e5ab57c6ca71","value":3524}},"90beb800a68f43b8acb5c8e778159da4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"937562a8d36b4ae995d5ff2f9dc4eb4e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"957ad0fef1584ff4829095987f93911e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"962b5412f044440f87727b9654de4676":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_894a1cd4b531407d8eb8b791d954e041","IPY_MODEL_561daaeffedc4279aa82de07c32d57dc"],"layout":"IPY_MODEL_dd1f06dd97f5430c8338ebfd34778df5"}},"97d4d92a577f460e81090b812679e877":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9b68888a7b194083a274b90204983924":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ba2c0d1913bb44b495e58d68552b3b17","IPY_MODEL_29ad889789254729adaf624fe1436e83"],"layout":"IPY_MODEL_746ec3f80a7d48cbba6a96381e4d142b"}},"9d990d0b19984b6ea05fad011f12a268":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6d2d8b2f9926414ca34a48b2fd2964a6","IPY_MODEL_cc5df39b097f45c395e161a2f45d9774"],"layout":"IPY_MODEL_e72d9562859c4086ae4e89fffdd88ea1"}},"9e0fb7ca24e74a2fbd9a886cb1241297":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1df37e6af4184e478d28bc1e426997c3","IPY_MODEL_7d9721459b884f03a12212dd8a49c89c"],"layout":"IPY_MODEL_877c29b1c39f4573bd98e567633c1781"}},"9f094c22470d48eb892ee90ca667e4bf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"b711afd927b54033bb026b6526325162":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"ba2c0d1913bb44b495e58d68552b3b17":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"100%","description_tooltip":null,"layout":"IPY_MODEL_43e30460face4b9585e3f4d5e0a83d12","max":3524,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1c1fde95b0514c2386d23562c6e74111","value":3524}},"bd6ab0e2579446aeb3649429358d2896":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c1fbaa249ff941f89afb8f0d1b53c3c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc5df39b097f45c395e161a2f45d9774":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_720949528cb9469d95994085b6aca532","placeholder":"​","style":"IPY_MODEL_bd6ab0e2579446aeb3649429358d2896","value":" 3367/3367 [16:25&lt;00:00,  3.42it/s]"}},"cf4f7857c8e547ff92922962b5d8c72d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_937562a8d36b4ae995d5ff2f9dc4eb4e","placeholder":"​","style":"IPY_MODEL_7361adfa6d1447b0ac0f5235289da59b","value":" 3367/3367 [16:25&lt;00:00,  3.42it/s]"}},"dd1f06dd97f5430c8338ebfd34778df5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e28a991943ba490ab34d2b4bf6530d41":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e3ac97577b054f9cb424cea593ce1d06":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"e3fe9293168642aa8117ebd2701b2111":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7f48e3798adf463894c797836393fa2a","placeholder":"​","style":"IPY_MODEL_891e658421564a15acec55785f2348c7","value":" 3493/3493 [01:58&lt;00:00, 29.55it/s]"}},"e66e977492df469a988c79682038d043":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e72d9562859c4086ae4e89fffdd88ea1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f6eb141d6ffe4b8aaac8e9f76759ef26":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb2ea040a1dc453cad50db782cadc1a2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6e871b8843934b13adeea4bb39385e24","IPY_MODEL_e3fe9293168642aa8117ebd2701b2111"],"layout":"IPY_MODEL_10d0f6c2ce434ca39368ee84a74cb0c4"}},"fd84584280eb48edb6c95d7af687778a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"100%","description_tooltip":null,"layout":"IPY_MODEL_c1fbaa249ff941f89afb8f0d1b53c3c8","max":3524,"min":0,"orientation":"horizontal","style":"IPY_MODEL_69cea7d8b8f0469aa2d25564a1f8e2c9","value":3524}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Homework 7 - Bert (Question Answering)**\n\nIf you have any questions, feel free to email us at ntu-ml-2021spring-ta@googlegroups.com\n\n\n\nSlide:    [Link](https://docs.google.com/presentation/d/1aQoWogAQo_xVJvMQMrGaYiWzuyfO0QyLLAhiMwFyS2w)　Kaggle: [Link](https://www.kaggle.com/c/ml2021-spring-hw7)　Data: [Link](https://drive.google.com/uc?id=1znKmX08v9Fygp-dgwo7BKiLIf2qL1FH1)\n\n\n","metadata":{"id":"xvSGDbExff_I"}},{"cell_type":"markdown","source":"## Task description\n- Chinese Extractive Question Answering\n  - Input: Paragraph + Question\n  - Output: Answer\n\n- Objective: Learn how to fine tune a pretrained model on downstream task using transformers\n\n- Todo\n    - Fine tune a pretrained chinese BERT model\n    - Change hyperparameters (e.g. doc_stride)\n    - Apply linear learning rate decay\n    - Try other pretrained models\n    - Improve preprocessing\n    - Improve postprocessing\n- Training tips\n    - Automatic mixed precision\n    - Gradient accumulation\n    - Ensemble\n\n- Estimated training time (tesla t4 with automatic mixed precision enabled)\n    - Simple: 8mins\n    - Medium: 8mins\n    - Strong: 25mins\n    - Boss: 2hrs\n  ","metadata":{"id":"WGOr_eS3wJJf"}},{"cell_type":"markdown","source":"## Download Dataset","metadata":{"id":"TJ1fSAJE2oaC"}},{"cell_type":"code","source":"!nvidia-smi","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kRT9I2f3-ewi","outputId":"3c58e04a-08f5-492e-9a96-8769764fb0c6","execution":{"iopub.status.busy":"2023-04-30T03:43:10.982030Z","iopub.execute_input":"2023-04-30T03:43:10.982498Z","iopub.status.idle":"2023-04-30T03:43:12.277154Z","shell.execute_reply.started":"2023-04-30T03:43:10.982468Z","shell.execute_reply":"2023-04-30T03:43:12.275397Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Sun Apr 30 03:43:12 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   47C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla T4            Off  | 00000000:00:05.0 Off |                    0 |\n| N/A   46C    P8    11W /  70W |      0MiB / 15109MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"# Download link 1\n#!gdown --id '1znKmX08v9Fygp-dgwo7BKiLIf2qL1FH1' --output hw7_data.zip\n\n# Download Link 2 (if the above link fails) \n# !gdown --id '1pOu3FdPdvzielUZyggeD7KDnVy9iW1uC' --output hw7_data.zip\n\n#!unzip -o hw7_data.zip\n\n# For this HW, K80 < P4 < T4 < P100 <= T4(fp16) < V100\n#!nvidia-smi","metadata":{"id":"YPrc4Eie9Yo5","execution":{"iopub.status.busy":"2023-04-30T03:43:12.287611Z","iopub.execute_input":"2023-04-30T03:43:12.292478Z","iopub.status.idle":"2023-04-30T03:43:12.302950Z","shell.execute_reply.started":"2023-04-30T03:43:12.292426Z","shell.execute_reply":"2023-04-30T03:43:12.301578Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Install transformers\n\nDocumentation for the toolkit:　https://huggingface.co/transformers/","metadata":{"id":"TevOvhC03m0h"}},{"cell_type":"code","source":"# You are allowed to change version of transformers or use other toolkits\n!pip install transformers==4.5.0","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tbxWFX_jpDom","outputId":"a1bd862f-c7dc-4356-823a-4601666c0df5","execution":{"iopub.status.busy":"2023-04-30T03:43:12.306839Z","iopub.execute_input":"2023-04-30T03:43:12.308556Z","iopub.status.idle":"2023-04-30T03:43:36.684911Z","shell.execute_reply.started":"2023-04-30T03:43:12.308507Z","shell.execute_reply":"2023-04-30T03:43:36.683693Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting transformers==4.5.0\n  Downloading transformers-4.5.0-py3-none-any.whl (2.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0) (2021.11.10)\nCollecting sacremoses\n  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0) (4.11.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0) (23.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0) (4.64.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0) (3.9.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0) (1.21.6)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0) (2.28.2)\nCollecting tokenizers<0.11,>=0.10.1\n  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.5.0) (4.4.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.5.0) (3.11.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.0) (1.26.14)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.0) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.0) (2022.12.7)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.0) (2.1.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.5.0) (1.16.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.5.0) (8.1.3)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.5.0) (1.2.0)\nBuilding wheels for collected packages: sacremoses\n  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=9e598960d320481e99867a854cfbaf0f748684c865fb179b9273072a48c73223\n  Stored in directory: /root/.cache/pip/wheels/5b/e0/77/05245143a5b31f65af6a21f7afd3219e9fa4896f918af45677\nSuccessfully built sacremoses\nInstalling collected packages: tokenizers, sacremoses, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.13.2\n    Uninstalling tokenizers-0.13.2:\n      Successfully uninstalled tokenizers-0.13.2\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.27.4\n    Uninstalling transformers-4.27.4:\n      Successfully uninstalled transformers-4.27.4\nSuccessfully installed sacremoses-0.0.53 tokenizers-0.10.3 transformers-4.5.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"## Import Packages","metadata":{"id":"8dKM4yCh4LI_"}},{"cell_type":"code","source":"import json\nimport numpy as np\nimport random\nimport torch\nfrom torch.utils.data import DataLoader, Dataset \nfrom transformers import AdamW, BertForQuestionAnswering, BertTokenizer, get_linear_schedule_with_warmup, BertModel, BertTokenizerFast\n\nfrom tqdm.auto import tqdm\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Fix random seed for reproducibility\ndef same_seeds(seed):\n\t  torch.manual_seed(seed)\n\t  if torch.cuda.is_available():\n\t\t    torch.cuda.manual_seed(seed)\n\t\t    torch.cuda.manual_seed_all(seed)\n\t  np.random.seed(seed)\n\t  random.seed(seed)\n\t  torch.backends.cudnn.benchmark = False\n\t  torch.backends.cudnn.deterministic = True\nsame_seeds(0)","metadata":{"id":"WOTHHtWJoahe","execution":{"iopub.status.busy":"2023-04-30T03:43:36.689134Z","iopub.execute_input":"2023-04-30T03:43:36.690939Z","iopub.status.idle":"2023-04-30T03:43:54.930241Z","shell.execute_reply.started":"2023-04-30T03:43:36.690891Z","shell.execute_reply":"2023-04-30T03:43:54.929072Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Change \"fp16_training\" to True to support automatic mixed precision training (fp16)\t\nfp16_training = True\n\nif fp16_training:\n    !pip install accelerate==0.2.0\n    from accelerate import Accelerator\n    accelerator = Accelerator(fp16=True)\n    device = accelerator.device\n\n# Documentation for the toolkit:  https://huggingface.co/docs/accelerate/","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7pBtSZP1SKQO","outputId":"c372f903-2989-4909-d56e-1aede9079c3d","execution":{"iopub.status.busy":"2023-04-30T03:43:54.932129Z","iopub.execute_input":"2023-04-30T03:43:54.932940Z","iopub.status.idle":"2023-04-30T03:44:04.909225Z","shell.execute_reply.started":"2023-04-30T03:43:54.932897Z","shell.execute_reply":"2023-04-30T03:44:04.908025Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Collecting accelerate==0.2.0\n  Downloading accelerate-0.2.0-py3-none-any.whl (47 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from accelerate==0.2.0) (1.13.0)\nRequirement already satisfied: pyaml>=20.4.0 in /opt/conda/lib/python3.7/site-packages (from accelerate==0.2.0) (21.10.1)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from pyaml>=20.4.0->accelerate==0.2.0) (6.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.4.0->accelerate==0.2.0) (4.4.0)\nInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.12.0\n    Uninstalling accelerate-0.12.0:\n      Successfully uninstalled accelerate-0.12.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncatalyst 22.4 requires accelerate>=0.5.1, but you have accelerate 0.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed accelerate-0.2.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"## Load Model and Tokenizer\n\n\n\n\n ","metadata":{"id":"2YgXHuVLp_6j"}},{"cell_type":"code","source":"tokenizer = BertTokenizerFast.from_pretrained(\"hfl/chinese-macbert-large\")\nmodel = BertForQuestionAnswering.from_pretrained(\"hfl/chinese-macbert-large\").to(device)\n\n# You can safely ignore the warning message (it pops up because new prediction heads for QA are initialized randomly)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xyBCYGjAp3ym","outputId":"da6b6561-a8f8-4405-e2bf-2355b5fd1ea2","execution":{"iopub.status.busy":"2023-04-30T03:44:04.911328Z","iopub.execute_input":"2023-04-30T03:44:04.911750Z","iopub.status.idle":"2023-04-30T03:44:48.949578Z","shell.execute_reply.started":"2023-04-30T03:44:04.911704Z","shell.execute_reply":"2023-04-30T03:44:48.948406Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/110k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f556d45aa8f4ddb8405b109e65068c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/269k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73bb59251b4f44569bb90bc06d05e0f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/2.00 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1a98044c23c4ba7bd51c64ec152562a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8b0a4fa102249919fa9164e51fc2cad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/19.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2571a89ca84b48c9b8f8675f08f2aa07"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/660 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf70776dde62447693e3a3fa8ced69db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.31G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98e48364740a44d0a32e267367cb8c7d"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at hfl/chinese-macbert-large were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForQuestionAnswering were not initialized from the model checkpoint at hfl/chinese-macbert-large and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Read Data\n\n- Training set: 26935 QA pairs\n- Dev set: 3523  QA pairs\n- Test set: 3492  QA pairs\n\n- {train/dev/test}_questions:\t\n  - List of dicts with the following keys:\n   - id (int)\n   - paragraph_id (int)\n   - question_text (string)\n   - answer_text (string)\n   - answer_start (int)\n   - answer_end (int)\n- {train/dev/test}_paragraphs: \n  - List of strings\n  - paragraph_ids in questions correspond to indexs in paragraphs\n  - A paragraph may be used by several questions ","metadata":{"id":"3Td-GTmk5OW4"}},{"cell_type":"code","source":"def read_data(file):\n    with open(file, 'r', encoding=\"utf-8\") as reader:\n        data = json.load(reader)\n    return data[\"questions\"], data[\"paragraphs\"]\n\ntrain_questions, train_paragraphs = read_data(\"/kaggle/input/hw5-data/hw7_train.json\")\ndev_questions, dev_paragraphs = read_data(\"/kaggle/input/hw5-data/hw7_dev.json\")\ntest_questions, test_paragraphs = read_data(\"/kaggle/input/hw5-data/hw7_test.json\")","metadata":{"id":"NvX7hlepogvu","execution":{"iopub.status.busy":"2023-04-30T03:44:48.951226Z","iopub.execute_input":"2023-04-30T03:44:48.951956Z","iopub.status.idle":"2023-04-30T03:44:49.785560Z","shell.execute_reply.started":"2023-04-30T03:44:48.951913Z","shell.execute_reply":"2023-04-30T03:44:49.784486Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"print(tokenizer.tokenize('✔', add_special_tokens=False))\nprint(tokenizer.tokenize('●', add_special_tokens=False))\nprint(tokenizer.tokenize('✦', add_special_tokens=False))\nprint(tokenizer.tokenize('☺', add_special_tokens=False))\nprint(tokenizer.tokenize('☆', add_special_tokens=False))\n\n# 取代' ' \\u200b \\u200e \\u3000 # 是為了讓tokenize前後index一致\n# 用✔ ● ✦ ☺ ☆ 當佔位符，沒有意義\n#train_paragraphs = [i.replace(' ','✔').replace('\\u200b','✦').replace('\\u200e', '☺').replace('\\u3000', '☆').replace('#','●') for i in train_paragraphs]\ndev_paragraphs = [i.replace(' ','✔').replace('\\u200b','✦').replace('\\u200e', '☺').replace('\\u3000', '☆').replace('#','●') for i in dev_paragraphs]\ntest_paragraphs = [i.replace(' ','✔').replace('\\u200b','✦').replace('\\u200e', '☺').replace('\\u3000', '☆').replace('#','●') for i in test_paragraphs]","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3FgKuiV3gnoe","outputId":"e11d66ce-78fd-475f-eb0a-4e01779482a4","execution":{"iopub.status.busy":"2023-04-30T03:44:49.790450Z","iopub.execute_input":"2023-04-30T03:44:49.792744Z","iopub.status.idle":"2023-04-30T03:44:49.823358Z","shell.execute_reply.started":"2023-04-30T03:44:49.792705Z","shell.execute_reply":"2023-04-30T03:44:49.822454Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"['✔']\n['●']\n['✦']\n['☺']\n['☆']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Tokenize Data","metadata":{"id":"Fm0rpTHq0e4N"}},{"cell_type":"code","source":"# Tokenize questions and paragraphs separately\n# 「add_special_tokens」 is set to False since special tokens will be added when tokenized questions and paragraphs are combined in datset __getitem__ \n\ntrain_questions_tokenized = tokenizer([train_question[\"question_text\"] for train_question in train_questions], add_special_tokens=False)\ndev_questions_tokenized = tokenizer([dev_question[\"question_text\"] for dev_question in dev_questions], add_special_tokens=False)\ntest_questions_tokenized = tokenizer([test_question[\"question_text\"] for test_question in test_questions], add_special_tokens=False) \n\ntrain_paragraphs_tokenized = tokenizer(train_paragraphs, add_special_tokens=False)\ndev_paragraphs_tokenized = tokenizer(dev_paragraphs, add_special_tokens=False)\ntest_paragraphs_tokenized = tokenizer(test_paragraphs, add_special_tokens=False)\n\n# You can safely ignore the warning message as tokenized sequences will be futher processed in datset __getitem__ before passing to model","metadata":{"id":"rTZ6B70Hoxie","execution":{"iopub.status.busy":"2023-04-30T03:44:49.827409Z","iopub.execute_input":"2023-04-30T03:44:49.829562Z","iopub.status.idle":"2023-04-30T03:45:05.398073Z","shell.execute_reply.started":"2023-04-30T03:44:49.829526Z","shell.execute_reply":"2023-04-30T03:45:05.396844Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Dataset and Dataloader","metadata":{"id":"Ws8c8_4d5UCI"}},{"cell_type":"code","source":"class QA_Dataset(Dataset):\n    def __init__(self, split, questions, tokenized_questions, tokenized_paragraphs):\n        self.split = split\n        self.questions = questions\n        self.tokenized_questions = tokenized_questions\n        self.tokenized_paragraphs = tokenized_paragraphs\n        self.max_question_len = 40\n        self.max_paragraph_len = 350\n        \n        ##### TODO: Change value of doc_stride #####\n        self.doc_stride = 300\n\n        # Input sequence length = [CLS] + question + [SEP] + paragraph + [SEP]\n        self.max_seq_len = 1 + self.max_question_len + 1 + self.max_paragraph_len + 1\n\n    def __len__(self):\n        return len(self.questions)\n\n    def __getitem__(self, idx):\n        question = self.questions[idx]\n        tokenized_question = self.tokenized_questions[idx]\n        tokenized_paragraph = self.tokenized_paragraphs[question[\"paragraph_id\"]]\n\n        ##### TODO: Preprocessing #####\n        # Hint: How to prevent model from learning something it should not learn\n\n        if self.split == \"train\":\n            # Convert answer's start/end positions in paragraph_text to start/end positions in tokenized_paragraph  \n            answer_start_token = tokenized_paragraph.char_to_token(question[\"answer_start\"])\n            answer_end_token = tokenized_paragraph.char_to_token(question[\"answer_end\"])\n\n            # A single window is obtained by slicing the portion of paragraph containing the answer\n            mid = int((answer_start_token + answer_end_token) // (2+random.uniform(-1,1)))\n            paragraph_start = max(0, min(mid - self.max_paragraph_len // 2, len(tokenized_paragraph) - self.max_paragraph_len))\n            #if answer_start_token >  self.max_paragraph_len:\n                # paragraph_start 不能從0開始\n            #    paragraph_start = min(answer_start_token - self.max_paragraph_len // 2, len(tokenized_paragraph) - self.max_paragraph_len)\n            #else:\n            #    paragraph_start = 0\n            paragraph_end = paragraph_start + self.max_paragraph_len\n            \n\n            # Slice question/paragraph and add special tokens (101: CLS, 102: SEP)\n            input_ids_question = [101] + tokenized_question.ids[:self.max_question_len] + [102] \n            input_ids_paragraph = tokenized_paragraph.ids[paragraph_start : paragraph_end] + [102]\t\t\n            \n            # Convert answer's start/end positions in tokenized_paragraph to start/end positions in the window  \n            answer_start_token += len(input_ids_question) - paragraph_start\n            answer_end_token += len(input_ids_question) - paragraph_start\n            \n            # Pad sequence and obtain inputs to model \n            input_ids, token_type_ids, attention_mask = self.padding(input_ids_question, input_ids_paragraph)\n            return torch.tensor(input_ids), torch.tensor(token_type_ids), torch.tensor(attention_mask), answer_start_token, answer_end_token\n\n        # Validation/Testing\n        else:\n            input_ids_list, token_type_ids_list, attention_mask_list = [], [], []\n            \n            # Paragraph is split into several windows, each with start positions separated by step \"doc_stride\"\n            for i in range(0, len(tokenized_paragraph), self.doc_stride):\n                \n                # Slice question/paragraph and add special tokens (101: CLS, 102: SEP)\n                input_ids_question = [101] + tokenized_question.ids[:self.max_question_len] + [102]\n                input_ids_paragraph = tokenized_paragraph.ids[i : i + self.max_paragraph_len] + [102]\n                \n                # Pad sequence and obtain inputs to model\n                input_ids, token_type_ids, attention_mask = self.padding(input_ids_question, input_ids_paragraph)\n                \n                input_ids_list.append(input_ids)\n                token_type_ids_list.append(token_type_ids)\n                attention_mask_list.append(attention_mask)\n            \n            return torch.tensor(input_ids_list), torch.tensor(token_type_ids_list), torch.tensor(attention_mask_list)\n\n    def padding(self, input_ids_question, input_ids_paragraph):\n        # Pad zeros if sequence length is shorter than max_seq_len\n        padding_len = self.max_seq_len - len(input_ids_question) - len(input_ids_paragraph)\n        # Indices of input sequence tokens in the vocabulary\n        input_ids = input_ids_question + input_ids_paragraph + [0] * padding_len\n        # Segment token indices to indicate first and second portions of the inputs. Indices are selected in [0, 1]\n        token_type_ids = [0] * len(input_ids_question) + [1] * len(input_ids_paragraph) + [0] * padding_len\n        # Mask to avoid performing attention on padding token indices. Mask values selected in [0, 1]\n        attention_mask = [1] * (len(input_ids_question) + len(input_ids_paragraph)) + [0] * padding_len\n        \n        return input_ids, token_type_ids, attention_mask\n\ntrain_set = QA_Dataset(\"train\", train_questions, train_questions_tokenized, train_paragraphs_tokenized)\ndev_set = QA_Dataset(\"dev\", dev_questions, dev_questions_tokenized, dev_paragraphs_tokenized)\ntest_set = QA_Dataset(\"test\", test_questions, test_questions_tokenized, test_paragraphs_tokenized)\n\ntrain_batch_size = 8\n\n# Note: Do NOT change batch size of dev_loader / test_loader !\n# Although batch size=1, it is actually a batch consisting of several windows from the same QA pair\ntrain_loader = DataLoader(train_set, batch_size=train_batch_size, shuffle=True, pin_memory=True)\ndev_loader = DataLoader(dev_set, batch_size=1, shuffle=False, pin_memory=True)\ntest_loader = DataLoader(test_set, batch_size=1, shuffle=False, pin_memory=True)","metadata":{"id":"Xjooag-Swnuh","execution":{"iopub.status.busy":"2023-04-30T03:45:05.402086Z","iopub.execute_input":"2023-04-30T03:45:05.402746Z","iopub.status.idle":"2023-04-30T03:45:05.425488Z","shell.execute_reply.started":"2023-04-30T03:45:05.402702Z","shell.execute_reply":"2023-04-30T03:45:05.424477Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Function for Evaluation","metadata":{"id":"5_H1kqhR8CdM"}},{"cell_type":"code","source":"def index_before_tokenize(tokens, start, end):\n    char_count, new_start, new_end = 0, 512, 512\n    start_flag = 0\n    end_flag = 0\n        \n    for i, token in enumerate(tokens):\n        if token == '[UNK]' or token == '[CLS]' or token == '[SEP]':\n            if i == start:\n                new_start = char_count\n            if i == end:\n                new_end = char_count\n            char_count += 1\n        else:\n            for c in token:\n                if i == start and start_flag == 0:\n                    #print(token)\n                    new_start = char_count\n                    start_flag = 1\n                if i == end:\n                    #print(token)\n                    new_end = char_count\n                    end_flag = 1\n                if c != '#':\n                    char_count += 1\n    return new_start, new_end","metadata":{"id":"nqAQYOsfhL5c","execution":{"iopub.status.busy":"2023-04-30T03:45:05.427200Z","iopub.execute_input":"2023-04-30T03:45:05.427736Z","iopub.status.idle":"2023-04-30T03:45:05.444587Z","shell.execute_reply.started":"2023-04-30T03:45:05.427697Z","shell.execute_reply":"2023-04-30T03:45:05.443501Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def evaluate(data, output, doc_stride=150, paragraph=None, paragraph_tokenized=None):\n    ##### TODO: Postprocessing #####\n    # There is a bug and room for improvement in postprocessing \n    # Hint: Open your prediction file to see what is wrong \n    \n    answer = ''\n    max_prob = float('-inf')\n    num_of_windows = data[0].shape[1]\n    \n    # index in the whole tokens (not just relative to window)\n    entire_start_index = 0\n    entire_end_index = 0\n    \n    for k in range(num_of_windows):\n        #print('window',k)\n        # Obtain answer by choosing the most probable start position / end position\n        mask = data[1][0][k].bool() &  data[2][0][k].bool() # token type & attention mask\n        masked_output_start = torch.masked_select(output.start_logits[k], mask)[:-1] # -1 is [SEP]\n        start_prob, start_index = torch.max(masked_output_start, dim=0)\n        masked_output_end = torch.masked_select(output.end_logits[k], mask)[start_index:-1] # -1 is [SEP]\n        #masked_output_end = torch.masked_select(output.end_logits[k], mask)[:-1] # -1 is [SEP]\n        end_prob, end_index = torch.max(masked_output_end, dim=0)\n        end_index += start_index \n        \n\n        # Probability of answer is calculated as sum of start_prob and end_prob\n        prob = start_prob + end_prob\n        masked_data = torch.masked_select(data[0][0][k], mask)[:-1] # -1 is [SEP]\n\n        # Replace answer if calculated probability is larger than previous windows\n        if (prob > max_prob) and (end_index - start_index <= 30) and (end_index > start_index):\n            max_prob = prob\n            entire_start_index = start_index.item() + doc_stride * k\n            entire_end_index = end_index.item() + doc_stride * k\n            #print('entire_start_index',entire_start_index)\n            #print('entire_end_index',entire_end_index)\n            # Convert tokens to chars (e.g. [1920, 7032] --> \"大 金\")\n            answer = tokenizer.decode(masked_data[start_index : end_index + 1])\n            # Remove spaces in answer (e.g. \"大 金\" --> \"大金\")\n            answer = answer.replace('✔', ' ').replace('✦','\\u200b').replace('☺','\\u200e').replace('☆','\\u3000').replace('●','#').replace(' ','')\n\n    \n    # if [UNK] in prediction, use orignal span of paragrah\n    if '[UNK]' in answer:\n        print('found [UNK] in prediction, using original text')\n        print('original prediction', answer)\n        # find the index of answer in the orinal paragrah\n\n        new_start, new_end = index_before_tokenize(tokens=paragraph_tokenized, \n                                                   start=entire_start_index, end=entire_end_index)\n        #print('new_start',new_start)\n        #print('new_end',new_end)\n        answer = paragraph[new_start:new_end+1]\n        answer = answer.replace('✔', ' ').replace('✦','\\u200b').replace('☺','\\u200e').replace('☆','\\u3000').replace('●','#')\n        print('final prediction',answer)\n\n            \n    \n    return answer","metadata":{"id":"SqeA3PLPxOHu","execution":{"iopub.status.busy":"2023-04-30T03:45:05.447342Z","iopub.execute_input":"2023-04-30T03:45:05.448240Z","iopub.status.idle":"2023-04-30T03:45:05.467795Z","shell.execute_reply.started":"2023-04-30T03:45:05.448201Z","shell.execute_reply":"2023-04-30T03:45:05.466677Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{"id":"rzHQit6eMnKG"}},{"cell_type":"code","source":"num_epoch = 3\nvalidation = True\nlogging_step = 100\nlearning_rate = 1e-5\ndoc_stride = 300\n# batch accumulation parameter\n# accum_iter = 4  \n\noptimizer = AdamW(model.parameters(), lr=learning_rate)\n\n\nif fp16_training:\n    model, optimizer, train_loader = accelerator.prepare(model, optimizer, train_loader) \n\n# Total number of training steps\ntotal_steps = len(train_loader) * num_epoch\nprint('total_steps', total_steps)\n# Set up the learning rate scheduler\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps= 0, # Default value\n                                                num_training_steps=total_steps)\n\nif not validation:\n    dev_set = QA_Dataset(\"train\", dev_questions, dev_questions_tokenized, dev_paragraphs_tokenized)\n    train_set = torch.utils.data.ConcatDataset([train_set, dev_set])\n    train_loader = DataLoader(train_set, batch_size=train_batch_size, shuffle=True, pin_memory=True)\n    \nmodel.train()\n\nprint(\"Start Training ...\")\n\nfor epoch in range(num_epoch):\n    step = 1\n    train_loss = train_acc = 0\n    \n    for idx, data in enumerate(tqdm(train_loader)):\t\n        # Load all data into GPU\n        data = [i.to(device) for i in data]\n        \n        # Model inputs: input_ids, token_type_ids, attention_mask, start_positions, end_positions (Note: only \"input_ids\" is mandatory)\n        # Model outputs: start_logits, end_logits, loss (return when start_positions/end_positions are provided)  \n        output = model(input_ids=data[0], token_type_ids=data[1], attention_mask=data[2], start_positions=data[3], end_positions=data[4])\n\n\n        # Choose the most probable start position / end position\n        start_index = torch.argmax(output.start_logits, dim=1)\n        end_index = torch.argmax(output.end_logits, dim=1)\n        \n        # Prediction is correct only if both start_index and end_index are correct\n        train_acc += ((start_index == data[3]) & (end_index == data[4])).float().mean()\n        train_loss += output.loss\n        \n        # normalize loss to account for batch accumulation\n        #output.loss = output.loss / accum_iter\n\n        if fp16_training:\n            accelerator.backward(output.loss)\n        else:\n            output.loss.backward()\n        \n        #if ((idx + 1) % accum_iter == 0) or (idx + 1 == len(train_loader)):\n        optimizer.step()\n        ##### TODO: Apply linear learning rate decay #####\n        scheduler.step()\n        optimizer.zero_grad()\n        \n        step += 1\n        \n        \n        # Print training loss and accuracy over past logging step\n        if step % logging_step == 0:\n            print(f\"Epoch {epoch + 1} | Step {step} | loss = {train_loss.item() / logging_step:.3f}, acc = {train_acc / logging_step:.3f}\")\n            train_loss = train_acc = 0\n\n    if validation:\n        print(\"Evaluating Dev Set ...\")\n        model.eval()\n        with torch.no_grad():\n            dev_acc = 0\n            for i, data in enumerate(tqdm(dev_loader)):\n                output = model(input_ids=data[0].squeeze(dim=0).to(device), token_type_ids=data[1].squeeze(dim=0).to(device),\n                       attention_mask=data[2].squeeze(dim=0).to(device))\n                # prediction is correct only if answer text exactly matches\n                dev_acc += evaluate(data, output, doc_stride, dev_paragraphs[dev_questions[i]['paragraph_id']],\n                    dev_paragraphs_tokenized[dev_questions[i]['paragraph_id']].tokens) == dev_questions[i][\"answer_text\"]\n            print(f\"Validation | Epoch {epoch + 1} | acc = {dev_acc / len(dev_loader):.3f}\")\n        model.train()\n\n# Save a model and its configuration file to the directory 「saved_model」 \n# i.e. there are two files under the direcory 「saved_model」: 「pytorch_model.bin」 and 「config.json」\n# Saved model can be re-loaded using 「model = BertForQuestionAnswering.from_pretrained(\"saved_model\")」\nprint(\"Saving Model ...\")\nmodel_save_dir = \"saved_model/macbert4\" \nmodel.save_pretrained(model_save_dir)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["22912102b8d64c58a811698e46466034","6e506deb5f594ae59c017e6af2174653","5f56fdb8cbf649a8bfc5488a52e1875b","cf4f7857c8e547ff92922962b5d8c72d","b711afd927b54033bb026b6526325162","97d4d92a577f460e81090b812679e877","7361adfa6d1447b0ac0f5235289da59b","937562a8d36b4ae995d5ff2f9dc4eb4e","1de3e29e090a472f99045df4db561910","f6eb141d6ffe4b8aaac8e9f76759ef26","fd84584280eb48edb6c95d7af687778a","2dfa673b5c30405b89ff2939b7de2c52","69cea7d8b8f0469aa2d25564a1f8e2c9","c1fbaa249ff941f89afb8f0d1b53c3c8","90beb800a68f43b8acb5c8e778159da4","32634ac4777f44c989a6f89277b2ac2d","9e0fb7ca24e74a2fbd9a886cb1241297","877c29b1c39f4573bd98e567633c1781","1df37e6af4184e478d28bc1e426997c3","7d9721459b884f03a12212dd8a49c89c","e3ac97577b054f9cb424cea593ce1d06","7dea96acd33c4f1f96fdc33a35610903","e28a991943ba490ab34d2b4bf6530d41","298e48e35f2549a78a9c913cef621f71","9b68888a7b194083a274b90204983924","746ec3f80a7d48cbba6a96381e4d142b","ba2c0d1913bb44b495e58d68552b3b17","29ad889789254729adaf624fe1436e83","1c1fde95b0514c2386d23562c6e74111","43e30460face4b9585e3f4d5e0a83d12","3cfbd85119d24c2980598a388a48ffe1","46e9e1b2e88c467f8cec4d943ed59f63","9d990d0b19984b6ea05fad011f12a268","e72d9562859c4086ae4e89fffdd88ea1","6d2d8b2f9926414ca34a48b2fd2964a6","cc5df39b097f45c395e161a2f45d9774","9f094c22470d48eb892ee90ca667e4bf","26af0fe4453c40988cb1223ade37b667","bd6ab0e2579446aeb3649429358d2896","720949528cb9469d95994085b6aca532","962b5412f044440f87727b9654de4676","dd1f06dd97f5430c8338ebfd34778df5","894a1cd4b531407d8eb8b791d954e041","561daaeffedc4279aa82de07c32d57dc","156089d6b4154cf5b7e3e5ab57c6ca71","e66e977492df469a988c79682038d043","37e63fdf92f84758bedb1c0553048576","49395392dd304204b9fd9d390edc0831"]},"id":"3Q-B6ka7xoCM","outputId":"f2df1eeb-815f-4d2d-a714-26c597a91781","execution":{"iopub.status.busy":"2023-04-30T03:45:05.469398Z","iopub.execute_input":"2023-04-30T03:45:05.470362Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"total_steps 9726\nStart Training ...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3242 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a639550ab3f4e2683d2065fe64f5220"}},"metadata":{}},{"name":"stdout","text":"Epoch 1 | Step 100 | loss = 2.958, acc = 0.246\nEpoch 1 | Step 200 | loss = 1.058, acc = 0.616\nEpoch 1 | Step 300 | loss = 0.788, acc = 0.684\nEpoch 1 | Step 400 | loss = 0.767, acc = 0.704\nEpoch 1 | Step 500 | loss = 0.713, acc = 0.717\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Testing","metadata":{"id":"kMmdLOKBMsdE"}},{"cell_type":"code","source":"print(\"Evaluating Test Set ...\")\n\nresult = []\n\nmodel.eval()\nwith torch.no_grad():\n    for i, data in enumerate(tqdm(test_loader)):\n        output = model(input_ids=data[0].squeeze(dim=0).to(device), token_type_ids=data[1].squeeze(dim=0).to(device),\n                       attention_mask=data[2].squeeze(dim=0).to(device))\n        result.append(evaluate(data, output, doc_stride, test_paragraphs[test_questions[i]['paragraph_id']],\n                               test_paragraphs_tokenized[test_questions[i]['paragraph_id']].tokens))\n\nresult_file = \"/content/gdrive/MyDrive/result_macbert4.csv\"\nwith open(result_file, 'w') as f:\t\n\t  f.write(\"ID,Answer\\n\")\n\t  for i, test_question in enumerate(test_questions):\n        # Replace commas in answers with empty strings (since csv is separated by comma)\n        # Answers in kaggle are processed in the same way\n\t\t    f.write(f\"{test_question['id']},{result[i].replace(',','')}\\n\")\n\nprint(f\"Completed! Result is in {result_file}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["fb2ea040a1dc453cad50db782cadc1a2","10d0f6c2ce434ca39368ee84a74cb0c4","6e871b8843934b13adeea4bb39385e24","e3fe9293168642aa8117ebd2701b2111","61a8295903b3444098f007b97a29eead","957ad0fef1584ff4829095987f93911e","891e658421564a15acec55785f2348c7","7f48e3798adf463894c797836393fa2a"]},"id":"U5scNKC9xz0C","outputId":"2e82bc2d-a8af-49ee-e67e-7629458a680a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Kaggle public: 0.85526","metadata":{}},{"cell_type":"markdown","source":"# Reference\n1. I modify the sample code from NTU machine learning course","metadata":{"id":"uYNnYHVHX1Rc"}}]}